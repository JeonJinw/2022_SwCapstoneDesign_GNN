{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9010e283-eb33-423e-8670-eda779dbceaf",
   "metadata": {},
   "source": [
    "# Typhoon Prediction Project\n",
    "* Predict the trajectory of typhoons\n",
    "    * 6h, 12h, 18h, 24h, ...\n",
    "* LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "9e1988b2-1225-42b4-8600-de1eb0aaa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import mean_absolute_error, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    device = 'cpu'\n",
    "    print('CUDA is not available. Training on CPU...')\n",
    "else:\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6ac5f-615f-4ea2-a480-208fa7a27190",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "bfa9e8a0-e6f3-475f-a1b3-90e7317ba237",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MAP = {'북': 0, '북동':1, '북서': 2, '북북서':3, '북북동':4, '서북서':5, '서':6, '서남서':7, '동북동':8, '남서':9,'남남동':10, '동남동':11, '남남서':12, '동':13, '남':14, '남동':15, -1:-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "2f1e20c0-3ece-4580-9268-0b00fbc3362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리\n",
    "def rep_nan(data):\n",
    "    data['진행방향'].fillna(-1, inplace = True)\n",
    "def drop_str(data):\n",
    "    cnt=0\n",
    "    for y,m,d,t in zip(data[\"Year\"],data[\"Month\"],data[\"Date\"],data[\"Time\"]):\n",
    "            data[\"Year\"][cnt] = y.split(\"년\")[0]\n",
    "            data[\"Month\"][cnt] = m.split(\"월\")[0]\n",
    "            data[\"Date\"][cnt] = d.split(\"일\")[0]\n",
    "            data[\"Time\"][cnt] = t.split(\"시\")[0]\n",
    "            cnt+=1\n",
    "    return data\n",
    "def str2int(data):\n",
    "    data[\"Year\"]=0\n",
    "    data[\"Month\"]=0\n",
    "    data[\"Date\"]=0\n",
    "    data[\"Time\"]=0\n",
    "    for i, element in enumerate(data[\"일시\"]):\n",
    "        data[\"Year\"][i]=element.split(' ')[0]\n",
    "        data[\"Month\"][i]=element.split(' ')[1]\n",
    "        data[\"Date\"][i]=element.split(' ')[2]\n",
    "        data[\"Time\"][i]=element.split(' ')[3]\n",
    "    return data\n",
    "def cat_1(data):\n",
    "    strength_map={}\n",
    "    for i, s in enumerate(data[\"강도\"].unique()):\n",
    "        strength_map[s] = i\n",
    "    data[\"강도\"] = data[\"강도\"].map(strength_map)\n",
    "    return data\n",
    "def cat_2(data):\n",
    "    for i, s in enumerate(data[\"크기\"]):\n",
    "        if s==\"-\":\n",
    "            data[\"크기\"][i] = 0\n",
    "        elif s==\"소형\":\n",
    "            data[\"크기\"][i] = 1\n",
    "        elif s==\"중형\":\n",
    "            data[\"크기\"][i] = 2\n",
    "        else:\n",
    "            data[\"크기\"][i] = 3\n",
    "    return data\n",
    "def dir_enc(data):\n",
    "    for i in range(len(data)):\n",
    "        data[\"진행방향\"][i] = DIR_MAP[data[\"진행방향\"][i]]\n",
    "def prepro(data):\n",
    "    idx = data[data[\"Unnamed: 1\"]!=0].index\n",
    "    data = data.drop(idx)\n",
    "    data = data.reset_index()\n",
    "    data = str2int(data)\n",
    "    drop_str(data)\n",
    "    data =cat_1(data)\n",
    "    data =cat_2(data)\n",
    "    data = data.drop([\"70% 확률 반경(km)\", \"Unnamed: 1\",\"일시\",\"index\"], axis=1)\n",
    "    rep_nan(data)\n",
    "    dir_enc(data)\n",
    "    return data\n",
    "# def prepro2(data):\n",
    "#     data = dir_enc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "id": "62c8dad5-384c-4b48-80cf-04d3f5b5b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Lab/Capstone/Typhoon/tp_\"\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "4a4dcc59-18f4-492f-b0ed-24064afd47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv 순서대로읽어와서 data에 저장\n",
    "for i in range(1,529):\n",
    "    data.append(pd.read_csv(data_dir+str(i).zfill(3)+\".csv\",encoding_errors='ignore',encoding=\"cp949\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "id": "c203df35-40e7-4826-94f8-d9a480779b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hin = data[521]\n",
    "Megi = data[512]\n",
    "del(data[521])\n",
    "del(data[512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "32589ea1-1c6d-4ec7-a321-e2ef3d7f75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i]=prepro(data[i])\n",
    "    #data[i].insert(0,'ID', i)\n",
    "Hin = prepro(Hin)\n",
    "Megi = prepro(Megi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "af640928-9aeb-4e7f-a548-a176f36d58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].drop([\"Year\",\"Month\",\"Date\",\"Time\"],axis=1)\n",
    "Hin = Hin.drop([\"Year\",\"Month\",\"Date\",\"Time\"],axis=1)\n",
    "Megi = Megi.drop([\"Year\",\"Month\",\"Date\",\"Time\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "4329538a-713b-4c5c-a3f6-4834f1ba775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "ec202ad8-66dd-4435-a817-3b76a8efe3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 1.6\n",
      "max 54.0\n",
      "min 99.8\n",
      "max 180.0\n",
      "min 0\n",
      "max 1010\n",
      "min -1\n",
      "max 61\n",
      "min -4\n",
      "max 220\n",
      "min 0\n",
      "max 1480.0\n",
      "min 0.0\n",
      "max 216\n"
     ]
    }
   ],
   "source": [
    "minmax = {}\n",
    "for j in ['위도(N)','경도(E)','중심기압','초속(m/s)','시속(km/h)','강풍반경(km)[예외반경]','이동속도(km/h)']:\n",
    "    m = np.inf\n",
    "    M = -np.inf\n",
    "    for i in range(len(data)):\n",
    "        m = m if np.min(data[i][[j]].values) > m else np.min(data[i][[j]].values)\n",
    "        # if m == -1 :\n",
    "            # print(i, end = ' ')\n",
    "    for i in range(len(data)):\n",
    "        M = M if np.max(data[i][[j]].values) < M  else np.max(data[i][[j]].values)\n",
    "    minmax[j] = [m,M]\n",
    "    print('min', m)\n",
    "    print('max', M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "3f747aa0-510e-4712-be01-8458cfed2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax['위도(N)'][0], minmax['위도(N)'][1] = 0, 60\n",
    "minmax['경도(E)'][0], minmax['경도(E)'][1] = 90, 180\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in ['위도(N)','경도(E)','중심기압','초속(m/s)','시속(km/h)','강풍반경(km)[예외반경]','이동속도(km/h)']:\n",
    "        data[i][j] = (data[i][j] - minmax[j][0])/(minmax[j][1] - minmax[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "650b6cb8-2a8b-4db3-98cd-34d3bd20f29b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "id": "57b9388f-8257-4355-9e04-012eacc17068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>위도(N)</th>\n",
       "      <th>경도(E)</th>\n",
       "      <th>중심기압</th>\n",
       "      <th>초속(m/s)</th>\n",
       "      <th>시속(km/h)</th>\n",
       "      <th>강풍반경(km)[예외반경]</th>\n",
       "      <th>강도</th>\n",
       "      <th>크기</th>\n",
       "      <th>진행방향</th>\n",
       "      <th>이동속도(km/h)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.986139</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.308036</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.982178</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.368333</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      위도(N)     경도(E)      중심기압   초속(m/s)  시속(km/h)  강풍반경(km)[예외반경]  강도  크기  \\\n",
       "0  0.300000  0.323333  0.986139  0.306452  0.308036        0.121622   0   1   \n",
       "1  0.323333  0.340000  0.982178  0.387097  0.388393        0.189189   0   1   \n",
       "2  0.368333  0.363333  0.980198  0.387097  0.388393        0.222973   0   2   \n",
       "\n",
       "   진행방향  이동속도(km/h)  \n",
       "0     0    0.069444  \n",
       "1     1    0.055556  \n",
       "2     1    0.138889  "
      ]
     },
     "execution_count": 1197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d4485-6c1a-4541-a175-173d6515832e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb80b3-5412-44b3-a1c8-285341f5692f",
   "metadata": {},
   "source": [
    "## Window DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "id": "9abebdb6-3997-441e-a6ec-42c27f821a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class windowDataset(Dataset):\n",
    "    def __init__(self, data, input_window, output_window,\n",
    "                 in_features = ['위도(N)','경도(E)','진행방향','이동속도(km/h)'],\n",
    "                 out_features = ['위도(N)','경도(E)'], stride=1):\n",
    "        deletion = []\n",
    "        num_samples = 0\n",
    "        \n",
    "        #총 데이터의 개수\n",
    "        #inputwindow + outputwindow 보다 짧은 길이의 태풍들은 데이터에서 제거해줌.\n",
    "        for i, d in enumerate(data):\n",
    "            if(len(d) < input_window + output_window):\n",
    "                deletion.append(i)\n",
    "                continue\n",
    "            #stride씩 움직일 때 생기는 총 sample의 개수\n",
    "            #태풍별로 확인해줄 예정이니 각 태풍마다의 sample개수를 구하여 전부 더해줌.\n",
    "            num_samples += (len(d) - input_window - output_window) // stride + 1\n",
    "        print('number of useable data : ', len(data)-len(deletion))\n",
    "        for i in deletion[::-1]:\n",
    "            del data[i]\n",
    "        #in feature의 개수와 out feature의 개수 저장\n",
    "        num_in_feature = len(in_features)\n",
    "        num_out_feature = len(out_features)\n",
    "\n",
    "        #input과 output : shape = (window 크기, sample 개수, feature 개수)\n",
    "        X = np.zeros([input_window, num_samples, num_in_feature])\n",
    "        Y = np.zeros([output_window, num_samples, num_out_feature])\n",
    "\n",
    "        j = 0\n",
    "        for d in data:\n",
    "            #data별로 불러옴\n",
    "            for i in range((len(d) - (input_window + output_window))// stride + 1):\n",
    "                #각 데이터의 길이를 확인하고 각 데이터별로 input_window와 output_window에따른\n",
    "                #활용가능한 학습용 데이터의 길이를 확인함\n",
    "                #각 태풍별로 input_window와 output_window에 따른 학습용 데이터를 추출함.\n",
    "                start_x = stride*i\n",
    "                end_x = start_x + input_window\n",
    "                X[:,j] = d[start_x:end_x][in_features]\n",
    "                \n",
    "                start_y = stride*i + input_window\n",
    "                end_y = start_y + output_window\n",
    "                Y[:,j] = d[start_y:end_y][out_features]\n",
    "                j += 1\n",
    "\n",
    "        #추출한 데이터를 Dataset 형식에 맞추어 reshape\n",
    "        X = X.reshape(X.shape[0], X.shape[1], num_in_feature).transpose((1,0,2))\n",
    "        Y = Y.reshape(Y.shape[0], Y.shape[1], num_out_feature).transpose((1,0,2))\n",
    "        self.x = X.astype(np.float32)\n",
    "        self.y = Y.astype(np.float32)\n",
    "        \n",
    "        self.len = len(X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "id": "f69f914c-8c33-4355-a9af-e0252d7cce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of useable data :  451\n",
      "number of useable data :  53\n"
     ]
    }
   ],
   "source": [
    "in_window=3\n",
    "out_window=1\n",
    "batch_size = 256\n",
    "\n",
    "val_idx = int(len(data)*0.9)\n",
    "\n",
    "train_data, valid_data = data[:val_idx], data[val_idx:] #, data[test_idx:]\n",
    "\n",
    "train_dataset = windowDataset(train_data,in_window,out_window)\n",
    "valid_dataset = windowDataset(valid_data,in_window,out_window)\n",
    "# test_dataset = windowDataset(test_data, in_window,out_window)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, 1 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "3778d5a2-9f7d-4893-bd16-9d95d05cbcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 4]), torch.Size([256, 3, 4]))"
      ]
     },
     "execution_count": 1200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape,next(iter(valid_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "id": "3868cc53-c276-4596-b8cc-7bbfeb90c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 1, 2]), torch.Size([256, 1, 2]))"
      ]
     },
     "execution_count": 1201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[1].shape,next(iter(valid_loader))[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165755c1-18b0-4f38-8034-9ed1053dd221",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "id": "dfd82ef5-2a2c-4ee2-9fef-269792afcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 7\n",
    "num_layer = 1\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e888a-15d7-46a9-8789-f3f5700aad71",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1594,
   "id": "2ba13c94-2870-41bb-9554-6c616a3c493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layer,batch_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.embed = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,num_layer, batch_first=True)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "         \n",
    "        self.dropout= nn.Dropout(0.)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.dropout(self.embed(input))\n",
    "        \n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "\n",
    "        hidden = self.ln(hidden)\n",
    "        \n",
    "        output = self.ln(output)\n",
    "        \n",
    "        return output, hidden\n",
    "        \n",
    "    def initHidden(self,num_layer,batch_size):\n",
    "        return torch.zeros(num_layer,batch_size,self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c8196-5e42-4c76-b796-5798026ad7c3",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "id": "b75261b7-8de3-4579-b441-0946c0c27d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layer, batch_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.emb = nn.Linear(output_size, hidden_size)\n",
    "        \n",
    "        self.attn = nn.Linear(hidden_size*2, in_window) # (128,1,32*2) -> (128,1,4) in_window -> padding_size로 설정하면?\n",
    "        \n",
    "        self.attn_combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,num_layer, batch_first=True)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        # (128, 1, 10) -> (128,1,32)\n",
    "        embed = self.dropout(self.emb(input))\n",
    "        # (128,1,32) -> (128,1,64) -> (128,1,4) -> softmax -> (128,1,4) ->attn_weigths shape\n",
    "        # hidden = hidden.max(dim=0)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.concat((embed, hidden.permute(1,0,2)), 2)),dim=2)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs.permute(1,0,2))\n",
    "        \n",
    "        output = torch.cat((embed, hidden.permute(1,0,2)),2)\n",
    "        \n",
    "        output = self.attn_combine(output) # (128,1,64) -> (128,1,32)\n",
    "        \n",
    "        output = self.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        hidden= self.ln(hidden)\n",
    "        \n",
    "        output = self.ln(output)\n",
    "        \n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, num_layer, batch_size):\n",
    "        return torch.zeros(num_layer,batch_size,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "id": "abadd1a0-6fed-42c1-8288-fb550c0c5fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN(\n",
       "   (embed): Linear(in_features=4, out_features=4, bias=True)\n",
       "   (gru): GRU(4, 4, batch_first=True)\n",
       "   (ln): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " DecoderRNN(\n",
       "   (emb): Linear(in_features=2, out_features=4, bias=True)\n",
       "   (attn): Linear(in_features=8, out_features=3, bias=True)\n",
       "   (attn_combine): Linear(in_features=8, out_features=4, bias=True)\n",
       "   (gru): GRU(4, 4, batch_first=True)\n",
       "   (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (ln): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       " ))"
      ]
     },
     "execution_count": 1661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size, hidden_size, num_layer, batch_size)\n",
    "decoder = DecoderRNN(output_size, hidden_size, num_layer, batch_size)\n",
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793aea46-ce20-4cc3-8008-d6d22161c5a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "id": "d4f707f6-0a08-4358-8566-4fa1b0c06532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# epoch = 2000\n",
    "# teacher_forcing_ratio = 0.5\n",
    "# learning_rate = 0.0000016\n",
    "\n",
    "\n",
    "\n",
    "# #0.00001\n",
    "\n",
    "# encoder.to(device)\n",
    "# decoder.to(device)\n",
    "# # embeder.to(device)\n",
    "# encoder_optimizer = optim.Adam(encoder.parameters(), lr = learning_rate)\n",
    "# decoder_optimizer = optim.Adam(decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "# # criterion = mse_loss\n",
    "# criterion = nn.MSELoss()\n",
    "# # Save training loss\n",
    "# train_loss = torch.zeros(epoch)\n",
    "# valid_loss_min = np.Inf\n",
    "# # Save validation loss\n",
    "# valid_loss = torch.zeros(epoch)\n",
    "# for e in trange(epoch):\n",
    "#     encoder.train()\n",
    "#     decoder.train()\n",
    "#     data, label = next(iter(train_loader))\n",
    "#     data, label = data.to(device), label.to(device)\n",
    "\n",
    "#     encoder_hidden = encoder.initHidden(num_layer,label.shape[0])\n",
    "#     encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "\n",
    "#     encoder_outputs = torch.zeros(in_window, label.shape[0], encoder.hidden_size).to(device)\n",
    "\n",
    "#     # Encoder\n",
    "#     for ei in range(in_window): #input_window_size 만큼\n",
    "#         encoder_output, encoder_hidden = encoder(data[:,ei,:].reshape(label.shape[0],1,-1), encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output.squeeze()\n",
    "\n",
    "#     # Decoder_init\n",
    "#     d_input = torch.zeros((label.shape[0],1,2)).to(device)\n",
    "#     decoder_input = d_input\n",
    "#     decoder_hidden = encoder_hidden\n",
    "\n",
    "#     # Decoder\n",
    "#     for di in range(out_window): # input_window_size + output_window_size 만큼\n",
    "#         decoder_output, decoder_hidden, attn_weigths = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "#         loss=criterion(decoder_output, label[:,di,:].reshape(label.shape[0],1,-1))\n",
    "#         decoder_input = label[:,di,:].reshape(label.shape[0],1,-1) #if teacher_force else decoder_output\n",
    "#         train_loss[e] += loss.item()\n",
    "\n",
    "#     loss.backward(retain_graph = True)\n",
    "#     encoder_optimizer.step()\n",
    "#     decoder_optimizer.step()\n",
    "#     train_loss[e] /= len(train_loader)\n",
    "#     print(f'Epoch : {e},\\ttrain_loss : {train_loss[e]}')\n",
    "#     # if (valid_loss[e] < valid_loss_min):\n",
    "#     #     print(f'**********Valid l oss decreased ({valid_loss_min:.6f} ==> {valid_loss[e]:.6f})**********')\n",
    "#     #     valid_loss_min = valid_loss[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1598,
   "id": "16bb809e-803a-4c27-8703-4bb04581e353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25020456460>]"
      ]
     },
     "execution_count": 1598,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "id": "60b23cf3-c4c3-4faa-ac74-cb8e59e8b8c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 4000\n",
    "learning_rate = 0.01\n",
    "\n",
    "def training_net(epoch, lr, hidden_size,hype):\n",
    "    input_size=4\n",
    "    output_size=2\n",
    "    num_layer=1\n",
    "    batch_size=256\n",
    "    \n",
    "    encoder = EncoderRNN(input_size, hidden_size, num_layer, batch_size)\n",
    "    decoder = DecoderRNN(output_size, hidden_size, num_layer, batch_size)\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    # embeder.to(device)\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "    # scheduler1 = optim.lr_scheduler.StepLR(encoder_optimizer,step_size=500,gamma=0.6,verbose=True)\n",
    "    # scheduler2 = optim.lr_scheduler.StepLR(decoder_optimizer,step_size=500,gamma=0.6,verbose=True)\n",
    "\n",
    "\n",
    "    scheduler1 = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer,T_max=epoch,verbose=True)\n",
    "    scheduler2 = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer,T_max=epoch,verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "    # criterion = mse_loss\n",
    "    criterion = nn.L1Loss()\n",
    "    # Save training loss\n",
    "    train_loss = torch.zeros(epoch)\n",
    "    valid_loss_min = np.Inf\n",
    "    # Save validation loss\n",
    "    valid_loss = torch.zeros(epoch)\n",
    "    for e in trange(epoch):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            encoder_hidden = encoder.initHidden(num_layer,label.shape[0])\n",
    "            encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_outputs = torch.zeros(in_window, label.shape[0], encoder.hidden_size).to(device)\n",
    "\n",
    "            # Encoder\n",
    "            for ei in range(in_window): #input_window_size 만큼\n",
    "                encoder_output, encoder_hidden = encoder(data[:,ei,:].reshape(label.shape[0],1,-1), encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output.squeeze()\n",
    "\n",
    "            # Decoder_init\n",
    "            d_input = torch.zeros((label.shape[0],1,2)).to(device)\n",
    "            decoder_input = d_input\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            # Decoder\n",
    "            for di in range(out_window): # input_window_size + output_window_size 만큼\n",
    "                decoder_output, decoder_hidden, attn_weigths = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "                loss=criterion(decoder_output, label[:,di,:].reshape(label.shape[0],1,-1))\n",
    "                decoder_input = label[:,di,:].reshape(label.shape[0],1,-1) #if teacher_force else decoder_output\n",
    "                train_loss[e] += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph = True)\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "        train_loss[e] /= len(train_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            for data, label in valid_loader:\n",
    "                data, label = data.to(device), label.to(device)\n",
    "\n",
    "                encoder_hidden = encoder.initHidden(num_layer,label.shape[0])\n",
    "                encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "                encoder_outputs = torch.zeros(in_window, label.shape[0], encoder.hidden_size).to(device)\n",
    "\n",
    "                # Encoder\n",
    "                for ei in range(in_window): #input_window_size 만큼\n",
    "                    encoder_output, encoder_hidden = encoder(data[:,ei,:].reshape(label.shape[0],1,-1), encoder_hidden)\n",
    "                    encoder_outputs[ei] = encoder_output.squeeze()\n",
    "\n",
    "                # Decoder_init\n",
    "                d_input = torch.zeros((label.shape[0],1,2)).to(device)\n",
    "                decoder_input = d_input\n",
    "                decoder_hidden = encoder_hidden\n",
    "\n",
    "                # Decoder\n",
    "                for di in range(out_window): # input_window_size + output_window_size 만큼\n",
    "                    decoder_output, decoder_hidden, attn_weights = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "                    loss=criterion(decoder_output, label[:,di,:].reshape(label.shape[0],1,-1))\n",
    "                    decoder_input = label[:,di,:].reshape(label.shape[0],1,-1) #if teacher_force else decoder_output\n",
    "                    valid_loss[e] += loss.item()\n",
    "            valid_loss[e] /= len(valid_loader)\n",
    "            scheduler1.step()\n",
    "            scheduler2.step()\n",
    "        print(f'Epoch : {e},\\ttrain_loss : {train_loss[e]}\\tvalid_loss :{valid_loss[e]}')\n",
    "        if (valid_loss[e] < valid_loss_min):\n",
    "            print(f'**********Valid loss decreased ({valid_loss_min:.6f} ==> {valid_loss[e]:.6f})**********')\n",
    "            valid_loss_min = valid_loss[e]\n",
    "            torch.save(encoder.state_dict(),'Encoder.pt')\n",
    "            torch.save(decoder.state_dict(),'Decoder.pt')\n",
    "            # torch.save(encoder.state_dict(),'Encoder_hype{}.pt'.format(hype))\n",
    "            # torch.save(decoder.state_dict(),'Decoder_hype{}.pt'.format(hype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1575,
   "id": "06a9b325-38f0-42be-bfb7-5a6d620f1bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_net(epoch=4000,lr=0.01,hidden_size=11,hype=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1759,
   "id": "fb0de8cf-4dbd-4928-8de5-0a6f23e45b8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Adjusting learning rate of group 0 to 8.0000e-03.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c7dd4a3f0b4c8896500e291389e805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Epoch : 0,\ttrain_loss : 0.30340734124183655\tvalid_loss :0.14895319938659668\n",
      "**********Valid loss decreased (inf ==> 0.148953)**********\n",
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Epoch : 1,\ttrain_loss : 0.13050027191638947\tvalid_loss :0.13382047414779663\n",
      "**********Valid loss decreased (0.148953 ==> 0.133820)**********\n",
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Adjusting learning rate of group 0 to 8.0000e-03.\n",
      "Epoch : 2,\ttrain_loss : 0.12250469624996185\tvalid_loss :0.12727409601211548\n",
      "**********Valid loss decreased (0.133820 ==> 0.127274)**********\n",
      "Adjusting learning rate of group 0 to 7.9999e-03.\n",
      "Adjusting learning rate of group 0 to 7.9999e-03.\n",
      "Epoch : 3,\ttrain_loss : 0.11877594888210297\tvalid_loss :0.12438628822565079\n",
      "**********Valid loss decreased (0.127274 ==> 0.124386)**********\n",
      "Adjusting learning rate of group 0 to 7.9999e-03.\n",
      "Adjusting learning rate of group 0 to 7.9999e-03.\n",
      "Epoch : 4,\ttrain_loss : 0.11553521454334259\tvalid_loss :0.12008988112211227\n",
      "**********Valid loss decreased (0.124386 ==> 0.120090)**********\n",
      "Adjusting learning rate of group 0 to 7.9998e-03.\n",
      "Adjusting learning rate of group 0 to 7.9998e-03.\n",
      "Epoch : 5,\ttrain_loss : 0.11361543089151382\tvalid_loss :0.11751842498779297\n",
      "**********Valid loss decreased (0.120090 ==> 0.117518)**********\n",
      "Adjusting learning rate of group 0 to 7.9998e-03.\n",
      "Adjusting learning rate of group 0 to 7.9998e-03.\n",
      "Epoch : 6,\ttrain_loss : 0.11240696907043457\tvalid_loss :0.11564800143241882\n",
      "**********Valid loss decreased (0.117518 ==> 0.115648)**********\n",
      "Adjusting learning rate of group 0 to 7.9997e-03.\n",
      "Adjusting learning rate of group 0 to 7.9997e-03.\n",
      "Epoch : 7,\ttrain_loss : 0.11136964708566666\tvalid_loss :0.11372275650501251\n",
      "**********Valid loss decreased (0.115648 ==> 0.113723)**********\n",
      "Adjusting learning rate of group 0 to 7.9996e-03.\n",
      "Adjusting learning rate of group 0 to 7.9996e-03.\n",
      "Epoch : 8,\ttrain_loss : 0.11004508286714554\tvalid_loss :0.1109427958726883\n",
      "**********Valid loss decreased (0.113723 ==> 0.110943)**********\n",
      "Adjusting learning rate of group 0 to 7.9995e-03.\n",
      "Adjusting learning rate of group 0 to 7.9995e-03.\n",
      "Epoch : 9,\ttrain_loss : 0.10781602561473846\tvalid_loss :0.10561808943748474\n",
      "**********Valid loss decreased (0.110943 ==> 0.105618)**********\n",
      "Adjusting learning rate of group 0 to 7.9994e-03.\n",
      "Adjusting learning rate of group 0 to 7.9994e-03.\n",
      "Epoch : 10,\ttrain_loss : 0.1043814942240715\tvalid_loss :0.09981649369001389\n",
      "**********Valid loss decreased (0.105618 ==> 0.099816)**********\n",
      "Adjusting learning rate of group 0 to 7.9993e-03.\n",
      "Adjusting learning rate of group 0 to 7.9993e-03.\n",
      "Epoch : 11,\ttrain_loss : 0.10182211548089981\tvalid_loss :0.1028960719704628\n",
      "Adjusting learning rate of group 0 to 7.9992e-03.\n",
      "Adjusting learning rate of group 0 to 7.9992e-03.\n",
      "Epoch : 12,\ttrain_loss : 0.09877388179302216\tvalid_loss :0.09531070291996002\n",
      "**********Valid loss decreased (0.099816 ==> 0.095311)**********\n",
      "Adjusting learning rate of group 0 to 7.9990e-03.\n",
      "Adjusting learning rate of group 0 to 7.9990e-03.\n",
      "Epoch : 13,\ttrain_loss : 0.09326893836259842\tvalid_loss :0.09000234305858612\n",
      "**********Valid loss decreased (0.095311 ==> 0.090002)**********\n",
      "Adjusting learning rate of group 0 to 7.9989e-03.\n",
      "Adjusting learning rate of group 0 to 7.9989e-03.\n",
      "Epoch : 14,\ttrain_loss : 0.09338169544935226\tvalid_loss :0.09125146269798279\n",
      "Adjusting learning rate of group 0 to 7.9987e-03.\n",
      "Adjusting learning rate of group 0 to 7.9987e-03.\n",
      "Epoch : 15,\ttrain_loss : 0.089356429874897\tvalid_loss :0.08356989920139313\n",
      "**********Valid loss decreased (0.090002 ==> 0.083570)**********\n",
      "Adjusting learning rate of group 0 to 7.9986e-03.\n",
      "Adjusting learning rate of group 0 to 7.9986e-03.\n",
      "Epoch : 16,\ttrain_loss : 0.08796628564596176\tvalid_loss :0.08349505811929703\n",
      "**********Valid loss decreased (0.083570 ==> 0.083495)**********\n",
      "Adjusting learning rate of group 0 to 7.9984e-03.\n",
      "Adjusting learning rate of group 0 to 7.9984e-03.\n",
      "Epoch : 17,\ttrain_loss : 0.0852028876543045\tvalid_loss :0.07915088534355164\n",
      "**********Valid loss decreased (0.083495 ==> 0.079151)**********\n",
      "Adjusting learning rate of group 0 to 7.9982e-03.\n",
      "Adjusting learning rate of group 0 to 7.9982e-03.\n",
      "Epoch : 18,\ttrain_loss : 0.08474279195070267\tvalid_loss :0.0798182487487793\n",
      "Adjusting learning rate of group 0 to 7.9980e-03.\n",
      "Adjusting learning rate of group 0 to 7.9980e-03.\n",
      "Epoch : 19,\ttrain_loss : 0.08265899121761322\tvalid_loss :0.07621242105960846\n",
      "**********Valid loss decreased (0.079151 ==> 0.076212)**********\n",
      "Adjusting learning rate of group 0 to 7.9978e-03.\n",
      "Adjusting learning rate of group 0 to 7.9978e-03.\n",
      "Epoch : 20,\ttrain_loss : 0.0815737321972847\tvalid_loss :0.0752192959189415\n",
      "**********Valid loss decreased (0.076212 ==> 0.075219)**********\n",
      "Adjusting learning rate of group 0 to 7.9976e-03.\n",
      "Adjusting learning rate of group 0 to 7.9976e-03.\n",
      "Epoch : 21,\ttrain_loss : 0.08007808774709702\tvalid_loss :0.07345897704362869\n",
      "**********Valid loss decreased (0.075219 ==> 0.073459)**********\n",
      "Adjusting learning rate of group 0 to 7.9974e-03.\n",
      "Adjusting learning rate of group 0 to 7.9974e-03.\n",
      "Epoch : 22,\ttrain_loss : 0.07730937004089355\tvalid_loss :0.07615074515342712\n",
      "Adjusting learning rate of group 0 to 7.9972e-03.\n",
      "Adjusting learning rate of group 0 to 7.9972e-03.\n",
      "Epoch : 23,\ttrain_loss : 0.07539958506822586\tvalid_loss :0.07403784990310669\n",
      "Adjusting learning rate of group 0 to 7.9969e-03.\n",
      "Adjusting learning rate of group 0 to 7.9969e-03.\n",
      "Epoch : 24,\ttrain_loss : 0.07527805864810944\tvalid_loss :0.07124146074056625\n",
      "**********Valid loss decreased (0.073459 ==> 0.071241)**********\n",
      "Adjusting learning rate of group 0 to 7.9967e-03.\n",
      "Adjusting learning rate of group 0 to 7.9967e-03.\n",
      "Epoch : 25,\ttrain_loss : 0.0744858831167221\tvalid_loss :0.06908966600894928\n",
      "**********Valid loss decreased (0.071241 ==> 0.069090)**********\n",
      "Adjusting learning rate of group 0 to 7.9964e-03.\n",
      "Adjusting learning rate of group 0 to 7.9964e-03.\n",
      "Epoch : 26,\ttrain_loss : 0.07295631617307663\tvalid_loss :0.06745970994234085\n",
      "**********Valid loss decreased (0.069090 ==> 0.067460)**********\n",
      "Adjusting learning rate of group 0 to 7.9961e-03.\n",
      "Adjusting learning rate of group 0 to 7.9961e-03.\n",
      "Epoch : 27,\ttrain_loss : 0.07099886983633041\tvalid_loss :0.06451410800218582\n",
      "**********Valid loss decreased (0.067460 ==> 0.064514)**********\n",
      "Adjusting learning rate of group 0 to 7.9959e-03.\n",
      "Adjusting learning rate of group 0 to 7.9959e-03.\n",
      "Epoch : 28,\ttrain_loss : 0.06946148723363876\tvalid_loss :0.06120148301124573\n",
      "**********Valid loss decreased (0.064514 ==> 0.061201)**********\n",
      "Adjusting learning rate of group 0 to 7.9956e-03.\n",
      "Adjusting learning rate of group 0 to 7.9956e-03.\n",
      "Epoch : 29,\ttrain_loss : 0.06577085703611374\tvalid_loss :0.05732034146785736\n",
      "**********Valid loss decreased (0.061201 ==> 0.057320)**********\n",
      "Adjusting learning rate of group 0 to 7.9953e-03.\n",
      "Adjusting learning rate of group 0 to 7.9953e-03.\n",
      "Epoch : 30,\ttrain_loss : 0.063620924949646\tvalid_loss :0.05995948240160942\n",
      "Adjusting learning rate of group 0 to 7.9949e-03.\n",
      "Adjusting learning rate of group 0 to 7.9949e-03.\n",
      "Epoch : 31,\ttrain_loss : 0.06179537624120712\tvalid_loss :0.05633392930030823\n",
      "**********Valid loss decreased (0.057320 ==> 0.056334)**********\n",
      "Adjusting learning rate of group 0 to 7.9946e-03.\n",
      "Adjusting learning rate of group 0 to 7.9946e-03.\n",
      "Epoch : 32,\ttrain_loss : 0.04231777787208557\tvalid_loss :0.044195111840963364\n",
      "**********Valid loss decreased (0.056334 ==> 0.044195)**********\n",
      "Adjusting learning rate of group 0 to 7.9943e-03.\n",
      "Adjusting learning rate of group 0 to 7.9943e-03.\n",
      "Epoch : 33,\ttrain_loss : 0.02883213572204113\tvalid_loss :0.02878371998667717\n",
      "**********Valid loss decreased (0.044195 ==> 0.028784)**********\n",
      "Adjusting learning rate of group 0 to 7.9940e-03.\n",
      "Adjusting learning rate of group 0 to 7.9940e-03.\n",
      "Epoch : 34,\ttrain_loss : 0.026353150606155396\tvalid_loss :0.04005032777786255\n",
      "Adjusting learning rate of group 0 to 7.9936e-03.\n",
      "Adjusting learning rate of group 0 to 7.9936e-03.\n",
      "Epoch : 35,\ttrain_loss : 0.025381969287991524\tvalid_loss :0.014047079719603062\n",
      "**********Valid loss decreased (0.028784 ==> 0.014047)**********\n",
      "Adjusting learning rate of group 0 to 7.9932e-03.\n",
      "Adjusting learning rate of group 0 to 7.9932e-03.\n",
      "Epoch : 36,\ttrain_loss : 0.019880855455994606\tvalid_loss :0.02036837860941887\n",
      "Adjusting learning rate of group 0 to 7.9929e-03.\n",
      "Adjusting learning rate of group 0 to 7.9929e-03.\n",
      "Epoch : 37,\ttrain_loss : 0.018819596618413925\tvalid_loss :0.013206060975790024\n",
      "**********Valid loss decreased (0.014047 ==> 0.013206)**********\n",
      "Adjusting learning rate of group 0 to 7.9925e-03.\n",
      "Adjusting learning rate of group 0 to 7.9925e-03.\n",
      "Epoch : 38,\ttrain_loss : 0.018147822469472885\tvalid_loss :0.02842555195093155\n",
      "Adjusting learning rate of group 0 to 7.9921e-03.\n",
      "Adjusting learning rate of group 0 to 7.9921e-03.\n",
      "Epoch : 39,\ttrain_loss : 0.019572781398892403\tvalid_loss :0.01800440438091755\n",
      "Adjusting learning rate of group 0 to 7.9917e-03.\n",
      "Adjusting learning rate of group 0 to 7.9917e-03.\n",
      "Epoch : 40,\ttrain_loss : 0.02192944847047329\tvalid_loss :0.026390716433525085\n",
      "Adjusting learning rate of group 0 to 7.9913e-03.\n",
      "Adjusting learning rate of group 0 to 7.9913e-03.\n",
      "Epoch : 41,\ttrain_loss : 0.018396660685539246\tvalid_loss :0.01332795713096857\n",
      "Adjusting learning rate of group 0 to 7.9909e-03.\n",
      "Adjusting learning rate of group 0 to 7.9909e-03.\n",
      "Epoch : 42,\ttrain_loss : 0.015799647197127342\tvalid_loss :0.011792006902396679\n",
      "**********Valid loss decreased (0.013206 ==> 0.011792)**********\n",
      "Adjusting learning rate of group 0 to 7.9905e-03.\n",
      "Adjusting learning rate of group 0 to 7.9905e-03.\n",
      "Epoch : 43,\ttrain_loss : 0.015581930987536907\tvalid_loss :0.011096419766545296\n",
      "**********Valid loss decreased (0.011792 ==> 0.011096)**********\n",
      "Adjusting learning rate of group 0 to 7.9900e-03.\n",
      "Adjusting learning rate of group 0 to 7.9900e-03.\n",
      "Epoch : 44,\ttrain_loss : 0.015679864212870598\tvalid_loss :0.010152074508368969\n",
      "**********Valid loss decreased (0.011096 ==> 0.010152)**********\n",
      "Adjusting learning rate of group 0 to 7.9896e-03.\n",
      "Adjusting learning rate of group 0 to 7.9896e-03.\n",
      "Epoch : 45,\ttrain_loss : 0.017099028453230858\tvalid_loss :0.01375485211610794\n",
      "Adjusting learning rate of group 0 to 7.9891e-03.\n",
      "Adjusting learning rate of group 0 to 7.9891e-03.\n",
      "Epoch : 46,\ttrain_loss : 0.01666412688791752\tvalid_loss :0.00989106111228466\n",
      "**********Valid loss decreased (0.010152 ==> 0.009891)**********\n",
      "Adjusting learning rate of group 0 to 7.9886e-03.\n",
      "Adjusting learning rate of group 0 to 7.9886e-03.\n",
      "Epoch : 47,\ttrain_loss : 0.015557480044662952\tvalid_loss :0.011720262467861176\n",
      "Adjusting learning rate of group 0 to 7.9882e-03.\n",
      "Adjusting learning rate of group 0 to 7.9882e-03.\n",
      "Epoch : 48,\ttrain_loss : 0.015981413424015045\tvalid_loss :0.015427240170538425\n",
      "Adjusting learning rate of group 0 to 7.9877e-03.\n",
      "Adjusting learning rate of group 0 to 7.9877e-03.\n",
      "Epoch : 49,\ttrain_loss : 0.014594641514122486\tvalid_loss :0.01143190823495388\n",
      "Adjusting learning rate of group 0 to 7.9872e-03.\n",
      "Adjusting learning rate of group 0 to 7.9872e-03.\n",
      "Epoch : 50,\ttrain_loss : 0.015424233861267567\tvalid_loss :0.01360040158033371\n",
      "Adjusting learning rate of group 0 to 7.9867e-03.\n",
      "Adjusting learning rate of group 0 to 7.9867e-03.\n",
      "Epoch : 51,\ttrain_loss : 0.014124637469649315\tvalid_loss :0.00986119918525219\n",
      "**********Valid loss decreased (0.009891 ==> 0.009861)**********\n",
      "Adjusting learning rate of group 0 to 7.9861e-03.\n",
      "Adjusting learning rate of group 0 to 7.9861e-03.\n",
      "Epoch : 52,\ttrain_loss : 0.015636499971151352\tvalid_loss :0.01120407972484827\n",
      "Adjusting learning rate of group 0 to 7.9856e-03.\n",
      "Adjusting learning rate of group 0 to 7.9856e-03.\n",
      "Epoch : 53,\ttrain_loss : 0.014644069597125053\tvalid_loss :0.009082520380616188\n",
      "**********Valid loss decreased (0.009861 ==> 0.009083)**********\n",
      "Adjusting learning rate of group 0 to 7.9851e-03.\n",
      "Adjusting learning rate of group 0 to 7.9851e-03.\n",
      "Epoch : 54,\ttrain_loss : 0.014669319614768028\tvalid_loss :0.009426286444067955\n",
      "Adjusting learning rate of group 0 to 7.9845e-03.\n",
      "Adjusting learning rate of group 0 to 7.9845e-03.\n",
      "Epoch : 55,\ttrain_loss : 0.013653806410729885\tvalid_loss :0.008152369409799576\n",
      "**********Valid loss decreased (0.009083 ==> 0.008152)**********\n",
      "Adjusting learning rate of group 0 to 7.9840e-03.\n",
      "Adjusting learning rate of group 0 to 7.9840e-03.\n",
      "Epoch : 56,\ttrain_loss : 0.01419516745954752\tvalid_loss :0.0098501518368721\n",
      "Adjusting learning rate of group 0 to 7.9834e-03.\n",
      "Adjusting learning rate of group 0 to 7.9834e-03.\n",
      "Epoch : 57,\ttrain_loss : 0.014190706424415112\tvalid_loss :0.011709696613252163\n",
      "Adjusting learning rate of group 0 to 7.9828e-03.\n",
      "Adjusting learning rate of group 0 to 7.9828e-03.\n",
      "Epoch : 58,\ttrain_loss : 0.015790684148669243\tvalid_loss :0.008271440863609314\n",
      "Adjusting learning rate of group 0 to 7.9822e-03.\n",
      "Adjusting learning rate of group 0 to 7.9822e-03.\n",
      "Epoch : 59,\ttrain_loss : 0.01632433570921421\tvalid_loss :0.008763566613197327\n",
      "Adjusting learning rate of group 0 to 7.9817e-03.\n",
      "Adjusting learning rate of group 0 to 7.9817e-03.\n",
      "Epoch : 60,\ttrain_loss : 0.016151074320077896\tvalid_loss :0.008282290771603584\n",
      "Adjusting learning rate of group 0 to 7.9810e-03.\n",
      "Adjusting learning rate of group 0 to 7.9810e-03.\n",
      "Epoch : 61,\ttrain_loss : 0.014389120042324066\tvalid_loss :0.009952257387340069\n",
      "Adjusting learning rate of group 0 to 7.9804e-03.\n",
      "Adjusting learning rate of group 0 to 7.9804e-03.\n",
      "Epoch : 62,\ttrain_loss : 0.014397014863789082\tvalid_loss :0.010880179703235626\n",
      "Adjusting learning rate of group 0 to 7.9798e-03.\n",
      "Adjusting learning rate of group 0 to 7.9798e-03.\n",
      "Epoch : 63,\ttrain_loss : 0.013676670379936695\tvalid_loss :0.009431582875549793\n",
      "Adjusting learning rate of group 0 to 7.9792e-03.\n",
      "Adjusting learning rate of group 0 to 7.9792e-03.\n",
      "Epoch : 64,\ttrain_loss : 0.012536915019154549\tvalid_loss :0.009196006692945957\n",
      "Adjusting learning rate of group 0 to 7.9785e-03.\n",
      "Adjusting learning rate of group 0 to 7.9785e-03.\n",
      "Epoch : 65,\ttrain_loss : 0.012827582657337189\tvalid_loss :0.007601920515298843\n",
      "**********Valid loss decreased (0.008152 ==> 0.007602)**********\n",
      "Adjusting learning rate of group 0 to 7.9779e-03.\n",
      "Adjusting learning rate of group 0 to 7.9779e-03.\n",
      "Epoch : 66,\ttrain_loss : 0.013158239424228668\tvalid_loss :0.009714278392493725\n",
      "Adjusting learning rate of group 0 to 7.9772e-03.\n",
      "Adjusting learning rate of group 0 to 7.9772e-03.\n",
      "Epoch : 67,\ttrain_loss : 0.012766407802700996\tvalid_loss :0.008693546056747437\n",
      "Adjusting learning rate of group 0 to 7.9765e-03.\n",
      "Adjusting learning rate of group 0 to 7.9765e-03.\n",
      "Epoch : 68,\ttrain_loss : 0.012620368972420692\tvalid_loss :0.009336589835584164\n",
      "Adjusting learning rate of group 0 to 7.9758e-03.\n",
      "Adjusting learning rate of group 0 to 7.9758e-03.\n",
      "Epoch : 69,\ttrain_loss : 0.013470062054693699\tvalid_loss :0.01147556584328413\n",
      "Adjusting learning rate of group 0 to 7.9751e-03.\n",
      "Adjusting learning rate of group 0 to 7.9751e-03.\n",
      "Epoch : 70,\ttrain_loss : 0.012976529076695442\tvalid_loss :0.013331100344657898\n",
      "Adjusting learning rate of group 0 to 7.9744e-03.\n",
      "Adjusting learning rate of group 0 to 7.9744e-03.\n",
      "Epoch : 71,\ttrain_loss : 0.012933971360325813\tvalid_loss :0.012198328971862793\n",
      "Adjusting learning rate of group 0 to 7.9737e-03.\n",
      "Adjusting learning rate of group 0 to 7.9737e-03.\n",
      "Epoch : 72,\ttrain_loss : 0.014664816670119762\tvalid_loss :0.009608970023691654\n",
      "Adjusting learning rate of group 0 to 7.9730e-03.\n",
      "Adjusting learning rate of group 0 to 7.9730e-03.\n",
      "Epoch : 73,\ttrain_loss : 0.011827102862298489\tvalid_loss :0.008146450854837894\n",
      "Adjusting learning rate of group 0 to 7.9723e-03.\n",
      "Adjusting learning rate of group 0 to 7.9723e-03.\n",
      "Epoch : 74,\ttrain_loss : 0.011477353982627392\tvalid_loss :0.00941101461648941\n",
      "Adjusting learning rate of group 0 to 7.9715e-03.\n",
      "Adjusting learning rate of group 0 to 7.9715e-03.\n",
      "Epoch : 75,\ttrain_loss : 0.013179278932511806\tvalid_loss :0.00762668251991272\n",
      "Adjusting learning rate of group 0 to 7.9708e-03.\n",
      "Adjusting learning rate of group 0 to 7.9708e-03.\n",
      "Epoch : 76,\ttrain_loss : 0.010829371400177479\tvalid_loss :0.007236838806420565\n",
      "**********Valid loss decreased (0.007602 ==> 0.007237)**********\n",
      "Adjusting learning rate of group 0 to 7.9700e-03.\n",
      "Adjusting learning rate of group 0 to 7.9700e-03.\n",
      "Epoch : 77,\ttrain_loss : 0.01168071012943983\tvalid_loss :0.010488256812095642\n",
      "Adjusting learning rate of group 0 to 7.9692e-03.\n",
      "Adjusting learning rate of group 0 to 7.9692e-03.\n",
      "Epoch : 78,\ttrain_loss : 0.013086972758173943\tvalid_loss :0.01289982721209526\n",
      "Adjusting learning rate of group 0 to 7.9685e-03.\n",
      "Adjusting learning rate of group 0 to 7.9685e-03.\n",
      "Epoch : 79,\ttrain_loss : 0.012528078630566597\tvalid_loss :0.008338670246303082\n",
      "Adjusting learning rate of group 0 to 7.9677e-03.\n",
      "Adjusting learning rate of group 0 to 7.9677e-03.\n",
      "Epoch : 80,\ttrain_loss : 0.012298050336539745\tvalid_loss :0.006945531815290451\n",
      "**********Valid loss decreased (0.007237 ==> 0.006946)**********\n",
      "Adjusting learning rate of group 0 to 7.9669e-03.\n",
      "Adjusting learning rate of group 0 to 7.9669e-03.\n",
      "Epoch : 81,\ttrain_loss : 0.011524973437190056\tvalid_loss :0.008659431710839272\n",
      "Adjusting learning rate of group 0 to 7.9661e-03.\n",
      "Adjusting learning rate of group 0 to 7.9661e-03.\n",
      "Epoch : 82,\ttrain_loss : 0.011644837446510792\tvalid_loss :0.008317753672599792\n",
      "Adjusting learning rate of group 0 to 7.9652e-03.\n",
      "Adjusting learning rate of group 0 to 7.9652e-03.\n",
      "Epoch : 83,\ttrain_loss : 0.01186081301420927\tvalid_loss :0.007916703820228577\n",
      "Adjusting learning rate of group 0 to 7.9644e-03.\n",
      "Adjusting learning rate of group 0 to 7.9644e-03.\n",
      "Epoch : 84,\ttrain_loss : 0.011573904193937778\tvalid_loss :0.009255887009203434\n",
      "Adjusting learning rate of group 0 to 7.9636e-03.\n",
      "Adjusting learning rate of group 0 to 7.9636e-03.\n",
      "Epoch : 85,\ttrain_loss : 0.011408251710236073\tvalid_loss :0.009654483757913113\n",
      "Adjusting learning rate of group 0 to 7.9627e-03.\n",
      "Adjusting learning rate of group 0 to 7.9627e-03.\n",
      "Epoch : 86,\ttrain_loss : 0.011637704446911812\tvalid_loss :0.010678783059120178\n",
      "Adjusting learning rate of group 0 to 7.9618e-03.\n",
      "Adjusting learning rate of group 0 to 7.9618e-03.\n",
      "Epoch : 87,\ttrain_loss : 0.012211843393743038\tvalid_loss :0.008878950029611588\n",
      "Adjusting learning rate of group 0 to 7.9610e-03.\n",
      "Adjusting learning rate of group 0 to 7.9610e-03.\n",
      "Epoch : 88,\ttrain_loss : 0.011245384812355042\tvalid_loss :0.007458222098648548\n",
      "Adjusting learning rate of group 0 to 7.9601e-03.\n",
      "Adjusting learning rate of group 0 to 7.9601e-03.\n",
      "Epoch : 89,\ttrain_loss : 0.011589651927351952\tvalid_loss :0.008462328463792801\n",
      "Adjusting learning rate of group 0 to 7.9592e-03.\n",
      "Adjusting learning rate of group 0 to 7.9592e-03.\n",
      "Epoch : 90,\ttrain_loss : 0.01085747592151165\tvalid_loss :0.006812769919633865\n",
      "**********Valid loss decreased (0.006946 ==> 0.006813)**********\n",
      "Adjusting learning rate of group 0 to 7.9583e-03.\n",
      "Adjusting learning rate of group 0 to 7.9583e-03.\n",
      "Epoch : 91,\ttrain_loss : 0.010806207545101643\tvalid_loss :0.0075780535116791725\n",
      "Adjusting learning rate of group 0 to 7.9574e-03.\n",
      "Adjusting learning rate of group 0 to 7.9574e-03.\n",
      "Epoch : 92,\ttrain_loss : 0.011003466323018074\tvalid_loss :0.00809119176119566\n",
      "Adjusting learning rate of group 0 to 7.9565e-03.\n",
      "Adjusting learning rate of group 0 to 7.9565e-03.\n",
      "Epoch : 93,\ttrain_loss : 0.011002525687217712\tvalid_loss :0.007875674404203892\n",
      "Adjusting learning rate of group 0 to 7.9555e-03.\n",
      "Adjusting learning rate of group 0 to 7.9555e-03.\n",
      "Epoch : 94,\ttrain_loss : 0.012263260781764984\tvalid_loss :0.016550343483686447\n",
      "Adjusting learning rate of group 0 to 7.9546e-03.\n",
      "Adjusting learning rate of group 0 to 7.9546e-03.\n",
      "Epoch : 95,\ttrain_loss : 0.010522199794650078\tvalid_loss :0.00764167495071888\n",
      "Adjusting learning rate of group 0 to 7.9537e-03.\n",
      "Adjusting learning rate of group 0 to 7.9537e-03.\n",
      "Epoch : 96,\ttrain_loss : 0.011049441993236542\tvalid_loss :0.01224514190107584\n",
      "Adjusting learning rate of group 0 to 7.9527e-03.\n",
      "Adjusting learning rate of group 0 to 7.9527e-03.\n",
      "Epoch : 97,\ttrain_loss : 0.011327496729791164\tvalid_loss :0.0075762346386909485\n",
      "Adjusting learning rate of group 0 to 7.9517e-03.\n",
      "Adjusting learning rate of group 0 to 7.9517e-03.\n",
      "Epoch : 98,\ttrain_loss : 0.01043905969709158\tvalid_loss :0.009858164936304092\n",
      "Adjusting learning rate of group 0 to 7.9508e-03.\n",
      "Adjusting learning rate of group 0 to 7.9508e-03.\n",
      "Epoch : 99,\ttrain_loss : 0.01125597395002842\tvalid_loss :0.007741461507976055\n",
      "Adjusting learning rate of group 0 to 7.9498e-03.\n",
      "Adjusting learning rate of group 0 to 7.9498e-03.\n",
      "Epoch : 100,\ttrain_loss : 0.010893946513533592\tvalid_loss :0.0074046701192855835\n",
      "Adjusting learning rate of group 0 to 7.9488e-03.\n",
      "Adjusting learning rate of group 0 to 7.9488e-03.\n",
      "Epoch : 101,\ttrain_loss : 0.011616358533501625\tvalid_loss :0.00963222049176693\n",
      "Adjusting learning rate of group 0 to 7.9478e-03.\n",
      "Adjusting learning rate of group 0 to 7.9478e-03.\n",
      "Epoch : 102,\ttrain_loss : 0.01089568343013525\tvalid_loss :0.007325297221541405\n",
      "Adjusting learning rate of group 0 to 7.9467e-03.\n",
      "Adjusting learning rate of group 0 to 7.9467e-03.\n",
      "Epoch : 103,\ttrain_loss : 0.010415912605822086\tvalid_loss :0.007174601778388023\n",
      "Adjusting learning rate of group 0 to 7.9457e-03.\n",
      "Adjusting learning rate of group 0 to 7.9457e-03.\n",
      "Epoch : 104,\ttrain_loss : 0.010855327360332012\tvalid_loss :0.007598922587931156\n",
      "Adjusting learning rate of group 0 to 7.9447e-03.\n",
      "Adjusting learning rate of group 0 to 7.9447e-03.\n",
      "Epoch : 105,\ttrain_loss : 0.011707729659974575\tvalid_loss :0.006869377568364143\n",
      "Adjusting learning rate of group 0 to 7.9436e-03.\n",
      "Adjusting learning rate of group 0 to 7.9436e-03.\n",
      "Epoch : 106,\ttrain_loss : 0.010809623636305332\tvalid_loss :0.007621461525559425\n",
      "Adjusting learning rate of group 0 to 7.9426e-03.\n",
      "Adjusting learning rate of group 0 to 7.9426e-03.\n",
      "Epoch : 107,\ttrain_loss : 0.011195160448551178\tvalid_loss :0.007247478235512972\n",
      "Adjusting learning rate of group 0 to 7.9415e-03.\n",
      "Adjusting learning rate of group 0 to 7.9415e-03.\n",
      "Epoch : 108,\ttrain_loss : 0.010278887115418911\tvalid_loss :0.007843936793506145\n",
      "Adjusting learning rate of group 0 to 7.9404e-03.\n",
      "Adjusting learning rate of group 0 to 7.9404e-03.\n",
      "Epoch : 109,\ttrain_loss : 0.010438913479447365\tvalid_loss :0.006736572831869125\n",
      "**********Valid loss decreased (0.006813 ==> 0.006737)**********\n",
      "Adjusting learning rate of group 0 to 7.9394e-03.\n",
      "Adjusting learning rate of group 0 to 7.9394e-03.\n",
      "Epoch : 110,\ttrain_loss : 0.010871070437133312\tvalid_loss :0.00717008113861084\n",
      "Adjusting learning rate of group 0 to 7.9383e-03.\n",
      "Adjusting learning rate of group 0 to 7.9383e-03.\n",
      "Epoch : 111,\ttrain_loss : 0.010952514596283436\tvalid_loss :0.007410069927573204\n",
      "Adjusting learning rate of group 0 to 7.9372e-03.\n",
      "Adjusting learning rate of group 0 to 7.9372e-03.\n",
      "Epoch : 112,\ttrain_loss : 0.011431671679019928\tvalid_loss :0.010724789462983608\n",
      "Adjusting learning rate of group 0 to 7.9360e-03.\n",
      "Adjusting learning rate of group 0 to 7.9360e-03.\n",
      "Epoch : 113,\ttrain_loss : 0.01014470774680376\tvalid_loss :0.0073031652718782425\n",
      "Adjusting learning rate of group 0 to 7.9349e-03.\n",
      "Adjusting learning rate of group 0 to 7.9349e-03.\n",
      "Epoch : 114,\ttrain_loss : 0.010704884305596352\tvalid_loss :0.00997091457247734\n",
      "Adjusting learning rate of group 0 to 7.9338e-03.\n",
      "Adjusting learning rate of group 0 to 7.9338e-03.\n",
      "Epoch : 115,\ttrain_loss : 0.010650789365172386\tvalid_loss :0.0074758692644536495\n",
      "Adjusting learning rate of group 0 to 7.9326e-03.\n",
      "Adjusting learning rate of group 0 to 7.9326e-03.\n",
      "Epoch : 116,\ttrain_loss : 0.010921594686806202\tvalid_loss :0.008356165140867233\n",
      "Adjusting learning rate of group 0 to 7.9315e-03.\n",
      "Adjusting learning rate of group 0 to 7.9315e-03.\n",
      "Epoch : 117,\ttrain_loss : 0.010556632652878761\tvalid_loss :0.006610858254134655\n",
      "**********Valid loss decreased (0.006737 ==> 0.006611)**********\n",
      "Adjusting learning rate of group 0 to 7.9303e-03.\n",
      "Adjusting learning rate of group 0 to 7.9303e-03.\n",
      "Epoch : 118,\ttrain_loss : 0.010092233307659626\tvalid_loss :0.006518985144793987\n",
      "**********Valid loss decreased (0.006611 ==> 0.006519)**********\n",
      "Adjusting learning rate of group 0 to 7.9291e-03.\n",
      "Adjusting learning rate of group 0 to 7.9291e-03.\n",
      "Epoch : 119,\ttrain_loss : 0.010968426242470741\tvalid_loss :0.008589095436036587\n",
      "Adjusting learning rate of group 0 to 7.9280e-03.\n",
      "Adjusting learning rate of group 0 to 7.9280e-03.\n",
      "Epoch : 120,\ttrain_loss : 0.011094688437879086\tvalid_loss :0.006707138381898403\n",
      "Adjusting learning rate of group 0 to 7.9268e-03.\n",
      "Adjusting learning rate of group 0 to 7.9268e-03.\n",
      "Epoch : 121,\ttrain_loss : 0.010763471946120262\tvalid_loss :0.007068381644785404\n",
      "Adjusting learning rate of group 0 to 7.9256e-03.\n",
      "Adjusting learning rate of group 0 to 7.9256e-03.\n",
      "Epoch : 122,\ttrain_loss : 0.011259998194873333\tvalid_loss :0.011945423670113087\n",
      "Adjusting learning rate of group 0 to 7.9244e-03.\n",
      "Adjusting learning rate of group 0 to 7.9244e-03.\n",
      "Epoch : 123,\ttrain_loss : 0.010055826976895332\tvalid_loss :0.007970008999109268\n",
      "Adjusting learning rate of group 0 to 7.9231e-03.\n",
      "Adjusting learning rate of group 0 to 7.9231e-03.\n",
      "Epoch : 124,\ttrain_loss : 0.011168649420142174\tvalid_loss :0.00872911885380745\n",
      "Adjusting learning rate of group 0 to 7.9219e-03.\n",
      "Adjusting learning rate of group 0 to 7.9219e-03.\n",
      "Epoch : 125,\ttrain_loss : 0.01025987509638071\tvalid_loss :0.007166764233261347\n",
      "Adjusting learning rate of group 0 to 7.9207e-03.\n",
      "Adjusting learning rate of group 0 to 7.9207e-03.\n",
      "Epoch : 126,\ttrain_loss : 0.010914773680269718\tvalid_loss :0.007905397564172745\n",
      "Adjusting learning rate of group 0 to 7.9194e-03.\n",
      "Adjusting learning rate of group 0 to 7.9194e-03.\n",
      "Epoch : 127,\ttrain_loss : 0.010937605984508991\tvalid_loss :0.009518823586404324\n",
      "Adjusting learning rate of group 0 to 7.9182e-03.\n",
      "Adjusting learning rate of group 0 to 7.9182e-03.\n",
      "Epoch : 128,\ttrain_loss : 0.010249980725347996\tvalid_loss :0.006852383259683847\n",
      "Adjusting learning rate of group 0 to 7.9169e-03.\n",
      "Adjusting learning rate of group 0 to 7.9169e-03.\n",
      "Epoch : 129,\ttrain_loss : 0.011109055951237679\tvalid_loss :0.00990079715847969\n",
      "Adjusting learning rate of group 0 to 7.9156e-03.\n",
      "Adjusting learning rate of group 0 to 7.9156e-03.\n",
      "Epoch : 130,\ttrain_loss : 0.010370229370892048\tvalid_loss :0.006713669281452894\n",
      "Adjusting learning rate of group 0 to 7.9143e-03.\n",
      "Adjusting learning rate of group 0 to 7.9143e-03.\n",
      "Epoch : 131,\ttrain_loss : 0.009877213276922703\tvalid_loss :0.006813777610659599\n",
      "Adjusting learning rate of group 0 to 7.9130e-03.\n",
      "Adjusting learning rate of group 0 to 7.9130e-03.\n",
      "Epoch : 132,\ttrain_loss : 0.010082518681883812\tvalid_loss :0.00778956338763237\n",
      "Adjusting learning rate of group 0 to 7.9117e-03.\n",
      "Adjusting learning rate of group 0 to 7.9117e-03.\n",
      "Epoch : 133,\ttrain_loss : 0.010610167868435383\tvalid_loss :0.006619467865675688\n",
      "Adjusting learning rate of group 0 to 7.9104e-03.\n",
      "Adjusting learning rate of group 0 to 7.9104e-03.\n",
      "Epoch : 134,\ttrain_loss : 0.00985851138830185\tvalid_loss :0.007142248563468456\n",
      "Adjusting learning rate of group 0 to 7.9091e-03.\n",
      "Adjusting learning rate of group 0 to 7.9091e-03.\n",
      "Epoch : 135,\ttrain_loss : 0.010258635506033897\tvalid_loss :0.0077458820305764675\n",
      "Adjusting learning rate of group 0 to 7.9077e-03.\n",
      "Adjusting learning rate of group 0 to 7.9077e-03.\n",
      "Epoch : 136,\ttrain_loss : 0.012018163688480854\tvalid_loss :0.012778820469975471\n",
      "Adjusting learning rate of group 0 to 7.9064e-03.\n",
      "Adjusting learning rate of group 0 to 7.9064e-03.\n",
      "Epoch : 137,\ttrain_loss : 0.011931100860238075\tvalid_loss :0.008999322541058064\n",
      "Adjusting learning rate of group 0 to 7.9050e-03.\n",
      "Adjusting learning rate of group 0 to 7.9050e-03.\n",
      "Epoch : 138,\ttrain_loss : 0.00885541271418333\tvalid_loss :0.006324703339487314\n",
      "**********Valid loss decreased (0.006519 ==> 0.006325)**********\n",
      "Adjusting learning rate of group 0 to 7.9037e-03.\n",
      "Adjusting learning rate of group 0 to 7.9037e-03.\n",
      "Epoch : 139,\ttrain_loss : 0.009382841177284718\tvalid_loss :0.008941469714045525\n",
      "Adjusting learning rate of group 0 to 7.9023e-03.\n",
      "Adjusting learning rate of group 0 to 7.9023e-03.\n",
      "Epoch : 140,\ttrain_loss : 0.009001560509204865\tvalid_loss :0.011198973283171654\n",
      "Adjusting learning rate of group 0 to 7.9009e-03.\n",
      "Adjusting learning rate of group 0 to 7.9009e-03.\n",
      "Epoch : 141,\ttrain_loss : 0.009366641752421856\tvalid_loss :0.010595779865980148\n",
      "Adjusting learning rate of group 0 to 7.8995e-03.\n",
      "Adjusting learning rate of group 0 to 7.8995e-03.\n",
      "Epoch : 142,\ttrain_loss : 0.010313702747225761\tvalid_loss :0.006607638671994209\n",
      "Adjusting learning rate of group 0 to 7.8981e-03.\n",
      "Adjusting learning rate of group 0 to 7.8981e-03.\n",
      "Epoch : 143,\ttrain_loss : 0.00986880250275135\tvalid_loss :0.007570790126919746\n",
      "Adjusting learning rate of group 0 to 7.8967e-03.\n",
      "Adjusting learning rate of group 0 to 7.8967e-03.\n",
      "Epoch : 144,\ttrain_loss : 0.00978610385209322\tvalid_loss :0.006460016593337059\n",
      "Adjusting learning rate of group 0 to 7.8953e-03.\n",
      "Adjusting learning rate of group 0 to 7.8953e-03.\n",
      "Epoch : 145,\ttrain_loss : 0.010044419206678867\tvalid_loss :0.007022359408438206\n",
      "Adjusting learning rate of group 0 to 7.8938e-03.\n",
      "Adjusting learning rate of group 0 to 7.8938e-03.\n",
      "Epoch : 146,\ttrain_loss : 0.009868571534752846\tvalid_loss :0.007233880925923586\n",
      "Adjusting learning rate of group 0 to 7.8924e-03.\n",
      "Adjusting learning rate of group 0 to 7.8924e-03.\n",
      "Epoch : 147,\ttrain_loss : 0.01120738871395588\tvalid_loss :0.00863881316035986\n",
      "Adjusting learning rate of group 0 to 7.8909e-03.\n",
      "Adjusting learning rate of group 0 to 7.8909e-03.\n",
      "Epoch : 148,\ttrain_loss : 0.009100040420889854\tvalid_loss :0.007370711304247379\n",
      "Adjusting learning rate of group 0 to 7.8895e-03.\n",
      "Adjusting learning rate of group 0 to 7.8895e-03.\n",
      "Epoch : 149,\ttrain_loss : 0.009480559267103672\tvalid_loss :0.006277197040617466\n",
      "**********Valid loss decreased (0.006325 ==> 0.006277)**********\n",
      "Adjusting learning rate of group 0 to 7.8880e-03.\n",
      "Adjusting learning rate of group 0 to 7.8880e-03.\n",
      "Epoch : 150,\ttrain_loss : 0.01016504131257534\tvalid_loss :0.008796114474534988\n",
      "Adjusting learning rate of group 0 to 7.8865e-03.\n",
      "Adjusting learning rate of group 0 to 7.8865e-03.\n",
      "Epoch : 151,\ttrain_loss : 0.010681991465389729\tvalid_loss :0.006095805671066046\n",
      "**********Valid loss decreased (0.006277 ==> 0.006096)**********\n",
      "Adjusting learning rate of group 0 to 7.8850e-03.\n",
      "Adjusting learning rate of group 0 to 7.8850e-03.\n",
      "Epoch : 152,\ttrain_loss : 0.010632927529513836\tvalid_loss :0.006970986258238554\n",
      "Adjusting learning rate of group 0 to 7.8835e-03.\n",
      "Adjusting learning rate of group 0 to 7.8835e-03.\n",
      "Epoch : 153,\ttrain_loss : 0.010168492794036865\tvalid_loss :0.0062276204116642475\n",
      "Adjusting learning rate of group 0 to 7.8820e-03.\n",
      "Adjusting learning rate of group 0 to 7.8820e-03.\n",
      "Epoch : 154,\ttrain_loss : 0.010734842158854008\tvalid_loss :0.006766703445464373\n",
      "Adjusting learning rate of group 0 to 7.8805e-03.\n",
      "Adjusting learning rate of group 0 to 7.8805e-03.\n",
      "Epoch : 155,\ttrain_loss : 0.010315358638763428\tvalid_loss :0.005876818671822548\n",
      "**********Valid loss decreased (0.006096 ==> 0.005877)**********\n",
      "Adjusting learning rate of group 0 to 7.8790e-03.\n",
      "Adjusting learning rate of group 0 to 7.8790e-03.\n",
      "Epoch : 156,\ttrain_loss : 0.00968925654888153\tvalid_loss :0.008891312405467033\n",
      "Adjusting learning rate of group 0 to 7.8774e-03.\n",
      "Adjusting learning rate of group 0 to 7.8774e-03.\n",
      "Epoch : 157,\ttrain_loss : 0.010983812622725964\tvalid_loss :0.006414490751922131\n",
      "Adjusting learning rate of group 0 to 7.8759e-03.\n",
      "Adjusting learning rate of group 0 to 7.8759e-03.\n",
      "Epoch : 158,\ttrain_loss : 0.009568898007273674\tvalid_loss :0.007189307827502489\n",
      "Adjusting learning rate of group 0 to 7.8743e-03.\n",
      "Adjusting learning rate of group 0 to 7.8743e-03.\n",
      "Epoch : 159,\ttrain_loss : 0.009428368881344795\tvalid_loss :0.008004026487469673\n",
      "Adjusting learning rate of group 0 to 7.8728e-03.\n",
      "Adjusting learning rate of group 0 to 7.8728e-03.\n",
      "Epoch : 160,\ttrain_loss : 0.009589035995304585\tvalid_loss :0.005970725789666176\n",
      "Adjusting learning rate of group 0 to 7.8712e-03.\n",
      "Adjusting learning rate of group 0 to 7.8712e-03.\n",
      "Epoch : 161,\ttrain_loss : 0.00982588715851307\tvalid_loss :0.006712976843118668\n",
      "Adjusting learning rate of group 0 to 7.8696e-03.\n",
      "Adjusting learning rate of group 0 to 7.8696e-03.\n",
      "Epoch : 162,\ttrain_loss : 0.010069114156067371\tvalid_loss :0.011183268390595913\n",
      "Adjusting learning rate of group 0 to 7.8680e-03.\n",
      "Adjusting learning rate of group 0 to 7.8680e-03.\n",
      "Epoch : 163,\ttrain_loss : 0.00935788732022047\tvalid_loss :0.005705737043172121\n",
      "**********Valid loss decreased (0.005877 ==> 0.005706)**********\n",
      "Adjusting learning rate of group 0 to 7.8664e-03.\n",
      "Adjusting learning rate of group 0 to 7.8664e-03.\n",
      "Epoch : 164,\ttrain_loss : 0.00930990930646658\tvalid_loss :0.008767343126237392\n",
      "Adjusting learning rate of group 0 to 7.8648e-03.\n",
      "Adjusting learning rate of group 0 to 7.8648e-03.\n",
      "Epoch : 165,\ttrain_loss : 0.010368379764258862\tvalid_loss :0.009017979726195335\n",
      "Adjusting learning rate of group 0 to 7.8632e-03.\n",
      "Adjusting learning rate of group 0 to 7.8632e-03.\n",
      "Epoch : 166,\ttrain_loss : 0.010446548461914062\tvalid_loss :0.006036489736288786\n",
      "Adjusting learning rate of group 0 to 7.8615e-03.\n",
      "Adjusting learning rate of group 0 to 7.8615e-03.\n",
      "Epoch : 167,\ttrain_loss : 0.010155967436730862\tvalid_loss :0.007242259569466114\n",
      "Adjusting learning rate of group 0 to 7.8599e-03.\n",
      "Adjusting learning rate of group 0 to 7.8599e-03.\n",
      "Epoch : 168,\ttrain_loss : 0.01089518703520298\tvalid_loss :0.011710705235600471\n",
      "Adjusting learning rate of group 0 to 7.8582e-03.\n",
      "Adjusting learning rate of group 0 to 7.8582e-03.\n",
      "Epoch : 169,\ttrain_loss : 0.009160934016108513\tvalid_loss :0.005703058559447527\n",
      "**********Valid loss decreased (0.005706 ==> 0.005703)**********\n",
      "Adjusting learning rate of group 0 to 7.8566e-03.\n",
      "Adjusting learning rate of group 0 to 7.8566e-03.\n",
      "Epoch : 170,\ttrain_loss : 0.009622401557862759\tvalid_loss :0.0067598032765090466\n",
      "Adjusting learning rate of group 0 to 7.8549e-03.\n",
      "Adjusting learning rate of group 0 to 7.8549e-03.\n",
      "Epoch : 171,\ttrain_loss : 0.009481444954872131\tvalid_loss :0.006258481182157993\n",
      "Adjusting learning rate of group 0 to 7.8532e-03.\n",
      "Adjusting learning rate of group 0 to 7.8532e-03.\n",
      "Epoch : 172,\ttrain_loss : 0.011043627746403217\tvalid_loss :0.009162910282611847\n",
      "Adjusting learning rate of group 0 to 7.8515e-03.\n",
      "Adjusting learning rate of group 0 to 7.8515e-03.\n",
      "Epoch : 173,\ttrain_loss : 0.009490164928138256\tvalid_loss :0.006618291139602661\n",
      "Adjusting learning rate of group 0 to 7.8498e-03.\n",
      "Adjusting learning rate of group 0 to 7.8498e-03.\n",
      "Epoch : 174,\ttrain_loss : 0.009370622225105762\tvalid_loss :0.007718143984675407\n",
      "Adjusting learning rate of group 0 to 7.8481e-03.\n",
      "Adjusting learning rate of group 0 to 7.8481e-03.\n",
      "Epoch : 175,\ttrain_loss : 0.00902284774929285\tvalid_loss :0.0066402023658156395\n",
      "Adjusting learning rate of group 0 to 7.8464e-03.\n",
      "Adjusting learning rate of group 0 to 7.8464e-03.\n",
      "Epoch : 176,\ttrain_loss : 0.009749523364007473\tvalid_loss :0.008651618845760822\n",
      "Adjusting learning rate of group 0 to 7.8447e-03.\n",
      "Adjusting learning rate of group 0 to 7.8447e-03.\n",
      "Epoch : 177,\ttrain_loss : 0.010314165614545345\tvalid_loss :0.006807050667703152\n",
      "Adjusting learning rate of group 0 to 7.8429e-03.\n",
      "Adjusting learning rate of group 0 to 7.8429e-03.\n",
      "Epoch : 178,\ttrain_loss : 0.00892705749720335\tvalid_loss :0.005783160217106342\n",
      "Adjusting learning rate of group 0 to 7.8412e-03.\n",
      "Adjusting learning rate of group 0 to 7.8412e-03.\n",
      "Epoch : 179,\ttrain_loss : 0.00841615255922079\tvalid_loss :0.007345740217715502\n",
      "Adjusting learning rate of group 0 to 7.8394e-03.\n",
      "Adjusting learning rate of group 0 to 7.8394e-03.\n",
      "Epoch : 180,\ttrain_loss : 0.009357340633869171\tvalid_loss :0.0077565331012010574\n",
      "Adjusting learning rate of group 0 to 7.8376e-03.\n",
      "Adjusting learning rate of group 0 to 7.8376e-03.\n",
      "Epoch : 181,\ttrain_loss : 0.008902188390493393\tvalid_loss :0.005820104386657476\n",
      "Adjusting learning rate of group 0 to 7.8359e-03.\n",
      "Adjusting learning rate of group 0 to 7.8359e-03.\n",
      "Epoch : 182,\ttrain_loss : 0.008653989061713219\tvalid_loss :0.00530805392190814\n",
      "**********Valid loss decreased (0.005703 ==> 0.005308)**********\n",
      "Adjusting learning rate of group 0 to 7.8341e-03.\n",
      "Adjusting learning rate of group 0 to 7.8341e-03.\n",
      "Epoch : 183,\ttrain_loss : 0.009226285852491856\tvalid_loss :0.008170128799974918\n",
      "Adjusting learning rate of group 0 to 7.8323e-03.\n",
      "Adjusting learning rate of group 0 to 7.8323e-03.\n",
      "Epoch : 184,\ttrain_loss : 0.009662056341767311\tvalid_loss :0.009089484810829163\n",
      "Adjusting learning rate of group 0 to 7.8305e-03.\n",
      "Adjusting learning rate of group 0 to 7.8305e-03.\n",
      "Epoch : 185,\ttrain_loss : 0.009030596353113651\tvalid_loss :0.005899269133806229\n",
      "Adjusting learning rate of group 0 to 7.8287e-03.\n",
      "Adjusting learning rate of group 0 to 7.8287e-03.\n",
      "Epoch : 186,\ttrain_loss : 0.00858099665492773\tvalid_loss :0.005352541338652372\n",
      "Adjusting learning rate of group 0 to 7.8268e-03.\n",
      "Adjusting learning rate of group 0 to 7.8268e-03.\n",
      "Epoch : 187,\ttrain_loss : 0.009985231794416904\tvalid_loss :0.007102008443325758\n",
      "Adjusting learning rate of group 0 to 7.8250e-03.\n",
      "Adjusting learning rate of group 0 to 7.8250e-03.\n",
      "Epoch : 188,\ttrain_loss : 0.008963932283222675\tvalid_loss :0.0067629688419401646\n",
      "Adjusting learning rate of group 0 to 7.8232e-03.\n",
      "Adjusting learning rate of group 0 to 7.8232e-03.\n",
      "Epoch : 189,\ttrain_loss : 0.008874484337866306\tvalid_loss :0.007263170089572668\n",
      "Adjusting learning rate of group 0 to 7.8213e-03.\n",
      "Adjusting learning rate of group 0 to 7.8213e-03.\n",
      "Epoch : 190,\ttrain_loss : 0.008894139900803566\tvalid_loss :0.006214693188667297\n",
      "Adjusting learning rate of group 0 to 7.8195e-03.\n",
      "Adjusting learning rate of group 0 to 7.8195e-03.\n",
      "Epoch : 191,\ttrain_loss : 0.009241847321391106\tvalid_loss :0.007456817664206028\n",
      "Adjusting learning rate of group 0 to 7.8176e-03.\n",
      "Adjusting learning rate of group 0 to 7.8176e-03.\n",
      "Epoch : 192,\ttrain_loss : 0.009344672784209251\tvalid_loss :0.007736716419458389\n",
      "Adjusting learning rate of group 0 to 7.8157e-03.\n",
      "Adjusting learning rate of group 0 to 7.8157e-03.\n",
      "Epoch : 193,\ttrain_loss : 0.00819560419768095\tvalid_loss :0.005208965390920639\n",
      "**********Valid loss decreased (0.005308 ==> 0.005209)**********\n",
      "Adjusting learning rate of group 0 to 7.8138e-03.\n",
      "Adjusting learning rate of group 0 to 7.8138e-03.\n",
      "Epoch : 194,\ttrain_loss : 0.009220358915627003\tvalid_loss :0.006813741289079189\n",
      "Adjusting learning rate of group 0 to 7.8119e-03.\n",
      "Adjusting learning rate of group 0 to 7.8119e-03.\n",
      "Epoch : 195,\ttrain_loss : 0.008318028412759304\tvalid_loss :0.005846092477440834\n",
      "Adjusting learning rate of group 0 to 7.8100e-03.\n",
      "Adjusting learning rate of group 0 to 7.8100e-03.\n",
      "Epoch : 196,\ttrain_loss : 0.007912968285381794\tvalid_loss :0.009038667194545269\n",
      "Adjusting learning rate of group 0 to 7.8081e-03.\n",
      "Adjusting learning rate of group 0 to 7.8081e-03.\n",
      "Epoch : 197,\ttrain_loss : 0.008315405808389187\tvalid_loss :0.00717091653496027\n",
      "Adjusting learning rate of group 0 to 7.8062e-03.\n",
      "Adjusting learning rate of group 0 to 7.8062e-03.\n",
      "Epoch : 198,\ttrain_loss : 0.008177157491445541\tvalid_loss :0.00663379393517971\n",
      "Adjusting learning rate of group 0 to 7.8042e-03.\n",
      "Adjusting learning rate of group 0 to 7.8042e-03.\n",
      "Epoch : 199,\ttrain_loss : 0.007797932717949152\tvalid_loss :0.005582079757004976\n",
      "Adjusting learning rate of group 0 to 7.8023e-03.\n",
      "Adjusting learning rate of group 0 to 7.8023e-03.\n",
      "Epoch : 200,\ttrain_loss : 0.007893257774412632\tvalid_loss :0.0064373742789030075\n",
      "Adjusting learning rate of group 0 to 7.8003e-03.\n",
      "Adjusting learning rate of group 0 to 7.8003e-03.\n",
      "Epoch : 201,\ttrain_loss : 0.008660160936415195\tvalid_loss :0.005921590141952038\n",
      "Adjusting learning rate of group 0 to 7.7984e-03.\n",
      "Adjusting learning rate of group 0 to 7.7984e-03.\n",
      "Epoch : 202,\ttrain_loss : 0.008416566997766495\tvalid_loss :0.012524297460913658\n",
      "Adjusting learning rate of group 0 to 7.7964e-03.\n",
      "Adjusting learning rate of group 0 to 7.7964e-03.\n",
      "Epoch : 203,\ttrain_loss : 0.0095151262357831\tvalid_loss :0.005420828238129616\n",
      "Adjusting learning rate of group 0 to 7.7944e-03.\n",
      "Adjusting learning rate of group 0 to 7.7944e-03.\n",
      "Epoch : 204,\ttrain_loss : 0.009461930021643639\tvalid_loss :0.00770031102001667\n",
      "Adjusting learning rate of group 0 to 7.7924e-03.\n",
      "Adjusting learning rate of group 0 to 7.7924e-03.\n",
      "Epoch : 205,\ttrain_loss : 0.008212734945118427\tvalid_loss :0.0061654639430344105\n",
      "Adjusting learning rate of group 0 to 7.7904e-03.\n",
      "Adjusting learning rate of group 0 to 7.7904e-03.\n",
      "Epoch : 206,\ttrain_loss : 0.008004417642951012\tvalid_loss :0.006297919899225235\n",
      "Adjusting learning rate of group 0 to 7.7884e-03.\n",
      "Adjusting learning rate of group 0 to 7.7884e-03.\n",
      "Epoch : 207,\ttrain_loss : 0.008576163090765476\tvalid_loss :0.006121687591075897\n",
      "Adjusting learning rate of group 0 to 7.7864e-03.\n",
      "Adjusting learning rate of group 0 to 7.7864e-03.\n",
      "Epoch : 208,\ttrain_loss : 0.008095952682197094\tvalid_loss :0.007157816085964441\n",
      "Adjusting learning rate of group 0 to 7.7843e-03.\n",
      "Adjusting learning rate of group 0 to 7.7843e-03.\n",
      "Epoch : 209,\ttrain_loss : 0.007884999737143517\tvalid_loss :0.0061506726779043674\n",
      "Adjusting learning rate of group 0 to 7.7823e-03.\n",
      "Adjusting learning rate of group 0 to 7.7823e-03.\n",
      "Epoch : 210,\ttrain_loss : 0.00807881634682417\tvalid_loss :0.0074798837304115295\n",
      "Adjusting learning rate of group 0 to 7.7803e-03.\n",
      "Adjusting learning rate of group 0 to 7.7803e-03.\n",
      "Epoch : 211,\ttrain_loss : 0.008694378659129143\tvalid_loss :0.007205066736787558\n",
      "Adjusting learning rate of group 0 to 7.7782e-03.\n",
      "Adjusting learning rate of group 0 to 7.7782e-03.\n",
      "Epoch : 212,\ttrain_loss : 0.007877045311033726\tvalid_loss :0.006749451160430908\n",
      "Adjusting learning rate of group 0 to 7.7761e-03.\n",
      "Adjusting learning rate of group 0 to 7.7761e-03.\n",
      "Epoch : 213,\ttrain_loss : 0.008287569507956505\tvalid_loss :0.007319896947592497\n",
      "Adjusting learning rate of group 0 to 7.7740e-03.\n",
      "Adjusting learning rate of group 0 to 7.7740e-03.\n",
      "Epoch : 214,\ttrain_loss : 0.008187183178961277\tvalid_loss :0.005706985481083393\n",
      "Adjusting learning rate of group 0 to 7.7720e-03.\n",
      "Adjusting learning rate of group 0 to 7.7720e-03.\n",
      "Epoch : 215,\ttrain_loss : 0.0074357083067297935\tvalid_loss :0.006607125513255596\n",
      "Adjusting learning rate of group 0 to 7.7699e-03.\n",
      "Adjusting learning rate of group 0 to 7.7699e-03.\n",
      "Epoch : 216,\ttrain_loss : 0.008366592228412628\tvalid_loss :0.010433766059577465\n",
      "Adjusting learning rate of group 0 to 7.7678e-03.\n",
      "Adjusting learning rate of group 0 to 7.7678e-03.\n",
      "Epoch : 217,\ttrain_loss : 0.008753940463066101\tvalid_loss :0.005574941635131836\n",
      "Adjusting learning rate of group 0 to 7.7656e-03.\n",
      "Adjusting learning rate of group 0 to 7.7656e-03.\n",
      "Epoch : 218,\ttrain_loss : 0.008920901454985142\tvalid_loss :0.009450956247746944\n",
      "Adjusting learning rate of group 0 to 7.7635e-03.\n",
      "Adjusting learning rate of group 0 to 7.7635e-03.\n",
      "Epoch : 219,\ttrain_loss : 0.009506321512162685\tvalid_loss :0.006876108702272177\n",
      "Adjusting learning rate of group 0 to 7.7614e-03.\n",
      "Adjusting learning rate of group 0 to 7.7614e-03.\n",
      "Epoch : 220,\ttrain_loss : 0.010123821906745434\tvalid_loss :0.009468515403568745\n",
      "Adjusting learning rate of group 0 to 7.7592e-03.\n",
      "Adjusting learning rate of group 0 to 7.7592e-03.\n",
      "Epoch : 221,\ttrain_loss : 0.009242156520485878\tvalid_loss :0.006889645010232925\n",
      "Adjusting learning rate of group 0 to 7.7571e-03.\n",
      "Adjusting learning rate of group 0 to 7.7571e-03.\n",
      "Epoch : 222,\ttrain_loss : 0.008468104526400566\tvalid_loss :0.004911214113235474\n",
      "**********Valid loss decreased (0.005209 ==> 0.004911)**********\n",
      "Adjusting learning rate of group 0 to 7.7549e-03.\n",
      "Adjusting learning rate of group 0 to 7.7549e-03.\n",
      "Epoch : 223,\ttrain_loss : 0.008475210517644882\tvalid_loss :0.006219147704541683\n",
      "Adjusting learning rate of group 0 to 7.7528e-03.\n",
      "Adjusting learning rate of group 0 to 7.7528e-03.\n",
      "Epoch : 224,\ttrain_loss : 0.008718853816390038\tvalid_loss :0.00721461046487093\n",
      "Adjusting learning rate of group 0 to 7.7506e-03.\n",
      "Adjusting learning rate of group 0 to 7.7506e-03.\n",
      "Epoch : 225,\ttrain_loss : 0.008527146652340889\tvalid_loss :0.005336742382496595\n",
      "Adjusting learning rate of group 0 to 7.7484e-03.\n",
      "Adjusting learning rate of group 0 to 7.7484e-03.\n",
      "Epoch : 226,\ttrain_loss : 0.0074258423410356045\tvalid_loss :0.0056320237927138805\n",
      "Adjusting learning rate of group 0 to 7.7462e-03.\n",
      "Adjusting learning rate of group 0 to 7.7462e-03.\n",
      "Epoch : 227,\ttrain_loss : 0.007722452748566866\tvalid_loss :0.00547209195792675\n",
      "Adjusting learning rate of group 0 to 7.7440e-03.\n",
      "Adjusting learning rate of group 0 to 7.7440e-03.\n",
      "Epoch : 228,\ttrain_loss : 0.008168574422597885\tvalid_loss :0.0052727763541042805\n",
      "Adjusting learning rate of group 0 to 7.7418e-03.\n",
      "Adjusting learning rate of group 0 to 7.7418e-03.\n",
      "Epoch : 229,\ttrain_loss : 0.008156610652804375\tvalid_loss :0.008689050562679768\n",
      "Adjusting learning rate of group 0 to 7.7396e-03.\n",
      "Adjusting learning rate of group 0 to 7.7396e-03.\n",
      "Epoch : 230,\ttrain_loss : 0.008330782875418663\tvalid_loss :0.005727753974497318\n",
      "Adjusting learning rate of group 0 to 7.7373e-03.\n",
      "Adjusting learning rate of group 0 to 7.7373e-03.\n",
      "Epoch : 231,\ttrain_loss : 0.007746364921331406\tvalid_loss :0.006362175568938255\n",
      "Adjusting learning rate of group 0 to 7.7351e-03.\n",
      "Adjusting learning rate of group 0 to 7.7351e-03.\n",
      "Epoch : 232,\ttrain_loss : 0.007796731311827898\tvalid_loss :0.005283687729388475\n",
      "Adjusting learning rate of group 0 to 7.7328e-03.\n",
      "Adjusting learning rate of group 0 to 7.7328e-03.\n",
      "Epoch : 233,\ttrain_loss : 0.008549278602004051\tvalid_loss :0.013223003596067429\n",
      "Adjusting learning rate of group 0 to 7.7306e-03.\n",
      "Adjusting learning rate of group 0 to 7.7306e-03.\n",
      "Epoch : 234,\ttrain_loss : 0.009634977206587791\tvalid_loss :0.0056911371648311615\n",
      "Adjusting learning rate of group 0 to 7.7283e-03.\n",
      "Adjusting learning rate of group 0 to 7.7283e-03.\n",
      "Epoch : 235,\ttrain_loss : 0.007617924362421036\tvalid_loss :0.006971478462219238\n",
      "Adjusting learning rate of group 0 to 7.7260e-03.\n",
      "Adjusting learning rate of group 0 to 7.7260e-03.\n",
      "Epoch : 236,\ttrain_loss : 0.007710214704275131\tvalid_loss :0.005483307875692844\n",
      "Adjusting learning rate of group 0 to 7.7237e-03.\n",
      "Adjusting learning rate of group 0 to 7.7237e-03.\n",
      "Epoch : 237,\ttrain_loss : 0.007783845532685518\tvalid_loss :0.005094143096357584\n",
      "Adjusting learning rate of group 0 to 7.7214e-03.\n",
      "Adjusting learning rate of group 0 to 7.7214e-03.\n",
      "Epoch : 238,\ttrain_loss : 0.008510945364832878\tvalid_loss :0.011564237996935844\n",
      "Adjusting learning rate of group 0 to 7.7191e-03.\n",
      "Adjusting learning rate of group 0 to 7.7191e-03.\n",
      "Epoch : 239,\ttrain_loss : 0.009018749929964542\tvalid_loss :0.005175542086362839\n",
      "Adjusting learning rate of group 0 to 7.7168e-03.\n",
      "Adjusting learning rate of group 0 to 7.7168e-03.\n",
      "Epoch : 240,\ttrain_loss : 0.008352628909051418\tvalid_loss :0.007029593922197819\n",
      "Adjusting learning rate of group 0 to 7.7145e-03.\n",
      "Adjusting learning rate of group 0 to 7.7145e-03.\n",
      "Epoch : 241,\ttrain_loss : 0.008039378561079502\tvalid_loss :0.0065739951096475124\n",
      "Adjusting learning rate of group 0 to 7.7121e-03.\n",
      "Adjusting learning rate of group 0 to 7.7121e-03.\n",
      "Epoch : 242,\ttrain_loss : 0.007952256128191948\tvalid_loss :0.006301581859588623\n",
      "Adjusting learning rate of group 0 to 7.7098e-03.\n",
      "Adjusting learning rate of group 0 to 7.7098e-03.\n",
      "Epoch : 243,\ttrain_loss : 0.008163373917341232\tvalid_loss :0.005652154330164194\n",
      "Adjusting learning rate of group 0 to 7.7074e-03.\n",
      "Adjusting learning rate of group 0 to 7.7074e-03.\n",
      "Epoch : 244,\ttrain_loss : 0.00791659951210022\tvalid_loss :0.005093630403280258\n",
      "Adjusting learning rate of group 0 to 7.7051e-03.\n",
      "Adjusting learning rate of group 0 to 7.7051e-03.\n",
      "Epoch : 245,\ttrain_loss : 0.007922505028545856\tvalid_loss :0.006016158033162355\n",
      "Adjusting learning rate of group 0 to 7.7027e-03.\n",
      "Adjusting learning rate of group 0 to 7.7027e-03.\n",
      "Epoch : 246,\ttrain_loss : 0.008127455599606037\tvalid_loss :0.006847957149147987\n",
      "Adjusting learning rate of group 0 to 7.7003e-03.\n",
      "Adjusting learning rate of group 0 to 7.7003e-03.\n",
      "Epoch : 247,\ttrain_loss : 0.008108234964311123\tvalid_loss :0.005672331899404526\n",
      "Adjusting learning rate of group 0 to 7.6979e-03.\n",
      "Adjusting learning rate of group 0 to 7.6979e-03.\n",
      "Epoch : 248,\ttrain_loss : 0.008045862428843975\tvalid_loss :0.0067253937013447285\n",
      "Adjusting learning rate of group 0 to 7.6955e-03.\n",
      "Adjusting learning rate of group 0 to 7.6955e-03.\n",
      "Epoch : 249,\ttrain_loss : 0.007619233336299658\tvalid_loss :0.00708001758903265\n",
      "Adjusting learning rate of group 0 to 7.6931e-03.\n",
      "Adjusting learning rate of group 0 to 7.6931e-03.\n",
      "Epoch : 250,\ttrain_loss : 0.008351115509867668\tvalid_loss :0.005269075743854046\n",
      "Adjusting learning rate of group 0 to 7.6907e-03.\n",
      "Adjusting learning rate of group 0 to 7.6907e-03.\n",
      "Epoch : 251,\ttrain_loss : 0.007791091687977314\tvalid_loss :0.005049921106547117\n",
      "Adjusting learning rate of group 0 to 7.6883e-03.\n",
      "Adjusting learning rate of group 0 to 7.6883e-03.\n",
      "Epoch : 252,\ttrain_loss : 0.008053923025727272\tvalid_loss :0.009287968277931213\n",
      "Adjusting learning rate of group 0 to 7.6858e-03.\n",
      "Adjusting learning rate of group 0 to 7.6858e-03.\n",
      "Epoch : 253,\ttrain_loss : 0.010366805829107761\tvalid_loss :0.008092173375189304\n",
      "Adjusting learning rate of group 0 to 7.6834e-03.\n",
      "Adjusting learning rate of group 0 to 7.6834e-03.\n",
      "Epoch : 254,\ttrain_loss : 0.007739517372101545\tvalid_loss :0.006169170141220093\n",
      "Adjusting learning rate of group 0 to 7.6809e-03.\n",
      "Adjusting learning rate of group 0 to 7.6809e-03.\n",
      "Epoch : 255,\ttrain_loss : 0.0091303875669837\tvalid_loss :0.007383247371762991\n",
      "Adjusting learning rate of group 0 to 7.6785e-03.\n",
      "Adjusting learning rate of group 0 to 7.6785e-03.\n",
      "Epoch : 256,\ttrain_loss : 0.008483560755848885\tvalid_loss :0.006345049012452364\n",
      "Adjusting learning rate of group 0 to 7.6760e-03.\n",
      "Adjusting learning rate of group 0 to 7.6760e-03.\n",
      "Epoch : 257,\ttrain_loss : 0.00877158809453249\tvalid_loss :0.010372406803071499\n",
      "Adjusting learning rate of group 0 to 7.6735e-03.\n",
      "Adjusting learning rate of group 0 to 7.6735e-03.\n",
      "Epoch : 258,\ttrain_loss : 0.009996543638408184\tvalid_loss :0.00768188014626503\n",
      "Adjusting learning rate of group 0 to 7.6710e-03.\n",
      "Adjusting learning rate of group 0 to 7.6710e-03.\n",
      "Epoch : 259,\ttrain_loss : 0.009591751731932163\tvalid_loss :0.011276153847575188\n",
      "Adjusting learning rate of group 0 to 7.6685e-03.\n",
      "Adjusting learning rate of group 0 to 7.6685e-03.\n",
      "Epoch : 260,\ttrain_loss : 0.00918542593717575\tvalid_loss :0.0074945129454135895\n",
      "Adjusting learning rate of group 0 to 7.6660e-03.\n",
      "Adjusting learning rate of group 0 to 7.6660e-03.\n",
      "Epoch : 261,\ttrain_loss : 0.007505781948566437\tvalid_loss :0.005630746483802795\n",
      "Adjusting learning rate of group 0 to 7.6635e-03.\n",
      "Adjusting learning rate of group 0 to 7.6635e-03.\n",
      "Epoch : 262,\ttrain_loss : 0.007294625975191593\tvalid_loss :0.00998280942440033\n",
      "Adjusting learning rate of group 0 to 7.6610e-03.\n",
      "Adjusting learning rate of group 0 to 7.6610e-03.\n",
      "Epoch : 263,\ttrain_loss : 0.008572076447308064\tvalid_loss :0.004778478294610977\n",
      "**********Valid loss decreased (0.004911 ==> 0.004778)**********\n",
      "Adjusting learning rate of group 0 to 7.6584e-03.\n",
      "Adjusting learning rate of group 0 to 7.6584e-03.\n",
      "Epoch : 264,\ttrain_loss : 0.007327385246753693\tvalid_loss :0.00822623074054718\n",
      "Adjusting learning rate of group 0 to 7.6559e-03.\n",
      "Adjusting learning rate of group 0 to 7.6559e-03.\n",
      "Epoch : 265,\ttrain_loss : 0.008379128761589527\tvalid_loss :0.008873927406966686\n",
      "Adjusting learning rate of group 0 to 7.6533e-03.\n",
      "Adjusting learning rate of group 0 to 7.6533e-03.\n",
      "Epoch : 266,\ttrain_loss : 0.008730370551347733\tvalid_loss :0.007524810265749693\n",
      "Adjusting learning rate of group 0 to 7.6508e-03.\n",
      "Adjusting learning rate of group 0 to 7.6508e-03.\n",
      "Epoch : 267,\ttrain_loss : 0.0077764177694916725\tvalid_loss :0.0063117435202002525\n",
      "Adjusting learning rate of group 0 to 7.6482e-03.\n",
      "Adjusting learning rate of group 0 to 7.6482e-03.\n",
      "Epoch : 268,\ttrain_loss : 0.007752580102533102\tvalid_loss :0.005111688748002052\n",
      "Adjusting learning rate of group 0 to 7.6456e-03.\n",
      "Adjusting learning rate of group 0 to 7.6456e-03.\n",
      "Epoch : 269,\ttrain_loss : 0.007586649153381586\tvalid_loss :0.005572114139795303\n",
      "Adjusting learning rate of group 0 to 7.6430e-03.\n",
      "Adjusting learning rate of group 0 to 7.6430e-03.\n",
      "Epoch : 270,\ttrain_loss : 0.007237584795802832\tvalid_loss :0.009296600706875324\n",
      "Adjusting learning rate of group 0 to 7.6404e-03.\n",
      "Adjusting learning rate of group 0 to 7.6404e-03.\n",
      "Epoch : 271,\ttrain_loss : 0.00785332452505827\tvalid_loss :0.00637727091088891\n",
      "Adjusting learning rate of group 0 to 7.6378e-03.\n",
      "Adjusting learning rate of group 0 to 7.6378e-03.\n",
      "Epoch : 272,\ttrain_loss : 0.00732466159388423\tvalid_loss :0.005432317964732647\n",
      "Adjusting learning rate of group 0 to 7.6352e-03.\n",
      "Adjusting learning rate of group 0 to 7.6352e-03.\n",
      "Epoch : 273,\ttrain_loss : 0.0070978254079818726\tvalid_loss :0.0075138043612241745\n",
      "Adjusting learning rate of group 0 to 7.6326e-03.\n",
      "Adjusting learning rate of group 0 to 7.6326e-03.\n",
      "Epoch : 274,\ttrain_loss : 0.007307504769414663\tvalid_loss :0.010127035900950432\n",
      "Adjusting learning rate of group 0 to 7.6299e-03.\n",
      "Adjusting learning rate of group 0 to 7.6299e-03.\n",
      "Epoch : 275,\ttrain_loss : 0.00869583897292614\tvalid_loss :0.00519332941621542\n",
      "Adjusting learning rate of group 0 to 7.6273e-03.\n",
      "Adjusting learning rate of group 0 to 7.6273e-03.\n",
      "Epoch : 276,\ttrain_loss : 0.00766561646014452\tvalid_loss :0.005073909647762775\n",
      "Adjusting learning rate of group 0 to 7.6246e-03.\n",
      "Adjusting learning rate of group 0 to 7.6246e-03.\n",
      "Epoch : 277,\ttrain_loss : 0.007125421427190304\tvalid_loss :0.005380063317716122\n",
      "Adjusting learning rate of group 0 to 7.6220e-03.\n",
      "Adjusting learning rate of group 0 to 7.6220e-03.\n",
      "Epoch : 278,\ttrain_loss : 0.007934154011309147\tvalid_loss :0.011960449628531933\n",
      "Adjusting learning rate of group 0 to 7.6193e-03.\n",
      "Adjusting learning rate of group 0 to 7.6193e-03.\n",
      "Epoch : 279,\ttrain_loss : 0.008572817780077457\tvalid_loss :0.006996924057602882\n",
      "Adjusting learning rate of group 0 to 7.6166e-03.\n",
      "Adjusting learning rate of group 0 to 7.6166e-03.\n",
      "Epoch : 280,\ttrain_loss : 0.007338128983974457\tvalid_loss :0.009545394219458103\n",
      "Adjusting learning rate of group 0 to 7.6139e-03.\n",
      "Adjusting learning rate of group 0 to 7.6139e-03.\n",
      "Epoch : 281,\ttrain_loss : 0.007979679852724075\tvalid_loss :0.008455980569124222\n",
      "Adjusting learning rate of group 0 to 7.6112e-03.\n",
      "Adjusting learning rate of group 0 to 7.6112e-03.\n",
      "Epoch : 282,\ttrain_loss : 0.0085305692628026\tvalid_loss :0.005180487409234047\n",
      "Adjusting learning rate of group 0 to 7.6085e-03.\n",
      "Adjusting learning rate of group 0 to 7.6085e-03.\n",
      "Epoch : 283,\ttrain_loss : 0.008109594695270061\tvalid_loss :0.006495345383882523\n",
      "Adjusting learning rate of group 0 to 7.6058e-03.\n",
      "Adjusting learning rate of group 0 to 7.6058e-03.\n",
      "Epoch : 284,\ttrain_loss : 0.007363732438534498\tvalid_loss :0.01071356050670147\n",
      "Adjusting learning rate of group 0 to 7.6031e-03.\n",
      "Adjusting learning rate of group 0 to 7.6031e-03.\n",
      "Epoch : 285,\ttrain_loss : 0.008716012351214886\tvalid_loss :0.007577977143228054\n",
      "Adjusting learning rate of group 0 to 7.6004e-03.\n",
      "Adjusting learning rate of group 0 to 7.6004e-03.\n",
      "Epoch : 286,\ttrain_loss : 0.007385718636214733\tvalid_loss :0.004795651417225599\n",
      "Adjusting learning rate of group 0 to 7.5976e-03.\n",
      "Adjusting learning rate of group 0 to 7.5976e-03.\n",
      "Epoch : 287,\ttrain_loss : 0.007957071997225285\tvalid_loss :0.009470116347074509\n",
      "Adjusting learning rate of group 0 to 7.5949e-03.\n",
      "Adjusting learning rate of group 0 to 7.5949e-03.\n",
      "Epoch : 288,\ttrain_loss : 0.008306586183607578\tvalid_loss :0.00928362738341093\n",
      "Adjusting learning rate of group 0 to 7.5921e-03.\n",
      "Adjusting learning rate of group 0 to 7.5921e-03.\n",
      "Epoch : 289,\ttrain_loss : 0.00803215242922306\tvalid_loss :0.0060403249226510525\n",
      "Adjusting learning rate of group 0 to 7.5893e-03.\n",
      "Adjusting learning rate of group 0 to 7.5893e-03.\n",
      "Epoch : 290,\ttrain_loss : 0.007249289657920599\tvalid_loss :0.005903142504394054\n",
      "Adjusting learning rate of group 0 to 7.5866e-03.\n",
      "Adjusting learning rate of group 0 to 7.5866e-03.\n",
      "Epoch : 291,\ttrain_loss : 0.007399801164865494\tvalid_loss :0.005038627423346043\n",
      "Adjusting learning rate of group 0 to 7.5838e-03.\n",
      "Adjusting learning rate of group 0 to 7.5838e-03.\n",
      "Epoch : 292,\ttrain_loss : 0.007477832026779652\tvalid_loss :0.00986743625253439\n",
      "Adjusting learning rate of group 0 to 7.5810e-03.\n",
      "Adjusting learning rate of group 0 to 7.5810e-03.\n",
      "Epoch : 293,\ttrain_loss : 0.008657874539494514\tvalid_loss :0.008084786124527454\n",
      "Adjusting learning rate of group 0 to 7.5782e-03.\n",
      "Adjusting learning rate of group 0 to 7.5782e-03.\n",
      "Epoch : 294,\ttrain_loss : 0.007879712618887424\tvalid_loss :0.007825778797268867\n",
      "Adjusting learning rate of group 0 to 7.5754e-03.\n",
      "Adjusting learning rate of group 0 to 7.5754e-03.\n",
      "Epoch : 295,\ttrain_loss : 0.007517592050135136\tvalid_loss :0.006238729227334261\n",
      "Adjusting learning rate of group 0 to 7.5725e-03.\n",
      "Adjusting learning rate of group 0 to 7.5725e-03.\n",
      "Epoch : 296,\ttrain_loss : 0.007333699613809586\tvalid_loss :0.007544012740254402\n",
      "Adjusting learning rate of group 0 to 7.5697e-03.\n",
      "Adjusting learning rate of group 0 to 7.5697e-03.\n",
      "Epoch : 297,\ttrain_loss : 0.007221634034067392\tvalid_loss :0.010344702750444412\n",
      "Adjusting learning rate of group 0 to 7.5669e-03.\n",
      "Adjusting learning rate of group 0 to 7.5669e-03.\n",
      "Epoch : 298,\ttrain_loss : 0.008629540912806988\tvalid_loss :0.006919087376445532\n",
      "Adjusting learning rate of group 0 to 7.5640e-03.\n",
      "Adjusting learning rate of group 0 to 7.5640e-03.\n",
      "Epoch : 299,\ttrain_loss : 0.007805648259818554\tvalid_loss :0.008735667914152145\n",
      "Adjusting learning rate of group 0 to 7.5612e-03.\n",
      "Adjusting learning rate of group 0 to 7.5612e-03.\n",
      "Epoch : 300,\ttrain_loss : 0.0076080686412751675\tvalid_loss :0.010377475991845131\n",
      "Adjusting learning rate of group 0 to 7.5583e-03.\n",
      "Adjusting learning rate of group 0 to 7.5583e-03.\n",
      "Epoch : 301,\ttrain_loss : 0.008960354141891003\tvalid_loss :0.006354966200888157\n",
      "Adjusting learning rate of group 0 to 7.5554e-03.\n",
      "Adjusting learning rate of group 0 to 7.5554e-03.\n",
      "Epoch : 302,\ttrain_loss : 0.007264621090143919\tvalid_loss :0.005445253569632769\n",
      "Adjusting learning rate of group 0 to 7.5525e-03.\n",
      "Adjusting learning rate of group 0 to 7.5525e-03.\n",
      "Epoch : 303,\ttrain_loss : 0.0072578866966068745\tvalid_loss :0.005013367626816034\n",
      "Adjusting learning rate of group 0 to 7.5497e-03.\n",
      "Adjusting learning rate of group 0 to 7.5497e-03.\n",
      "Epoch : 304,\ttrain_loss : 0.007132020778954029\tvalid_loss :0.005416817031800747\n",
      "Adjusting learning rate of group 0 to 7.5468e-03.\n",
      "Adjusting learning rate of group 0 to 7.5468e-03.\n",
      "Epoch : 305,\ttrain_loss : 0.007121238857507706\tvalid_loss :0.009046876803040504\n",
      "Adjusting learning rate of group 0 to 7.5438e-03.\n",
      "Adjusting learning rate of group 0 to 7.5438e-03.\n",
      "Epoch : 306,\ttrain_loss : 0.008837013505399227\tvalid_loss :0.006006977055221796\n",
      "Adjusting learning rate of group 0 to 7.5409e-03.\n",
      "Adjusting learning rate of group 0 to 7.5409e-03.\n",
      "Epoch : 307,\ttrain_loss : 0.007606047671288252\tvalid_loss :0.006452761124819517\n",
      "Adjusting learning rate of group 0 to 7.5380e-03.\n",
      "Adjusting learning rate of group 0 to 7.5380e-03.\n",
      "Epoch : 308,\ttrain_loss : 0.006997022777795792\tvalid_loss :0.010242819786071777\n",
      "Adjusting learning rate of group 0 to 7.5351e-03.\n",
      "Adjusting learning rate of group 0 to 7.5351e-03.\n",
      "Epoch : 309,\ttrain_loss : 0.008648036047816277\tvalid_loss :0.0060376981273293495\n",
      "Adjusting learning rate of group 0 to 7.5321e-03.\n",
      "Adjusting learning rate of group 0 to 7.5321e-03.\n",
      "Epoch : 310,\ttrain_loss : 0.008418184705078602\tvalid_loss :0.005726745817810297\n",
      "Adjusting learning rate of group 0 to 7.5292e-03.\n",
      "Adjusting learning rate of group 0 to 7.5292e-03.\n",
      "Epoch : 311,\ttrain_loss : 0.007738972548395395\tvalid_loss :0.0059501095674932\n",
      "Adjusting learning rate of group 0 to 7.5262e-03.\n",
      "Adjusting learning rate of group 0 to 7.5262e-03.\n",
      "Epoch : 312,\ttrain_loss : 0.0069435168989002705\tvalid_loss :0.004600765649229288\n",
      "**********Valid loss decreased (0.004778 ==> 0.004601)**********\n",
      "Adjusting learning rate of group 0 to 7.5232e-03.\n",
      "Adjusting learning rate of group 0 to 7.5232e-03.\n",
      "Epoch : 313,\ttrain_loss : 0.0069609032943844795\tvalid_loss :0.008891893550753593\n",
      "Adjusting learning rate of group 0 to 7.5203e-03.\n",
      "Adjusting learning rate of group 0 to 7.5203e-03.\n",
      "Epoch : 314,\ttrain_loss : 0.008089822717010975\tvalid_loss :0.009593158029019833\n",
      "Adjusting learning rate of group 0 to 7.5173e-03.\n",
      "Adjusting learning rate of group 0 to 7.5173e-03.\n",
      "Epoch : 315,\ttrain_loss : 0.009026533924043179\tvalid_loss :0.005375548265874386\n",
      "Adjusting learning rate of group 0 to 7.5143e-03.\n",
      "Adjusting learning rate of group 0 to 7.5143e-03.\n",
      "Epoch : 316,\ttrain_loss : 0.008185048587620258\tvalid_loss :0.005963984876871109\n",
      "Adjusting learning rate of group 0 to 7.5113e-03.\n",
      "Adjusting learning rate of group 0 to 7.5113e-03.\n",
      "Epoch : 317,\ttrain_loss : 0.0068889581598341465\tvalid_loss :0.00948776863515377\n",
      "Adjusting learning rate of group 0 to 7.5082e-03.\n",
      "Adjusting learning rate of group 0 to 7.5082e-03.\n",
      "Epoch : 318,\ttrain_loss : 0.008414292708039284\tvalid_loss :0.00914929248392582\n",
      "Adjusting learning rate of group 0 to 7.5052e-03.\n",
      "Adjusting learning rate of group 0 to 7.5052e-03.\n",
      "Epoch : 319,\ttrain_loss : 0.007482209708541632\tvalid_loss :0.00493460800498724\n",
      "Adjusting learning rate of group 0 to 7.5022e-03.\n",
      "Adjusting learning rate of group 0 to 7.5022e-03.\n",
      "Epoch : 320,\ttrain_loss : 0.006938875652849674\tvalid_loss :0.005469766911119223\n",
      "Adjusting learning rate of group 0 to 7.4992e-03.\n",
      "Adjusting learning rate of group 0 to 7.4992e-03.\n",
      "Epoch : 321,\ttrain_loss : 0.006650017108768225\tvalid_loss :0.005987264681607485\n",
      "Adjusting learning rate of group 0 to 7.4961e-03.\n",
      "Adjusting learning rate of group 0 to 7.4961e-03.\n",
      "Epoch : 322,\ttrain_loss : 0.008239621296525002\tvalid_loss :0.009396028704941273\n",
      "Adjusting learning rate of group 0 to 7.4930e-03.\n",
      "Adjusting learning rate of group 0 to 7.4930e-03.\n",
      "Epoch : 323,\ttrain_loss : 0.007546048145741224\tvalid_loss :0.006886267103254795\n",
      "Adjusting learning rate of group 0 to 7.4900e-03.\n",
      "Adjusting learning rate of group 0 to 7.4900e-03.\n",
      "Epoch : 324,\ttrain_loss : 0.007003479171544313\tvalid_loss :0.009601667523384094\n",
      "Adjusting learning rate of group 0 to 7.4869e-03.\n",
      "Adjusting learning rate of group 0 to 7.4869e-03.\n",
      "Epoch : 325,\ttrain_loss : 0.008260434493422508\tvalid_loss :0.007236930541694164\n",
      "Adjusting learning rate of group 0 to 7.4838e-03.\n",
      "Adjusting learning rate of group 0 to 7.4838e-03.\n",
      "Epoch : 326,\ttrain_loss : 0.008525206707417965\tvalid_loss :0.007704935036599636\n",
      "Adjusting learning rate of group 0 to 7.4807e-03.\n",
      "Adjusting learning rate of group 0 to 7.4807e-03.\n",
      "Epoch : 327,\ttrain_loss : 0.009292728267610073\tvalid_loss :0.006600826047360897\n",
      "Adjusting learning rate of group 0 to 7.4776e-03.\n",
      "Adjusting learning rate of group 0 to 7.4776e-03.\n",
      "Epoch : 328,\ttrain_loss : 0.008832243271172047\tvalid_loss :0.007442146074026823\n",
      "Adjusting learning rate of group 0 to 7.4745e-03.\n",
      "Adjusting learning rate of group 0 to 7.4745e-03.\n",
      "Epoch : 329,\ttrain_loss : 0.0072710635140538216\tvalid_loss :0.009228799492120743\n",
      "Adjusting learning rate of group 0 to 7.4714e-03.\n",
      "Adjusting learning rate of group 0 to 7.4714e-03.\n",
      "Epoch : 330,\ttrain_loss : 0.008327296935021877\tvalid_loss :0.00940694473683834\n",
      "Adjusting learning rate of group 0 to 7.4683e-03.\n",
      "Adjusting learning rate of group 0 to 7.4683e-03.\n",
      "Epoch : 331,\ttrain_loss : 0.007724856026470661\tvalid_loss :0.004874314181506634\n",
      "Adjusting learning rate of group 0 to 7.4651e-03.\n",
      "Adjusting learning rate of group 0 to 7.4651e-03.\n",
      "Epoch : 332,\ttrain_loss : 0.006865871604532003\tvalid_loss :0.006320079788565636\n",
      "Adjusting learning rate of group 0 to 7.4620e-03.\n",
      "Adjusting learning rate of group 0 to 7.4620e-03.\n",
      "Epoch : 333,\ttrain_loss : 0.007999541237950325\tvalid_loss :0.005220863036811352\n",
      "Adjusting learning rate of group 0 to 7.4589e-03.\n",
      "Adjusting learning rate of group 0 to 7.4589e-03.\n",
      "Epoch : 334,\ttrain_loss : 0.009170607663691044\tvalid_loss :0.009963613003492355\n",
      "Adjusting learning rate of group 0 to 7.4557e-03.\n",
      "Adjusting learning rate of group 0 to 7.4557e-03.\n",
      "Epoch : 335,\ttrain_loss : 0.010964426212012768\tvalid_loss :0.005527237895876169\n",
      "Adjusting learning rate of group 0 to 7.4525e-03.\n",
      "Adjusting learning rate of group 0 to 7.4525e-03.\n",
      "Epoch : 336,\ttrain_loss : 0.0073631140403449535\tvalid_loss :0.004916079342365265\n",
      "Adjusting learning rate of group 0 to 7.4493e-03.\n",
      "Adjusting learning rate of group 0 to 7.4493e-03.\n",
      "Epoch : 337,\ttrain_loss : 0.007279880810528994\tvalid_loss :0.00513946358114481\n",
      "Adjusting learning rate of group 0 to 7.4462e-03.\n",
      "Adjusting learning rate of group 0 to 7.4462e-03.\n",
      "Epoch : 338,\ttrain_loss : 0.007316716481000185\tvalid_loss :0.005983799695968628\n",
      "Adjusting learning rate of group 0 to 7.4430e-03.\n",
      "Adjusting learning rate of group 0 to 7.4430e-03.\n",
      "Epoch : 339,\ttrain_loss : 0.007891211658716202\tvalid_loss :0.009765892289578915\n",
      "Adjusting learning rate of group 0 to 7.4398e-03.\n",
      "Adjusting learning rate of group 0 to 7.4398e-03.\n",
      "Epoch : 340,\ttrain_loss : 0.008141407743096352\tvalid_loss :0.00555081944912672\n",
      "Adjusting learning rate of group 0 to 7.4366e-03.\n",
      "Adjusting learning rate of group 0 to 7.4366e-03.\n",
      "Epoch : 341,\ttrain_loss : 0.007018716540187597\tvalid_loss :0.009992698207497597\n",
      "Adjusting learning rate of group 0 to 7.4333e-03.\n",
      "Adjusting learning rate of group 0 to 7.4333e-03.\n",
      "Epoch : 342,\ttrain_loss : 0.008519052527844906\tvalid_loss :0.008459596894681454\n",
      "Adjusting learning rate of group 0 to 7.4301e-03.\n",
      "Adjusting learning rate of group 0 to 7.4301e-03.\n",
      "Epoch : 343,\ttrain_loss : 0.007112422492355108\tvalid_loss :0.005263621918857098\n",
      "Adjusting learning rate of group 0 to 7.4269e-03.\n",
      "Adjusting learning rate of group 0 to 7.4269e-03.\n",
      "Epoch : 344,\ttrain_loss : 0.006866729352623224\tvalid_loss :0.005478224717080593\n",
      "Adjusting learning rate of group 0 to 7.4236e-03.\n",
      "Adjusting learning rate of group 0 to 7.4236e-03.\n",
      "Epoch : 345,\ttrain_loss : 0.006688840687274933\tvalid_loss :0.005603162571787834\n",
      "Adjusting learning rate of group 0 to 7.4204e-03.\n",
      "Adjusting learning rate of group 0 to 7.4204e-03.\n",
      "Epoch : 346,\ttrain_loss : 0.007016003131866455\tvalid_loss :0.005257752723991871\n",
      "Adjusting learning rate of group 0 to 7.4171e-03.\n",
      "Adjusting learning rate of group 0 to 7.4171e-03.\n",
      "Epoch : 347,\ttrain_loss : 0.008025567047297955\tvalid_loss :0.005152254365384579\n",
      "Adjusting learning rate of group 0 to 7.4138e-03.\n",
      "Adjusting learning rate of group 0 to 7.4138e-03.\n",
      "Epoch : 348,\ttrain_loss : 0.007427506148815155\tvalid_loss :0.008279982022941113\n",
      "Adjusting learning rate of group 0 to 7.4106e-03.\n",
      "Adjusting learning rate of group 0 to 7.4106e-03.\n",
      "Epoch : 349,\ttrain_loss : 0.007180613465607166\tvalid_loss :0.004824405070394278\n",
      "Adjusting learning rate of group 0 to 7.4073e-03.\n",
      "Adjusting learning rate of group 0 to 7.4073e-03.\n",
      "Epoch : 350,\ttrain_loss : 0.0072034671902656555\tvalid_loss :0.005067260004580021\n",
      "Adjusting learning rate of group 0 to 7.4040e-03.\n",
      "Adjusting learning rate of group 0 to 7.4040e-03.\n",
      "Epoch : 351,\ttrain_loss : 0.007589682005345821\tvalid_loss :0.004941571969538927\n",
      "Adjusting learning rate of group 0 to 7.4007e-03.\n",
      "Adjusting learning rate of group 0 to 7.4007e-03.\n",
      "Epoch : 352,\ttrain_loss : 0.006963271647691727\tvalid_loss :0.005098741501569748\n",
      "Adjusting learning rate of group 0 to 7.3974e-03.\n",
      "Adjusting learning rate of group 0 to 7.3974e-03.\n",
      "Epoch : 353,\ttrain_loss : 0.007498319260776043\tvalid_loss :0.010767921805381775\n",
      "Adjusting learning rate of group 0 to 7.3940e-03.\n",
      "Adjusting learning rate of group 0 to 7.3940e-03.\n",
      "Epoch : 354,\ttrain_loss : 0.00844288244843483\tvalid_loss :0.007943622767925262\n",
      "Adjusting learning rate of group 0 to 7.3907e-03.\n",
      "Adjusting learning rate of group 0 to 7.3907e-03.\n",
      "Epoch : 355,\ttrain_loss : 0.007466552313417196\tvalid_loss :0.006482405122369528\n",
      "Adjusting learning rate of group 0 to 7.3874e-03.\n",
      "Adjusting learning rate of group 0 to 7.3874e-03.\n",
      "Epoch : 356,\ttrain_loss : 0.007968426682054996\tvalid_loss :0.00516264233738184\n",
      "Adjusting learning rate of group 0 to 7.3840e-03.\n",
      "Adjusting learning rate of group 0 to 7.3840e-03.\n",
      "Epoch : 357,\ttrain_loss : 0.007152216043323278\tvalid_loss :0.00839308649301529\n",
      "Adjusting learning rate of group 0 to 7.3807e-03.\n",
      "Adjusting learning rate of group 0 to 7.3807e-03.\n",
      "Epoch : 358,\ttrain_loss : 0.007844074629247189\tvalid_loss :0.006028288509696722\n",
      "Adjusting learning rate of group 0 to 7.3773e-03.\n",
      "Adjusting learning rate of group 0 to 7.3773e-03.\n",
      "Epoch : 359,\ttrain_loss : 0.007019213400781155\tvalid_loss :0.005968150682747364\n",
      "Adjusting learning rate of group 0 to 7.3739e-03.\n",
      "Adjusting learning rate of group 0 to 7.3739e-03.\n",
      "Epoch : 360,\ttrain_loss : 0.007533195894211531\tvalid_loss :0.008119281381368637\n",
      "Adjusting learning rate of group 0 to 7.3706e-03.\n",
      "Adjusting learning rate of group 0 to 7.3706e-03.\n",
      "Epoch : 361,\ttrain_loss : 0.008362914435565472\tvalid_loss :0.00895002018660307\n",
      "Adjusting learning rate of group 0 to 7.3672e-03.\n",
      "Adjusting learning rate of group 0 to 7.3672e-03.\n",
      "Epoch : 362,\ttrain_loss : 0.007385612931102514\tvalid_loss :0.007311214692890644\n",
      "Adjusting learning rate of group 0 to 7.3638e-03.\n",
      "Adjusting learning rate of group 0 to 7.3638e-03.\n",
      "Epoch : 363,\ttrain_loss : 0.00880222488194704\tvalid_loss :0.009457563981413841\n",
      "Adjusting learning rate of group 0 to 7.3604e-03.\n",
      "Adjusting learning rate of group 0 to 7.3604e-03.\n",
      "Epoch : 364,\ttrain_loss : 0.007967769168317318\tvalid_loss :0.006506721489131451\n",
      "Adjusting learning rate of group 0 to 7.3570e-03.\n",
      "Adjusting learning rate of group 0 to 7.3570e-03.\n",
      "Epoch : 365,\ttrain_loss : 0.006924182176589966\tvalid_loss :0.007661275565624237\n",
      "Adjusting learning rate of group 0 to 7.3535e-03.\n",
      "Adjusting learning rate of group 0 to 7.3535e-03.\n",
      "Epoch : 366,\ttrain_loss : 0.007851837202906609\tvalid_loss :0.006143154110759497\n",
      "Adjusting learning rate of group 0 to 7.3501e-03.\n",
      "Adjusting learning rate of group 0 to 7.3501e-03.\n",
      "Epoch : 367,\ttrain_loss : 0.007329657673835754\tvalid_loss :0.00781848281621933\n",
      "Adjusting learning rate of group 0 to 7.3467e-03.\n",
      "Adjusting learning rate of group 0 to 7.3467e-03.\n",
      "Epoch : 368,\ttrain_loss : 0.007636391092091799\tvalid_loss :0.004550171550363302\n",
      "**********Valid loss decreased (0.004601 ==> 0.004550)**********\n",
      "Adjusting learning rate of group 0 to 7.3432e-03.\n",
      "Adjusting learning rate of group 0 to 7.3432e-03.\n",
      "Epoch : 369,\ttrain_loss : 0.006882511079311371\tvalid_loss :0.005239384714514017\n",
      "Adjusting learning rate of group 0 to 7.3398e-03.\n",
      "Adjusting learning rate of group 0 to 7.3398e-03.\n",
      "Epoch : 370,\ttrain_loss : 0.00685768062248826\tvalid_loss :0.01007043942809105\n",
      "Adjusting learning rate of group 0 to 7.3363e-03.\n",
      "Adjusting learning rate of group 0 to 7.3363e-03.\n",
      "Epoch : 371,\ttrain_loss : 0.008001862093806267\tvalid_loss :0.0048145717009902\n",
      "Adjusting learning rate of group 0 to 7.3328e-03.\n",
      "Adjusting learning rate of group 0 to 7.3328e-03.\n",
      "Epoch : 372,\ttrain_loss : 0.0074499319307506084\tvalid_loss :0.008318983018398285\n",
      "Adjusting learning rate of group 0 to 7.3294e-03.\n",
      "Adjusting learning rate of group 0 to 7.3294e-03.\n",
      "Epoch : 373,\ttrain_loss : 0.00777846947312355\tvalid_loss :0.009433272294700146\n",
      "Adjusting learning rate of group 0 to 7.3259e-03.\n",
      "Adjusting learning rate of group 0 to 7.3259e-03.\n",
      "Epoch : 374,\ttrain_loss : 0.008094134740531445\tvalid_loss :0.00770032312721014\n",
      "Adjusting learning rate of group 0 to 7.3224e-03.\n",
      "Adjusting learning rate of group 0 to 7.3224e-03.\n",
      "Epoch : 375,\ttrain_loss : 0.007389585021883249\tvalid_loss :0.008946401998400688\n",
      "Adjusting learning rate of group 0 to 7.3189e-03.\n",
      "Adjusting learning rate of group 0 to 7.3189e-03.\n",
      "Epoch : 376,\ttrain_loss : 0.008168346248567104\tvalid_loss :0.009951259940862656\n",
      "Adjusting learning rate of group 0 to 7.3154e-03.\n",
      "Adjusting learning rate of group 0 to 7.3154e-03.\n",
      "Epoch : 377,\ttrain_loss : 0.007469405420124531\tvalid_loss :0.006842887960374355\n",
      "Adjusting learning rate of group 0 to 7.3118e-03.\n",
      "Adjusting learning rate of group 0 to 7.3118e-03.\n",
      "Epoch : 378,\ttrain_loss : 0.006885840557515621\tvalid_loss :0.004917063284665346\n",
      "Adjusting learning rate of group 0 to 7.3083e-03.\n",
      "Adjusting learning rate of group 0 to 7.3083e-03.\n",
      "Epoch : 379,\ttrain_loss : 0.007333326153457165\tvalid_loss :0.004930976778268814\n",
      "Adjusting learning rate of group 0 to 7.3048e-03.\n",
      "Adjusting learning rate of group 0 to 7.3048e-03.\n",
      "Epoch : 380,\ttrain_loss : 0.007170477416366339\tvalid_loss :0.005562404170632362\n",
      "Adjusting learning rate of group 0 to 7.3012e-03.\n",
      "Adjusting learning rate of group 0 to 7.3012e-03.\n",
      "Epoch : 381,\ttrain_loss : 0.00702511053532362\tvalid_loss :0.004850757773965597\n",
      "Adjusting learning rate of group 0 to 7.2977e-03.\n",
      "Adjusting learning rate of group 0 to 7.2977e-03.\n",
      "Epoch : 382,\ttrain_loss : 0.007041011471301317\tvalid_loss :0.006193163339048624\n",
      "Adjusting learning rate of group 0 to 7.2941e-03.\n",
      "Adjusting learning rate of group 0 to 7.2941e-03.\n",
      "Epoch : 383,\ttrain_loss : 0.0069976202212274075\tvalid_loss :0.005869352724403143\n",
      "Adjusting learning rate of group 0 to 7.2906e-03.\n",
      "Adjusting learning rate of group 0 to 7.2906e-03.\n",
      "Epoch : 384,\ttrain_loss : 0.007091215346008539\tvalid_loss :0.006505648605525494\n",
      "Adjusting learning rate of group 0 to 7.2870e-03.\n",
      "Adjusting learning rate of group 0 to 7.2870e-03.\n",
      "Epoch : 385,\ttrain_loss : 0.007484795060008764\tvalid_loss :0.004533442202955484\n",
      "**********Valid loss decreased (0.004550 ==> 0.004533)**********\n",
      "Adjusting learning rate of group 0 to 7.2834e-03.\n",
      "Adjusting learning rate of group 0 to 7.2834e-03.\n",
      "Epoch : 386,\ttrain_loss : 0.006971344351768494\tvalid_loss :0.006778319366276264\n",
      "Adjusting learning rate of group 0 to 7.2798e-03.\n",
      "Adjusting learning rate of group 0 to 7.2798e-03.\n",
      "Epoch : 387,\ttrain_loss : 0.007592230569571257\tvalid_loss :0.0061394767835736275\n",
      "Adjusting learning rate of group 0 to 7.2762e-03.\n",
      "Adjusting learning rate of group 0 to 7.2762e-03.\n",
      "Epoch : 388,\ttrain_loss : 0.006903181318193674\tvalid_loss :0.0045437938533723354\n",
      "Adjusting learning rate of group 0 to 7.2726e-03.\n",
      "Adjusting learning rate of group 0 to 7.2726e-03.\n",
      "Epoch : 389,\ttrain_loss : 0.006717039737850428\tvalid_loss :0.0065652476623654366\n",
      "Adjusting learning rate of group 0 to 7.2690e-03.\n",
      "Adjusting learning rate of group 0 to 7.2690e-03.\n",
      "Epoch : 390,\ttrain_loss : 0.0069699459709227085\tvalid_loss :0.0073110321536660194\n",
      "Adjusting learning rate of group 0 to 7.2654e-03.\n",
      "Adjusting learning rate of group 0 to 7.2654e-03.\n",
      "Epoch : 391,\ttrain_loss : 0.0072126309387385845\tvalid_loss :0.008019658736884594\n",
      "Adjusting learning rate of group 0 to 7.2617e-03.\n",
      "Adjusting learning rate of group 0 to 7.2617e-03.\n",
      "Epoch : 392,\ttrain_loss : 0.008192114531993866\tvalid_loss :0.005788039416074753\n",
      "Adjusting learning rate of group 0 to 7.2581e-03.\n",
      "Adjusting learning rate of group 0 to 7.2581e-03.\n",
      "Epoch : 393,\ttrain_loss : 0.007676092442125082\tvalid_loss :0.004945945926010609\n",
      "Adjusting learning rate of group 0 to 7.2544e-03.\n",
      "Adjusting learning rate of group 0 to 7.2544e-03.\n",
      "Epoch : 394,\ttrain_loss : 0.007294492330402136\tvalid_loss :0.004844189155846834\n",
      "Adjusting learning rate of group 0 to 7.2508e-03.\n",
      "Adjusting learning rate of group 0 to 7.2508e-03.\n",
      "Epoch : 395,\ttrain_loss : 0.007450978737324476\tvalid_loss :0.007324179634451866\n",
      "Adjusting learning rate of group 0 to 7.2471e-03.\n",
      "Adjusting learning rate of group 0 to 7.2471e-03.\n",
      "Epoch : 396,\ttrain_loss : 0.008258035406470299\tvalid_loss :0.0047887032851576805\n",
      "Adjusting learning rate of group 0 to 7.2434e-03.\n",
      "Adjusting learning rate of group 0 to 7.2434e-03.\n",
      "Epoch : 397,\ttrain_loss : 0.007284229155629873\tvalid_loss :0.004956427030265331\n",
      "Adjusting learning rate of group 0 to 7.2398e-03.\n",
      "Adjusting learning rate of group 0 to 7.2398e-03.\n",
      "Epoch : 398,\ttrain_loss : 0.007104041986167431\tvalid_loss :0.0074129411950707436\n",
      "Adjusting learning rate of group 0 to 7.2361e-03.\n",
      "Adjusting learning rate of group 0 to 7.2361e-03.\n",
      "Epoch : 399,\ttrain_loss : 0.007302442099899054\tvalid_loss :0.005329102277755737\n",
      "Adjusting learning rate of group 0 to 7.2324e-03.\n",
      "Adjusting learning rate of group 0 to 7.2324e-03.\n",
      "Epoch : 400,\ttrain_loss : 0.007594249211251736\tvalid_loss :0.005735410843044519\n",
      "Adjusting learning rate of group 0 to 7.2287e-03.\n",
      "Adjusting learning rate of group 0 to 7.2287e-03.\n",
      "Epoch : 401,\ttrain_loss : 0.007532202638685703\tvalid_loss :0.007248031906783581\n",
      "Adjusting learning rate of group 0 to 7.2250e-03.\n",
      "Adjusting learning rate of group 0 to 7.2250e-03.\n",
      "Epoch : 402,\ttrain_loss : 0.007144541945308447\tvalid_loss :0.005917898844927549\n",
      "Adjusting learning rate of group 0 to 7.2212e-03.\n",
      "Adjusting learning rate of group 0 to 7.2212e-03.\n",
      "Epoch : 403,\ttrain_loss : 0.007467092480510473\tvalid_loss :0.005551905836910009\n",
      "Adjusting learning rate of group 0 to 7.2175e-03.\n",
      "Adjusting learning rate of group 0 to 7.2175e-03.\n",
      "Epoch : 404,\ttrain_loss : 0.0073065683245658875\tvalid_loss :0.006298539228737354\n",
      "Adjusting learning rate of group 0 to 7.2138e-03.\n",
      "Adjusting learning rate of group 0 to 7.2138e-03.\n",
      "Epoch : 405,\ttrain_loss : 0.008278227411210537\tvalid_loss :0.005245568230748177\n",
      "Adjusting learning rate of group 0 to 7.2100e-03.\n",
      "Adjusting learning rate of group 0 to 7.2100e-03.\n",
      "Epoch : 406,\ttrain_loss : 0.007275682408362627\tvalid_loss :0.00841076485812664\n",
      "Adjusting learning rate of group 0 to 7.2063e-03.\n",
      "Adjusting learning rate of group 0 to 7.2063e-03.\n",
      "Epoch : 407,\ttrain_loss : 0.008719043806195259\tvalid_loss :0.006287253461778164\n",
      "Adjusting learning rate of group 0 to 7.2025e-03.\n",
      "Adjusting learning rate of group 0 to 7.2025e-03.\n",
      "Epoch : 408,\ttrain_loss : 0.007418784778565168\tvalid_loss :0.005147544667124748\n",
      "Adjusting learning rate of group 0 to 7.1987e-03.\n",
      "Adjusting learning rate of group 0 to 7.1987e-03.\n",
      "Epoch : 409,\ttrain_loss : 0.007699982263147831\tvalid_loss :0.00503362575545907\n",
      "Adjusting learning rate of group 0 to 7.1950e-03.\n",
      "Adjusting learning rate of group 0 to 7.1950e-03.\n",
      "Epoch : 410,\ttrain_loss : 0.007322969846427441\tvalid_loss :0.007115775253623724\n",
      "Adjusting learning rate of group 0 to 7.1912e-03.\n",
      "Adjusting learning rate of group 0 to 7.1912e-03.\n",
      "Epoch : 411,\ttrain_loss : 0.006787266116589308\tvalid_loss :0.004649265669286251\n",
      "Adjusting learning rate of group 0 to 7.1874e-03.\n",
      "Adjusting learning rate of group 0 to 7.1874e-03.\n",
      "Epoch : 412,\ttrain_loss : 0.00684652104973793\tvalid_loss :0.005733083933591843\n",
      "Adjusting learning rate of group 0 to 7.1836e-03.\n",
      "Adjusting learning rate of group 0 to 7.1836e-03.\n",
      "Epoch : 413,\ttrain_loss : 0.00754659716039896\tvalid_loss :0.006102592218667269\n",
      "Adjusting learning rate of group 0 to 7.1798e-03.\n",
      "Adjusting learning rate of group 0 to 7.1798e-03.\n",
      "Epoch : 414,\ttrain_loss : 0.007063698954880238\tvalid_loss :0.008145020343363285\n",
      "Adjusting learning rate of group 0 to 7.1760e-03.\n",
      "Adjusting learning rate of group 0 to 7.1760e-03.\n",
      "Epoch : 415,\ttrain_loss : 0.007167031057178974\tvalid_loss :0.005372784100472927\n",
      "Adjusting learning rate of group 0 to 7.1721e-03.\n",
      "Adjusting learning rate of group 0 to 7.1721e-03.\n",
      "Epoch : 416,\ttrain_loss : 0.006856964435428381\tvalid_loss :0.004922398831695318\n",
      "Adjusting learning rate of group 0 to 7.1683e-03.\n",
      "Adjusting learning rate of group 0 to 7.1683e-03.\n",
      "Epoch : 417,\ttrain_loss : 0.007239255588501692\tvalid_loss :0.0051574064418673515\n",
      "Adjusting learning rate of group 0 to 7.1645e-03.\n",
      "Adjusting learning rate of group 0 to 7.1645e-03.\n",
      "Epoch : 418,\ttrain_loss : 0.007085101213306189\tvalid_loss :0.005429847631603479\n",
      "Adjusting learning rate of group 0 to 7.1606e-03.\n",
      "Adjusting learning rate of group 0 to 7.1606e-03.\n",
      "Epoch : 419,\ttrain_loss : 0.006892398931086063\tvalid_loss :0.005045051220804453\n",
      "Adjusting learning rate of group 0 to 7.1568e-03.\n",
      "Adjusting learning rate of group 0 to 7.1568e-03.\n",
      "Epoch : 420,\ttrain_loss : 0.006860686931759119\tvalid_loss :0.006926496047526598\n",
      "Adjusting learning rate of group 0 to 7.1529e-03.\n",
      "Adjusting learning rate of group 0 to 7.1529e-03.\n",
      "Epoch : 421,\ttrain_loss : 0.008475111797451973\tvalid_loss :0.004696247633546591\n",
      "Adjusting learning rate of group 0 to 7.1490e-03.\n",
      "Adjusting learning rate of group 0 to 7.1490e-03.\n",
      "Epoch : 422,\ttrain_loss : 0.007236826233565807\tvalid_loss :0.005018508993089199\n",
      "Adjusting learning rate of group 0 to 7.1452e-03.\n",
      "Adjusting learning rate of group 0 to 7.1452e-03.\n",
      "Epoch : 423,\ttrain_loss : 0.006644074805080891\tvalid_loss :0.007607293780893087\n",
      "Adjusting learning rate of group 0 to 7.1413e-03.\n",
      "Adjusting learning rate of group 0 to 7.1413e-03.\n",
      "Epoch : 424,\ttrain_loss : 0.00856285821646452\tvalid_loss :0.00880012009292841\n",
      "Adjusting learning rate of group 0 to 7.1374e-03.\n",
      "Adjusting learning rate of group 0 to 7.1374e-03.\n",
      "Epoch : 425,\ttrain_loss : 0.007217109203338623\tvalid_loss :0.006521998438984156\n",
      "Adjusting learning rate of group 0 to 7.1335e-03.\n",
      "Adjusting learning rate of group 0 to 7.1335e-03.\n",
      "Epoch : 426,\ttrain_loss : 0.006929278839379549\tvalid_loss :0.006580469198524952\n",
      "Adjusting learning rate of group 0 to 7.1296e-03.\n",
      "Adjusting learning rate of group 0 to 7.1296e-03.\n",
      "Epoch : 427,\ttrain_loss : 0.007360164076089859\tvalid_loss :0.005599232856184244\n",
      "Adjusting learning rate of group 0 to 7.1256e-03.\n",
      "Adjusting learning rate of group 0 to 7.1256e-03.\n",
      "Epoch : 428,\ttrain_loss : 0.006824618671089411\tvalid_loss :0.004691862966865301\n",
      "Adjusting learning rate of group 0 to 7.1217e-03.\n",
      "Adjusting learning rate of group 0 to 7.1217e-03.\n",
      "Epoch : 429,\ttrain_loss : 0.007532854098826647\tvalid_loss :0.006071805953979492\n",
      "Adjusting learning rate of group 0 to 7.1178e-03.\n",
      "Adjusting learning rate of group 0 to 7.1178e-03.\n",
      "Epoch : 430,\ttrain_loss : 0.007290945854038\tvalid_loss :0.004675857722759247\n",
      "Adjusting learning rate of group 0 to 7.1138e-03.\n",
      "Adjusting learning rate of group 0 to 7.1138e-03.\n",
      "Epoch : 431,\ttrain_loss : 0.007185257971286774\tvalid_loss :0.0072157057002186775\n",
      "Adjusting learning rate of group 0 to 7.1099e-03.\n",
      "Adjusting learning rate of group 0 to 7.1099e-03.\n",
      "Epoch : 432,\ttrain_loss : 0.006822686176747084\tvalid_loss :0.005329357925802469\n",
      "Adjusting learning rate of group 0 to 7.1059e-03.\n",
      "Adjusting learning rate of group 0 to 7.1059e-03.\n",
      "Epoch : 433,\ttrain_loss : 0.007753427606076002\tvalid_loss :0.006675634998828173\n",
      "Adjusting learning rate of group 0 to 7.1020e-03.\n",
      "Adjusting learning rate of group 0 to 7.1020e-03.\n",
      "Epoch : 434,\ttrain_loss : 0.007229368668049574\tvalid_loss :0.005446942523121834\n",
      "Adjusting learning rate of group 0 to 7.0980e-03.\n",
      "Adjusting learning rate of group 0 to 7.0980e-03.\n",
      "Epoch : 435,\ttrain_loss : 0.007081693969666958\tvalid_loss :0.004720892291516066\n",
      "Adjusting learning rate of group 0 to 7.0940e-03.\n",
      "Adjusting learning rate of group 0 to 7.0940e-03.\n",
      "Epoch : 436,\ttrain_loss : 0.006647345144301653\tvalid_loss :0.005425234790891409\n",
      "Adjusting learning rate of group 0 to 7.0900e-03.\n",
      "Adjusting learning rate of group 0 to 7.0900e-03.\n",
      "Epoch : 437,\ttrain_loss : 0.0069168866612017155\tvalid_loss :0.007075871806591749\n",
      "Adjusting learning rate of group 0 to 7.0861e-03.\n",
      "Adjusting learning rate of group 0 to 7.0861e-03.\n",
      "Epoch : 438,\ttrain_loss : 0.0071793473325669765\tvalid_loss :0.006451776251196861\n",
      "Adjusting learning rate of group 0 to 7.0821e-03.\n",
      "Adjusting learning rate of group 0 to 7.0821e-03.\n",
      "Epoch : 439,\ttrain_loss : 0.006898586172610521\tvalid_loss :0.0059469472616910934\n",
      "Adjusting learning rate of group 0 to 7.0780e-03.\n",
      "Adjusting learning rate of group 0 to 7.0780e-03.\n",
      "Epoch : 440,\ttrain_loss : 0.007022790145128965\tvalid_loss :0.008843643590807915\n",
      "Adjusting learning rate of group 0 to 7.0740e-03.\n",
      "Adjusting learning rate of group 0 to 7.0740e-03.\n",
      "Epoch : 441,\ttrain_loss : 0.008051271550357342\tvalid_loss :0.005726264789700508\n",
      "Adjusting learning rate of group 0 to 7.0700e-03.\n",
      "Adjusting learning rate of group 0 to 7.0700e-03.\n",
      "Epoch : 442,\ttrain_loss : 0.006948742084205151\tvalid_loss :0.006593342404812574\n",
      "Adjusting learning rate of group 0 to 7.0660e-03.\n",
      "Adjusting learning rate of group 0 to 7.0660e-03.\n",
      "Epoch : 443,\ttrain_loss : 0.00744634959846735\tvalid_loss :0.004959096200764179\n",
      "Adjusting learning rate of group 0 to 7.0619e-03.\n",
      "Adjusting learning rate of group 0 to 7.0619e-03.\n",
      "Epoch : 444,\ttrain_loss : 0.006725182291120291\tvalid_loss :0.004609596915543079\n",
      "Adjusting learning rate of group 0 to 7.0579e-03.\n",
      "Adjusting learning rate of group 0 to 7.0579e-03.\n",
      "Epoch : 445,\ttrain_loss : 0.007012377493083477\tvalid_loss :0.006242245435714722\n",
      "Adjusting learning rate of group 0 to 7.0538e-03.\n",
      "Adjusting learning rate of group 0 to 7.0538e-03.\n",
      "Epoch : 446,\ttrain_loss : 0.007195253390818834\tvalid_loss :0.005114156287163496\n",
      "Adjusting learning rate of group 0 to 7.0498e-03.\n",
      "Adjusting learning rate of group 0 to 7.0498e-03.\n",
      "Epoch : 447,\ttrain_loss : 0.007403969299048185\tvalid_loss :0.009129664860665798\n",
      "Adjusting learning rate of group 0 to 7.0457e-03.\n",
      "Adjusting learning rate of group 0 to 7.0457e-03.\n",
      "Epoch : 448,\ttrain_loss : 0.008051796816289425\tvalid_loss :0.0050418926402926445\n",
      "Adjusting learning rate of group 0 to 7.0416e-03.\n",
      "Adjusting learning rate of group 0 to 7.0416e-03.\n",
      "Epoch : 449,\ttrain_loss : 0.006823610980063677\tvalid_loss :0.0052675106562674046\n",
      "Adjusting learning rate of group 0 to 7.0375e-03.\n",
      "Adjusting learning rate of group 0 to 7.0375e-03.\n",
      "Epoch : 450,\ttrain_loss : 0.006797473412007093\tvalid_loss :0.006131120957434177\n",
      "Adjusting learning rate of group 0 to 7.0334e-03.\n",
      "Adjusting learning rate of group 0 to 7.0334e-03.\n",
      "Epoch : 451,\ttrain_loss : 0.007363216020166874\tvalid_loss :0.010676862671971321\n",
      "Adjusting learning rate of group 0 to 7.0293e-03.\n",
      "Adjusting learning rate of group 0 to 7.0293e-03.\n",
      "Epoch : 452,\ttrain_loss : 0.007979663088917732\tvalid_loss :0.00949549488723278\n",
      "Adjusting learning rate of group 0 to 7.0252e-03.\n",
      "Adjusting learning rate of group 0 to 7.0252e-03.\n",
      "Epoch : 453,\ttrain_loss : 0.0075964778661727905\tvalid_loss :0.004850889556109905\n",
      "Adjusting learning rate of group 0 to 7.0211e-03.\n",
      "Adjusting learning rate of group 0 to 7.0211e-03.\n",
      "Epoch : 454,\ttrain_loss : 0.0066656069830060005\tvalid_loss :0.005071010906249285\n",
      "Adjusting learning rate of group 0 to 7.0170e-03.\n",
      "Adjusting learning rate of group 0 to 7.0170e-03.\n",
      "Epoch : 455,\ttrain_loss : 0.00728690717369318\tvalid_loss :0.004858201369643211\n",
      "Adjusting learning rate of group 0 to 7.0129e-03.\n",
      "Adjusting learning rate of group 0 to 7.0129e-03.\n",
      "Epoch : 456,\ttrain_loss : 0.006567499600350857\tvalid_loss :0.008836478926241398\n",
      "Adjusting learning rate of group 0 to 7.0087e-03.\n",
      "Adjusting learning rate of group 0 to 7.0087e-03.\n",
      "Epoch : 457,\ttrain_loss : 0.007822231389582157\tvalid_loss :0.0048561738803982735\n",
      "Adjusting learning rate of group 0 to 7.0046e-03.\n",
      "Adjusting learning rate of group 0 to 7.0046e-03.\n",
      "Epoch : 458,\ttrain_loss : 0.006791600026190281\tvalid_loss :0.005280587822198868\n",
      "Adjusting learning rate of group 0 to 7.0004e-03.\n",
      "Adjusting learning rate of group 0 to 7.0004e-03.\n",
      "Epoch : 459,\ttrain_loss : 0.006684246938675642\tvalid_loss :0.007244433276355267\n",
      "Adjusting learning rate of group 0 to 6.9963e-03.\n",
      "Adjusting learning rate of group 0 to 6.9963e-03.\n",
      "Epoch : 460,\ttrain_loss : 0.008495648391544819\tvalid_loss :0.006236705929040909\n",
      "Adjusting learning rate of group 0 to 6.9921e-03.\n",
      "Adjusting learning rate of group 0 to 6.9921e-03.\n",
      "Epoch : 461,\ttrain_loss : 0.006745914462953806\tvalid_loss :0.005948432721197605\n",
      "Adjusting learning rate of group 0 to 6.9879e-03.\n",
      "Adjusting learning rate of group 0 to 6.9879e-03.\n",
      "Epoch : 462,\ttrain_loss : 0.0066698226146399975\tvalid_loss :0.00558466836810112\n",
      "Adjusting learning rate of group 0 to 6.9838e-03.\n",
      "Adjusting learning rate of group 0 to 6.9838e-03.\n",
      "Epoch : 463,\ttrain_loss : 0.006914963014423847\tvalid_loss :0.005890422035008669\n",
      "Adjusting learning rate of group 0 to 6.9796e-03.\n",
      "Adjusting learning rate of group 0 to 6.9796e-03.\n",
      "Epoch : 464,\ttrain_loss : 0.006845824886113405\tvalid_loss :0.004803268704563379\n",
      "Adjusting learning rate of group 0 to 6.9754e-03.\n",
      "Adjusting learning rate of group 0 to 6.9754e-03.\n",
      "Epoch : 465,\ttrain_loss : 0.006487040780484676\tvalid_loss :0.009018959477543831\n",
      "Adjusting learning rate of group 0 to 6.9712e-03.\n",
      "Adjusting learning rate of group 0 to 6.9712e-03.\n",
      "Epoch : 466,\ttrain_loss : 0.008017770014703274\tvalid_loss :0.008104872889816761\n",
      "Adjusting learning rate of group 0 to 6.9670e-03.\n",
      "Adjusting learning rate of group 0 to 6.9670e-03.\n",
      "Epoch : 467,\ttrain_loss : 0.00810586754232645\tvalid_loss :0.005124898627400398\n",
      "Adjusting learning rate of group 0 to 6.9627e-03.\n",
      "Adjusting learning rate of group 0 to 6.9627e-03.\n",
      "Epoch : 468,\ttrain_loss : 0.007962005212903023\tvalid_loss :0.007775959558784962\n",
      "Adjusting learning rate of group 0 to 6.9585e-03.\n",
      "Adjusting learning rate of group 0 to 6.9585e-03.\n",
      "Epoch : 469,\ttrain_loss : 0.0066748252138495445\tvalid_loss :0.004952558316290379\n",
      "Adjusting learning rate of group 0 to 6.9543e-03.\n",
      "Adjusting learning rate of group 0 to 6.9543e-03.\n",
      "Epoch : 470,\ttrain_loss : 0.006854302249848843\tvalid_loss :0.006087732966989279\n",
      "Adjusting learning rate of group 0 to 6.9501e-03.\n",
      "Adjusting learning rate of group 0 to 6.9501e-03.\n",
      "Epoch : 471,\ttrain_loss : 0.007464162539690733\tvalid_loss :0.005297099240124226\n",
      "Adjusting learning rate of group 0 to 6.9458e-03.\n",
      "Adjusting learning rate of group 0 to 6.9458e-03.\n",
      "Epoch : 472,\ttrain_loss : 0.0077148922719061375\tvalid_loss :0.00597140658646822\n",
      "Adjusting learning rate of group 0 to 6.9416e-03.\n",
      "Adjusting learning rate of group 0 to 6.9416e-03.\n",
      "Epoch : 473,\ttrain_loss : 0.007098921108990908\tvalid_loss :0.004764166660606861\n",
      "Adjusting learning rate of group 0 to 6.9373e-03.\n",
      "Adjusting learning rate of group 0 to 6.9373e-03.\n",
      "Epoch : 474,\ttrain_loss : 0.007067665923386812\tvalid_loss :0.005278606899082661\n",
      "Adjusting learning rate of group 0 to 6.9330e-03.\n",
      "Adjusting learning rate of group 0 to 6.9330e-03.\n",
      "Epoch : 475,\ttrain_loss : 0.0071301767602562904\tvalid_loss :0.005017091520130634\n",
      "Adjusting learning rate of group 0 to 6.9287e-03.\n",
      "Adjusting learning rate of group 0 to 6.9287e-03.\n",
      "Epoch : 476,\ttrain_loss : 0.007621279917657375\tvalid_loss :0.007137327920645475\n",
      "Adjusting learning rate of group 0 to 6.9245e-03.\n",
      "Adjusting learning rate of group 0 to 6.9245e-03.\n",
      "Epoch : 477,\ttrain_loss : 0.006783465854823589\tvalid_loss :0.006474245339632034\n",
      "Adjusting learning rate of group 0 to 6.9202e-03.\n",
      "Adjusting learning rate of group 0 to 6.9202e-03.\n",
      "Epoch : 478,\ttrain_loss : 0.006761976517736912\tvalid_loss :0.008441400714218616\n",
      "Adjusting learning rate of group 0 to 6.9159e-03.\n",
      "Adjusting learning rate of group 0 to 6.9159e-03.\n",
      "Epoch : 479,\ttrain_loss : 0.0076231746934354305\tvalid_loss :0.005076880566775799\n",
      "Adjusting learning rate of group 0 to 6.9116e-03.\n",
      "Adjusting learning rate of group 0 to 6.9116e-03.\n",
      "Epoch : 480,\ttrain_loss : 0.006521142553538084\tvalid_loss :0.004796568304300308\n",
      "Adjusting learning rate of group 0 to 6.9073e-03.\n",
      "Adjusting learning rate of group 0 to 6.9073e-03.\n",
      "Epoch : 481,\ttrain_loss : 0.00657834205776453\tvalid_loss :0.004479294642806053\n",
      "**********Valid loss decreased (0.004533 ==> 0.004479)**********\n",
      "Adjusting learning rate of group 0 to 6.9029e-03.\n",
      "Adjusting learning rate of group 0 to 6.9029e-03.\n",
      "Epoch : 482,\ttrain_loss : 0.0065187313593924046\tvalid_loss :0.00809163972735405\n",
      "Adjusting learning rate of group 0 to 6.8986e-03.\n",
      "Adjusting learning rate of group 0 to 6.8986e-03.\n",
      "Epoch : 483,\ttrain_loss : 0.007641947362571955\tvalid_loss :0.0045276666060090065\n",
      "Adjusting learning rate of group 0 to 6.8943e-03.\n",
      "Adjusting learning rate of group 0 to 6.8943e-03.\n",
      "Epoch : 484,\ttrain_loss : 0.008217751048505306\tvalid_loss :0.008427656255662441\n",
      "Adjusting learning rate of group 0 to 6.8899e-03.\n",
      "Adjusting learning rate of group 0 to 6.8899e-03.\n",
      "Epoch : 485,\ttrain_loss : 0.007535554934293032\tvalid_loss :0.0072011142037808895\n",
      "Adjusting learning rate of group 0 to 6.8856e-03.\n",
      "Adjusting learning rate of group 0 to 6.8856e-03.\n",
      "Epoch : 486,\ttrain_loss : 0.006570423953235149\tvalid_loss :0.004599355161190033\n",
      "Adjusting learning rate of group 0 to 6.8812e-03.\n",
      "Adjusting learning rate of group 0 to 6.8812e-03.\n",
      "Epoch : 487,\ttrain_loss : 0.006825342308729887\tvalid_loss :0.005947645287960768\n",
      "Adjusting learning rate of group 0 to 6.8769e-03.\n",
      "Adjusting learning rate of group 0 to 6.8769e-03.\n",
      "Epoch : 488,\ttrain_loss : 0.0066986726596951485\tvalid_loss :0.00469166086986661\n",
      "Adjusting learning rate of group 0 to 6.8725e-03.\n",
      "Adjusting learning rate of group 0 to 6.8725e-03.\n",
      "Epoch : 489,\ttrain_loss : 0.007170389406383038\tvalid_loss :0.006077539641410112\n",
      "Adjusting learning rate of group 0 to 6.8681e-03.\n",
      "Adjusting learning rate of group 0 to 6.8681e-03.\n",
      "Epoch : 490,\ttrain_loss : 0.007071733940392733\tvalid_loss :0.005075028166174889\n",
      "Adjusting learning rate of group 0 to 6.8637e-03.\n",
      "Adjusting learning rate of group 0 to 6.8637e-03.\n",
      "Epoch : 491,\ttrain_loss : 0.007125150412321091\tvalid_loss :0.007774165831506252\n",
      "Adjusting learning rate of group 0 to 6.8594e-03.\n",
      "Adjusting learning rate of group 0 to 6.8594e-03.\n",
      "Epoch : 492,\ttrain_loss : 0.0077042230404913425\tvalid_loss :0.004718083888292313\n",
      "Adjusting learning rate of group 0 to 6.8550e-03.\n",
      "Adjusting learning rate of group 0 to 6.8550e-03.\n",
      "Epoch : 493,\ttrain_loss : 0.006539922207593918\tvalid_loss :0.004786343313753605\n",
      "Adjusting learning rate of group 0 to 6.8506e-03.\n",
      "Adjusting learning rate of group 0 to 6.8506e-03.\n",
      "Epoch : 494,\ttrain_loss : 0.006565824616700411\tvalid_loss :0.006550063844770193\n",
      "Adjusting learning rate of group 0 to 6.8461e-03.\n",
      "Adjusting learning rate of group 0 to 6.8461e-03.\n",
      "Epoch : 495,\ttrain_loss : 0.006794400047510862\tvalid_loss :0.005226591601967812\n",
      "Adjusting learning rate of group 0 to 6.8417e-03.\n",
      "Adjusting learning rate of group 0 to 6.8417e-03.\n",
      "Epoch : 496,\ttrain_loss : 0.00684322277083993\tvalid_loss :0.005605291109532118\n",
      "Adjusting learning rate of group 0 to 6.8373e-03.\n",
      "Adjusting learning rate of group 0 to 6.8373e-03.\n",
      "Epoch : 497,\ttrain_loss : 0.006933274213224649\tvalid_loss :0.004739005118608475\n",
      "Adjusting learning rate of group 0 to 6.8329e-03.\n",
      "Adjusting learning rate of group 0 to 6.8329e-03.\n",
      "Epoch : 498,\ttrain_loss : 0.007017142605036497\tvalid_loss :0.0057852603495121\n",
      "Adjusting learning rate of group 0 to 6.8284e-03.\n",
      "Adjusting learning rate of group 0 to 6.8284e-03.\n",
      "Epoch : 499,\ttrain_loss : 0.0069411033764481544\tvalid_loss :0.005252501927316189\n",
      "Adjusting learning rate of group 0 to 6.8240e-03.\n",
      "Adjusting learning rate of group 0 to 6.8240e-03.\n",
      "Epoch : 500,\ttrain_loss : 0.006836270447820425\tvalid_loss :0.00891704112291336\n",
      "Adjusting learning rate of group 0 to 6.8195e-03.\n",
      "Adjusting learning rate of group 0 to 6.8195e-03.\n",
      "Epoch : 501,\ttrain_loss : 0.008174912072718143\tvalid_loss :0.004962576553225517\n",
      "Adjusting learning rate of group 0 to 6.8151e-03.\n",
      "Adjusting learning rate of group 0 to 6.8151e-03.\n",
      "Epoch : 502,\ttrain_loss : 0.0070251948200166225\tvalid_loss :0.004738702904433012\n",
      "Adjusting learning rate of group 0 to 6.8106e-03.\n",
      "Adjusting learning rate of group 0 to 6.8106e-03.\n",
      "Epoch : 503,\ttrain_loss : 0.0065474314615130424\tvalid_loss :0.007104738149791956\n",
      "Adjusting learning rate of group 0 to 6.8061e-03.\n",
      "Adjusting learning rate of group 0 to 6.8061e-03.\n",
      "Epoch : 504,\ttrain_loss : 0.007236074656248093\tvalid_loss :0.004925934597849846\n",
      "Adjusting learning rate of group 0 to 6.8016e-03.\n",
      "Adjusting learning rate of group 0 to 6.8016e-03.\n",
      "Epoch : 505,\ttrain_loss : 0.007144231349229813\tvalid_loss :0.005165714770555496\n",
      "Adjusting learning rate of group 0 to 6.7972e-03.\n",
      "Adjusting learning rate of group 0 to 6.7972e-03.\n",
      "Epoch : 506,\ttrain_loss : 0.0071990760043263435\tvalid_loss :0.00451058940961957\n",
      "Adjusting learning rate of group 0 to 6.7927e-03.\n",
      "Adjusting learning rate of group 0 to 6.7927e-03.\n",
      "Epoch : 507,\ttrain_loss : 0.0070067159831523895\tvalid_loss :0.006224521901458502\n",
      "Adjusting learning rate of group 0 to 6.7882e-03.\n",
      "Adjusting learning rate of group 0 to 6.7882e-03.\n",
      "Epoch : 508,\ttrain_loss : 0.00669707590714097\tvalid_loss :0.0049857208505272865\n",
      "Adjusting learning rate of group 0 to 6.7837e-03.\n",
      "Adjusting learning rate of group 0 to 6.7837e-03.\n",
      "Epoch : 509,\ttrain_loss : 0.006855029612779617\tvalid_loss :0.005148058757185936\n",
      "Adjusting learning rate of group 0 to 6.7791e-03.\n",
      "Adjusting learning rate of group 0 to 6.7791e-03.\n",
      "Epoch : 510,\ttrain_loss : 0.006611877121031284\tvalid_loss :0.0053465054370462894\n",
      "Adjusting learning rate of group 0 to 6.7746e-03.\n",
      "Adjusting learning rate of group 0 to 6.7746e-03.\n",
      "Epoch : 511,\ttrain_loss : 0.0070491028018295765\tvalid_loss :0.004917742218822241\n",
      "Adjusting learning rate of group 0 to 6.7701e-03.\n",
      "Adjusting learning rate of group 0 to 6.7701e-03.\n",
      "Epoch : 512,\ttrain_loss : 0.006633967626839876\tvalid_loss :0.005007900297641754\n",
      "Adjusting learning rate of group 0 to 6.7655e-03.\n",
      "Adjusting learning rate of group 0 to 6.7655e-03.\n",
      "Epoch : 513,\ttrain_loss : 0.0069512720219790936\tvalid_loss :0.005181724205613136\n",
      "Adjusting learning rate of group 0 to 6.7610e-03.\n",
      "Adjusting learning rate of group 0 to 6.7610e-03.\n",
      "Epoch : 514,\ttrain_loss : 0.0075642988085746765\tvalid_loss :0.007098957430571318\n",
      "Adjusting learning rate of group 0 to 6.7565e-03.\n",
      "Adjusting learning rate of group 0 to 6.7565e-03.\n",
      "Epoch : 515,\ttrain_loss : 0.008308690041303635\tvalid_loss :0.011430729180574417\n",
      "Adjusting learning rate of group 0 to 6.7519e-03.\n",
      "Adjusting learning rate of group 0 to 6.7519e-03.\n",
      "Epoch : 516,\ttrain_loss : 0.009485832415521145\tvalid_loss :0.007685658521950245\n",
      "Adjusting learning rate of group 0 to 6.7473e-03.\n",
      "Adjusting learning rate of group 0 to 6.7473e-03.\n",
      "Epoch : 517,\ttrain_loss : 0.008088110014796257\tvalid_loss :0.009112784639000893\n",
      "Adjusting learning rate of group 0 to 6.7428e-03.\n",
      "Adjusting learning rate of group 0 to 6.7428e-03.\n",
      "Epoch : 518,\ttrain_loss : 0.008204136975109577\tvalid_loss :0.005836463533341885\n",
      "Adjusting learning rate of group 0 to 6.7382e-03.\n",
      "Adjusting learning rate of group 0 to 6.7382e-03.\n",
      "Epoch : 519,\ttrain_loss : 0.006885861046612263\tvalid_loss :0.007614043541252613\n",
      "Adjusting learning rate of group 0 to 6.7336e-03.\n",
      "Adjusting learning rate of group 0 to 6.7336e-03.\n",
      "Epoch : 520,\ttrain_loss : 0.007052081637084484\tvalid_loss :0.004974246025085449\n",
      "Adjusting learning rate of group 0 to 6.7290e-03.\n",
      "Adjusting learning rate of group 0 to 6.7290e-03.\n",
      "Epoch : 521,\ttrain_loss : 0.006561636924743652\tvalid_loss :0.004490950144827366\n",
      "Adjusting learning rate of group 0 to 6.7244e-03.\n",
      "Adjusting learning rate of group 0 to 6.7244e-03.\n",
      "Epoch : 522,\ttrain_loss : 0.006591234356164932\tvalid_loss :0.0051430705934762955\n",
      "Adjusting learning rate of group 0 to 6.7198e-03.\n",
      "Adjusting learning rate of group 0 to 6.7198e-03.\n",
      "Epoch : 523,\ttrain_loss : 0.006526124197989702\tvalid_loss :0.004637555219233036\n",
      "Adjusting learning rate of group 0 to 6.7152e-03.\n",
      "Adjusting learning rate of group 0 to 6.7152e-03.\n",
      "Epoch : 524,\ttrain_loss : 0.006611153017729521\tvalid_loss :0.0052336230874061584\n",
      "Adjusting learning rate of group 0 to 6.7106e-03.\n",
      "Adjusting learning rate of group 0 to 6.7106e-03.\n",
      "Epoch : 525,\ttrain_loss : 0.00705405930057168\tvalid_loss :0.005267503671348095\n",
      "Adjusting learning rate of group 0 to 6.7060e-03.\n",
      "Adjusting learning rate of group 0 to 6.7060e-03.\n",
      "Epoch : 526,\ttrain_loss : 0.006816284265369177\tvalid_loss :0.007408609613776207\n",
      "Adjusting learning rate of group 0 to 6.7013e-03.\n",
      "Adjusting learning rate of group 0 to 6.7013e-03.\n",
      "Epoch : 527,\ttrain_loss : 0.008130758069455624\tvalid_loss :0.0056136599741876125\n",
      "Adjusting learning rate of group 0 to 6.6967e-03.\n",
      "Adjusting learning rate of group 0 to 6.6967e-03.\n",
      "Epoch : 528,\ttrain_loss : 0.006814244668930769\tvalid_loss :0.007468996569514275\n",
      "Adjusting learning rate of group 0 to 6.6921e-03.\n",
      "Adjusting learning rate of group 0 to 6.6921e-03.\n",
      "Epoch : 529,\ttrain_loss : 0.007101311814039946\tvalid_loss :0.006590164266526699\n",
      "Adjusting learning rate of group 0 to 6.6874e-03.\n",
      "Adjusting learning rate of group 0 to 6.6874e-03.\n",
      "Epoch : 530,\ttrain_loss : 0.0073437439277768135\tvalid_loss :0.004846393130719662\n",
      "Adjusting learning rate of group 0 to 6.6827e-03.\n",
      "Adjusting learning rate of group 0 to 6.6827e-03.\n",
      "Epoch : 531,\ttrain_loss : 0.0070482646115124226\tvalid_loss :0.004739482421427965\n",
      "Adjusting learning rate of group 0 to 6.6781e-03.\n",
      "Adjusting learning rate of group 0 to 6.6781e-03.\n",
      "Epoch : 532,\ttrain_loss : 0.006684583146125078\tvalid_loss :0.005315225571393967\n",
      "Adjusting learning rate of group 0 to 6.6734e-03.\n",
      "Adjusting learning rate of group 0 to 6.6734e-03.\n",
      "Epoch : 533,\ttrain_loss : 0.007349004037678242\tvalid_loss :0.005738060921430588\n",
      "Adjusting learning rate of group 0 to 6.6687e-03.\n",
      "Adjusting learning rate of group 0 to 6.6687e-03.\n",
      "Epoch : 534,\ttrain_loss : 0.007229595445096493\tvalid_loss :0.010383724234998226\n",
      "Adjusting learning rate of group 0 to 6.6640e-03.\n",
      "Adjusting learning rate of group 0 to 6.6640e-03.\n",
      "Epoch : 535,\ttrain_loss : 0.00766049325466156\tvalid_loss :0.0072964830324053764\n",
      "Adjusting learning rate of group 0 to 6.6594e-03.\n",
      "Adjusting learning rate of group 0 to 6.6594e-03.\n",
      "Epoch : 536,\ttrain_loss : 0.007374633103609085\tvalid_loss :0.007115200627595186\n",
      "Adjusting learning rate of group 0 to 6.6547e-03.\n",
      "Adjusting learning rate of group 0 to 6.6547e-03.\n",
      "Epoch : 537,\ttrain_loss : 0.007800833787769079\tvalid_loss :0.00471812067553401\n",
      "Adjusting learning rate of group 0 to 6.6500e-03.\n",
      "Adjusting learning rate of group 0 to 6.6500e-03.\n",
      "Epoch : 538,\ttrain_loss : 0.006820507813245058\tvalid_loss :0.005135925021022558\n",
      "Adjusting learning rate of group 0 to 6.6452e-03.\n",
      "Adjusting learning rate of group 0 to 6.6452e-03.\n",
      "Epoch : 539,\ttrain_loss : 0.007056612987071276\tvalid_loss :0.00473485654219985\n",
      "Adjusting learning rate of group 0 to 6.6405e-03.\n",
      "Adjusting learning rate of group 0 to 6.6405e-03.\n",
      "Epoch : 540,\ttrain_loss : 0.0068596042692661285\tvalid_loss :0.004870404954999685\n",
      "Adjusting learning rate of group 0 to 6.6358e-03.\n",
      "Adjusting learning rate of group 0 to 6.6358e-03.\n",
      "Epoch : 541,\ttrain_loss : 0.006980872713029385\tvalid_loss :0.0048100873827934265\n",
      "Adjusting learning rate of group 0 to 6.6311e-03.\n",
      "Adjusting learning rate of group 0 to 6.6311e-03.\n",
      "Epoch : 542,\ttrain_loss : 0.008175370283424854\tvalid_loss :0.009495275095105171\n",
      "Adjusting learning rate of group 0 to 6.6263e-03.\n",
      "Adjusting learning rate of group 0 to 6.6263e-03.\n",
      "Epoch : 543,\ttrain_loss : 0.009924034588038921\tvalid_loss :0.005590631160885096\n",
      "Adjusting learning rate of group 0 to 6.6216e-03.\n",
      "Adjusting learning rate of group 0 to 6.6216e-03.\n",
      "Epoch : 544,\ttrain_loss : 0.007513448596000671\tvalid_loss :0.0048614791594445705\n",
      "Adjusting learning rate of group 0 to 6.6169e-03.\n",
      "Adjusting learning rate of group 0 to 6.6169e-03.\n",
      "Epoch : 545,\ttrain_loss : 0.006848681718111038\tvalid_loss :0.005854746326804161\n",
      "Adjusting learning rate of group 0 to 6.6121e-03.\n",
      "Adjusting learning rate of group 0 to 6.6121e-03.\n",
      "Epoch : 546,\ttrain_loss : 0.00753207178786397\tvalid_loss :0.00493875565007329\n",
      "Adjusting learning rate of group 0 to 6.6073e-03.\n",
      "Adjusting learning rate of group 0 to 6.6073e-03.\n",
      "Epoch : 547,\ttrain_loss : 0.006927810609340668\tvalid_loss :0.00532646244391799\n",
      "Adjusting learning rate of group 0 to 6.6026e-03.\n",
      "Adjusting learning rate of group 0 to 6.6026e-03.\n",
      "Epoch : 548,\ttrain_loss : 0.006666465662419796\tvalid_loss :0.005342882592231035\n",
      "Adjusting learning rate of group 0 to 6.5978e-03.\n",
      "Adjusting learning rate of group 0 to 6.5978e-03.\n",
      "Epoch : 549,\ttrain_loss : 0.006985116750001907\tvalid_loss :0.005663618445396423\n",
      "Adjusting learning rate of group 0 to 6.5930e-03.\n",
      "Adjusting learning rate of group 0 to 6.5930e-03.\n",
      "Epoch : 550,\ttrain_loss : 0.006803348660469055\tvalid_loss :0.005723565351217985\n",
      "Adjusting learning rate of group 0 to 6.5882e-03.\n",
      "Adjusting learning rate of group 0 to 6.5882e-03.\n",
      "Epoch : 551,\ttrain_loss : 0.00679439352825284\tvalid_loss :0.005070515908300877\n",
      "Adjusting learning rate of group 0 to 6.5834e-03.\n",
      "Adjusting learning rate of group 0 to 6.5834e-03.\n",
      "Epoch : 552,\ttrain_loss : 0.006744292099028826\tvalid_loss :0.005401169415563345\n",
      "Adjusting learning rate of group 0 to 6.5786e-03.\n",
      "Adjusting learning rate of group 0 to 6.5786e-03.\n",
      "Epoch : 553,\ttrain_loss : 0.0075731477700173855\tvalid_loss :0.0056450255215168\n",
      "Adjusting learning rate of group 0 to 6.5738e-03.\n",
      "Adjusting learning rate of group 0 to 6.5738e-03.\n",
      "Epoch : 554,\ttrain_loss : 0.0072252932004630566\tvalid_loss :0.004816979169845581\n",
      "Adjusting learning rate of group 0 to 6.5690e-03.\n",
      "Adjusting learning rate of group 0 to 6.5690e-03.\n",
      "Epoch : 555,\ttrain_loss : 0.006739468313753605\tvalid_loss :0.004806360229849815\n",
      "Adjusting learning rate of group 0 to 6.5642e-03.\n",
      "Adjusting learning rate of group 0 to 6.5642e-03.\n",
      "Epoch : 556,\ttrain_loss : 0.006765421014279127\tvalid_loss :0.00566832534968853\n",
      "Adjusting learning rate of group 0 to 6.5594e-03.\n",
      "Adjusting learning rate of group 0 to 6.5594e-03.\n",
      "Epoch : 557,\ttrain_loss : 0.006872502621263266\tvalid_loss :0.005098740104585886\n",
      "Adjusting learning rate of group 0 to 6.5545e-03.\n",
      "Adjusting learning rate of group 0 to 6.5545e-03.\n",
      "Epoch : 558,\ttrain_loss : 0.007287833373993635\tvalid_loss :0.007958618924021721\n",
      "Adjusting learning rate of group 0 to 6.5497e-03.\n",
      "Adjusting learning rate of group 0 to 6.5497e-03.\n",
      "Epoch : 559,\ttrain_loss : 0.0075563183054327965\tvalid_loss :0.006847847253084183\n",
      "Adjusting learning rate of group 0 to 6.5449e-03.\n",
      "Adjusting learning rate of group 0 to 6.5449e-03.\n",
      "Epoch : 560,\ttrain_loss : 0.00699226651340723\tvalid_loss :0.005164863541722298\n",
      "Adjusting learning rate of group 0 to 6.5400e-03.\n",
      "Adjusting learning rate of group 0 to 6.5400e-03.\n",
      "Epoch : 561,\ttrain_loss : 0.0069422596134245396\tvalid_loss :0.0044774254783988\n",
      "**********Valid loss decreased (0.004479 ==> 0.004477)**********\n",
      "Adjusting learning rate of group 0 to 6.5351e-03.\n",
      "Adjusting learning rate of group 0 to 6.5351e-03.\n",
      "Epoch : 562,\ttrain_loss : 0.00690170843154192\tvalid_loss :0.005054287612438202\n",
      "Adjusting learning rate of group 0 to 6.5303e-03.\n",
      "Adjusting learning rate of group 0 to 6.5303e-03.\n",
      "Epoch : 563,\ttrain_loss : 0.007970182225108147\tvalid_loss :0.007983722724020481\n",
      "Adjusting learning rate of group 0 to 6.5254e-03.\n",
      "Adjusting learning rate of group 0 to 6.5254e-03.\n",
      "Epoch : 564,\ttrain_loss : 0.008608387783169746\tvalid_loss :0.004817641340196133\n",
      "Adjusting learning rate of group 0 to 6.5205e-03.\n",
      "Adjusting learning rate of group 0 to 6.5205e-03.\n",
      "Epoch : 565,\ttrain_loss : 0.006446958985179663\tvalid_loss :0.005264617968350649\n",
      "Adjusting learning rate of group 0 to 6.5157e-03.\n",
      "Adjusting learning rate of group 0 to 6.5157e-03.\n",
      "Epoch : 566,\ttrain_loss : 0.0070212832652032375\tvalid_loss :0.00490960106253624\n",
      "Adjusting learning rate of group 0 to 6.5108e-03.\n",
      "Adjusting learning rate of group 0 to 6.5108e-03.\n",
      "Epoch : 567,\ttrain_loss : 0.007329170126467943\tvalid_loss :0.0069290706887841225\n",
      "Adjusting learning rate of group 0 to 6.5059e-03.\n",
      "Adjusting learning rate of group 0 to 6.5059e-03.\n",
      "Epoch : 568,\ttrain_loss : 0.007652871310710907\tvalid_loss :0.005165791139006615\n",
      "Adjusting learning rate of group 0 to 6.5010e-03.\n",
      "Adjusting learning rate of group 0 to 6.5010e-03.\n",
      "Epoch : 569,\ttrain_loss : 0.007144194561988115\tvalid_loss :0.0048255231231451035\n",
      "Adjusting learning rate of group 0 to 6.4961e-03.\n",
      "Adjusting learning rate of group 0 to 6.4961e-03.\n",
      "Epoch : 570,\ttrain_loss : 0.006675105541944504\tvalid_loss :0.004901883192360401\n",
      "Adjusting learning rate of group 0 to 6.4912e-03.\n",
      "Adjusting learning rate of group 0 to 6.4912e-03.\n",
      "Epoch : 571,\ttrain_loss : 0.006617475766688585\tvalid_loss :0.004469820763915777\n",
      "**********Valid loss decreased (0.004477 ==> 0.004470)**********\n",
      "Adjusting learning rate of group 0 to 6.4862e-03.\n",
      "Adjusting learning rate of group 0 to 6.4862e-03.\n",
      "Epoch : 572,\ttrain_loss : 0.006549745798110962\tvalid_loss :0.004741296172142029\n",
      "Adjusting learning rate of group 0 to 6.4813e-03.\n",
      "Adjusting learning rate of group 0 to 6.4813e-03.\n",
      "Epoch : 573,\ttrain_loss : 0.0067372736521065235\tvalid_loss :0.005080590024590492\n",
      "Adjusting learning rate of group 0 to 6.4764e-03.\n",
      "Adjusting learning rate of group 0 to 6.4764e-03.\n",
      "Epoch : 574,\ttrain_loss : 0.006412405055016279\tvalid_loss :0.005394722800701857\n",
      "Adjusting learning rate of group 0 to 6.4714e-03.\n",
      "Adjusting learning rate of group 0 to 6.4714e-03.\n",
      "Epoch : 575,\ttrain_loss : 0.006933612283319235\tvalid_loss :0.0047780657187104225\n",
      "Adjusting learning rate of group 0 to 6.4665e-03.\n",
      "Adjusting learning rate of group 0 to 6.4665e-03.\n",
      "Epoch : 576,\ttrain_loss : 0.006808171980082989\tvalid_loss :0.005207039415836334\n",
      "Adjusting learning rate of group 0 to 6.4615e-03.\n",
      "Adjusting learning rate of group 0 to 6.4615e-03.\n",
      "Epoch : 577,\ttrain_loss : 0.006473451387137175\tvalid_loss :0.004490452818572521\n",
      "Adjusting learning rate of group 0 to 6.4566e-03.\n",
      "Adjusting learning rate of group 0 to 6.4566e-03.\n",
      "Epoch : 578,\ttrain_loss : 0.006879209075123072\tvalid_loss :0.006707738619297743\n",
      "Adjusting learning rate of group 0 to 6.4516e-03.\n",
      "Adjusting learning rate of group 0 to 6.4516e-03.\n",
      "Epoch : 579,\ttrain_loss : 0.006500791292637587\tvalid_loss :0.004824849311262369\n",
      "Adjusting learning rate of group 0 to 6.4467e-03.\n",
      "Adjusting learning rate of group 0 to 6.4467e-03.\n",
      "Epoch : 580,\ttrain_loss : 0.006751435808837414\tvalid_loss :0.0049555557779967785\n",
      "Adjusting learning rate of group 0 to 6.4417e-03.\n",
      "Adjusting learning rate of group 0 to 6.4417e-03.\n",
      "Epoch : 581,\ttrain_loss : 0.006614767014980316\tvalid_loss :0.004841527435928583\n",
      "Adjusting learning rate of group 0 to 6.4367e-03.\n",
      "Adjusting learning rate of group 0 to 6.4367e-03.\n",
      "Epoch : 582,\ttrain_loss : 0.006540212314575911\tvalid_loss :0.004675189033150673\n",
      "Adjusting learning rate of group 0 to 6.4317e-03.\n",
      "Adjusting learning rate of group 0 to 6.4317e-03.\n",
      "Epoch : 583,\ttrain_loss : 0.006426595617085695\tvalid_loss :0.00490827951580286\n",
      "Adjusting learning rate of group 0 to 6.4267e-03.\n",
      "Adjusting learning rate of group 0 to 6.4267e-03.\n",
      "Epoch : 584,\ttrain_loss : 0.006866133771836758\tvalid_loss :0.004695418756455183\n",
      "Adjusting learning rate of group 0 to 6.4217e-03.\n",
      "Adjusting learning rate of group 0 to 6.4217e-03.\n",
      "Epoch : 585,\ttrain_loss : 0.006971403025090694\tvalid_loss :0.005648941267281771\n",
      "Adjusting learning rate of group 0 to 6.4167e-03.\n",
      "Adjusting learning rate of group 0 to 6.4167e-03.\n",
      "Epoch : 586,\ttrain_loss : 0.00649986881762743\tvalid_loss :0.0054792845621705055\n",
      "Adjusting learning rate of group 0 to 6.4117e-03.\n",
      "Adjusting learning rate of group 0 to 6.4117e-03.\n",
      "Epoch : 587,\ttrain_loss : 0.006965319626033306\tvalid_loss :0.004428345710039139\n",
      "**********Valid loss decreased (0.004470 ==> 0.004428)**********\n",
      "Adjusting learning rate of group 0 to 6.4067e-03.\n",
      "Adjusting learning rate of group 0 to 6.4067e-03.\n",
      "Epoch : 588,\ttrain_loss : 0.006781918462365866\tvalid_loss :0.004469617735594511\n",
      "Adjusting learning rate of group 0 to 6.4017e-03.\n",
      "Adjusting learning rate of group 0 to 6.4017e-03.\n",
      "Epoch : 589,\ttrain_loss : 0.006743235047906637\tvalid_loss :0.004739394877105951\n",
      "Adjusting learning rate of group 0 to 6.3967e-03.\n",
      "Adjusting learning rate of group 0 to 6.3967e-03.\n",
      "Epoch : 590,\ttrain_loss : 0.006561590824276209\tvalid_loss :0.00499256607145071\n",
      "Adjusting learning rate of group 0 to 6.3916e-03.\n",
      "Adjusting learning rate of group 0 to 6.3916e-03.\n",
      "Epoch : 591,\ttrain_loss : 0.006591340992599726\tvalid_loss :0.004572147503495216\n",
      "Adjusting learning rate of group 0 to 6.3866e-03.\n",
      "Adjusting learning rate of group 0 to 6.3866e-03.\n",
      "Epoch : 592,\ttrain_loss : 0.008074438199400902\tvalid_loss :0.006484875455498695\n",
      "Adjusting learning rate of group 0 to 6.3815e-03.\n",
      "Adjusting learning rate of group 0 to 6.3815e-03.\n",
      "Epoch : 593,\ttrain_loss : 0.00853413064032793\tvalid_loss :0.005177811719477177\n",
      "Adjusting learning rate of group 0 to 6.3765e-03.\n",
      "Adjusting learning rate of group 0 to 6.3765e-03.\n",
      "Epoch : 594,\ttrain_loss : 0.006587109062820673\tvalid_loss :0.004467766731977463\n",
      "Adjusting learning rate of group 0 to 6.3714e-03.\n",
      "Adjusting learning rate of group 0 to 6.3714e-03.\n",
      "Epoch : 595,\ttrain_loss : 0.0067064291797578335\tvalid_loss :0.005568880587816238\n",
      "Adjusting learning rate of group 0 to 6.3664e-03.\n",
      "Adjusting learning rate of group 0 to 6.3664e-03.\n",
      "Epoch : 596,\ttrain_loss : 0.006442445330321789\tvalid_loss :0.004753807559609413\n",
      "Adjusting learning rate of group 0 to 6.3613e-03.\n",
      "Adjusting learning rate of group 0 to 6.3613e-03.\n",
      "Epoch : 597,\ttrain_loss : 0.006833546794950962\tvalid_loss :0.0067139072343707085\n",
      "Adjusting learning rate of group 0 to 6.3562e-03.\n",
      "Adjusting learning rate of group 0 to 6.3562e-03.\n",
      "Epoch : 598,\ttrain_loss : 0.006346484180539846\tvalid_loss :0.004579395055770874\n",
      "Adjusting learning rate of group 0 to 6.3511e-03.\n",
      "Adjusting learning rate of group 0 to 6.3511e-03.\n",
      "Epoch : 599,\ttrain_loss : 0.006626491900533438\tvalid_loss :0.004424750339239836\n",
      "**********Valid loss decreased (0.004428 ==> 0.004425)**********\n",
      "Adjusting learning rate of group 0 to 6.3461e-03.\n",
      "Adjusting learning rate of group 0 to 6.3461e-03.\n",
      "Epoch : 600,\ttrain_loss : 0.0076917377300560474\tvalid_loss :0.00761831970885396\n",
      "Adjusting learning rate of group 0 to 6.3410e-03.\n",
      "Adjusting learning rate of group 0 to 6.3410e-03.\n",
      "Epoch : 601,\ttrain_loss : 0.00737251341342926\tvalid_loss :0.007600891869515181\n",
      "Adjusting learning rate of group 0 to 6.3359e-03.\n",
      "Adjusting learning rate of group 0 to 6.3359e-03.\n",
      "Epoch : 602,\ttrain_loss : 0.00795060582458973\tvalid_loss :0.007109908387064934\n",
      "Adjusting learning rate of group 0 to 6.3308e-03.\n",
      "Adjusting learning rate of group 0 to 6.3308e-03.\n",
      "Epoch : 603,\ttrain_loss : 0.007287248503416777\tvalid_loss :0.0071256100200116634\n",
      "Adjusting learning rate of group 0 to 6.3257e-03.\n",
      "Adjusting learning rate of group 0 to 6.3257e-03.\n",
      "Epoch : 604,\ttrain_loss : 0.006690113339573145\tvalid_loss :0.0065402318723499775\n",
      "Adjusting learning rate of group 0 to 6.3205e-03.\n",
      "Adjusting learning rate of group 0 to 6.3205e-03.\n",
      "Epoch : 605,\ttrain_loss : 0.007267917972058058\tvalid_loss :0.005496131256222725\n",
      "Adjusting learning rate of group 0 to 6.3154e-03.\n",
      "Adjusting learning rate of group 0 to 6.3154e-03.\n",
      "Epoch : 606,\ttrain_loss : 0.00712586659938097\tvalid_loss :0.008540480397641659\n",
      "Adjusting learning rate of group 0 to 6.3103e-03.\n",
      "Adjusting learning rate of group 0 to 6.3103e-03.\n",
      "Epoch : 607,\ttrain_loss : 0.0078084408305585384\tvalid_loss :0.005870339460670948\n",
      "Adjusting learning rate of group 0 to 6.3052e-03.\n",
      "Adjusting learning rate of group 0 to 6.3052e-03.\n",
      "Epoch : 608,\ttrain_loss : 0.006680711172521114\tvalid_loss :0.004803354851901531\n",
      "Adjusting learning rate of group 0 to 6.3000e-03.\n",
      "Adjusting learning rate of group 0 to 6.3000e-03.\n",
      "Epoch : 609,\ttrain_loss : 0.006532315164804459\tvalid_loss :0.004574445076286793\n",
      "Adjusting learning rate of group 0 to 6.2949e-03.\n",
      "Adjusting learning rate of group 0 to 6.2949e-03.\n",
      "Epoch : 610,\ttrain_loss : 0.006344061344861984\tvalid_loss :0.004429814871400595\n",
      "Adjusting learning rate of group 0 to 6.2897e-03.\n",
      "Adjusting learning rate of group 0 to 6.2897e-03.\n",
      "Epoch : 611,\ttrain_loss : 0.006724830716848373\tvalid_loss :0.004863025154918432\n",
      "Adjusting learning rate of group 0 to 6.2846e-03.\n",
      "Adjusting learning rate of group 0 to 6.2846e-03.\n",
      "Epoch : 612,\ttrain_loss : 0.006495507899671793\tvalid_loss :0.004693563096225262\n",
      "Adjusting learning rate of group 0 to 6.2794e-03.\n",
      "Adjusting learning rate of group 0 to 6.2794e-03.\n",
      "Epoch : 613,\ttrain_loss : 0.006746270693838596\tvalid_loss :0.005810455419123173\n",
      "Adjusting learning rate of group 0 to 6.2742e-03.\n",
      "Adjusting learning rate of group 0 to 6.2742e-03.\n",
      "Epoch : 614,\ttrain_loss : 0.006545945070683956\tvalid_loss :0.004788625985383987\n",
      "Adjusting learning rate of group 0 to 6.2691e-03.\n",
      "Adjusting learning rate of group 0 to 6.2691e-03.\n",
      "Epoch : 615,\ttrain_loss : 0.00684706587344408\tvalid_loss :0.005146114621311426\n",
      "Adjusting learning rate of group 0 to 6.2639e-03.\n",
      "Adjusting learning rate of group 0 to 6.2639e-03.\n",
      "Epoch : 616,\ttrain_loss : 0.006738477386534214\tvalid_loss :0.004913674667477608\n",
      "Adjusting learning rate of group 0 to 6.2587e-03.\n",
      "Adjusting learning rate of group 0 to 6.2587e-03.\n",
      "Epoch : 617,\ttrain_loss : 0.006595897488296032\tvalid_loss :0.00490987952798605\n",
      "Adjusting learning rate of group 0 to 6.2535e-03.\n",
      "Adjusting learning rate of group 0 to 6.2535e-03.\n",
      "Epoch : 618,\ttrain_loss : 0.0068604894913733006\tvalid_loss :0.00528842443600297\n",
      "Adjusting learning rate of group 0 to 6.2483e-03.\n",
      "Adjusting learning rate of group 0 to 6.2483e-03.\n",
      "Epoch : 619,\ttrain_loss : 0.0065200976096093655\tvalid_loss :0.004987246356904507\n",
      "Adjusting learning rate of group 0 to 6.2431e-03.\n",
      "Adjusting learning rate of group 0 to 6.2431e-03.\n",
      "Epoch : 620,\ttrain_loss : 0.006974752992391586\tvalid_loss :0.005616653710603714\n",
      "Adjusting learning rate of group 0 to 6.2379e-03.\n",
      "Adjusting learning rate of group 0 to 6.2379e-03.\n",
      "Epoch : 621,\ttrain_loss : 0.007458854001015425\tvalid_loss :0.007603932172060013\n",
      "Adjusting learning rate of group 0 to 6.2327e-03.\n",
      "Adjusting learning rate of group 0 to 6.2327e-03.\n",
      "Epoch : 622,\ttrain_loss : 0.007068491540849209\tvalid_loss :0.007406427059322596\n",
      "Adjusting learning rate of group 0 to 6.2275e-03.\n",
      "Adjusting learning rate of group 0 to 6.2275e-03.\n",
      "Epoch : 623,\ttrain_loss : 0.006845569703727961\tvalid_loss :0.008598261512815952\n",
      "Adjusting learning rate of group 0 to 6.2223e-03.\n",
      "Adjusting learning rate of group 0 to 6.2223e-03.\n",
      "Epoch : 624,\ttrain_loss : 0.008209103718400002\tvalid_loss :0.004889228381216526\n",
      "Adjusting learning rate of group 0 to 6.2171e-03.\n",
      "Adjusting learning rate of group 0 to 6.2171e-03.\n",
      "Epoch : 625,\ttrain_loss : 0.006797955371439457\tvalid_loss :0.005099879577755928\n",
      "Adjusting learning rate of group 0 to 6.2118e-03.\n",
      "Adjusting learning rate of group 0 to 6.2118e-03.\n",
      "Epoch : 626,\ttrain_loss : 0.0064337365329265594\tvalid_loss :0.004718089010566473\n",
      "Adjusting learning rate of group 0 to 6.2066e-03.\n",
      "Adjusting learning rate of group 0 to 6.2066e-03.\n",
      "Epoch : 627,\ttrain_loss : 0.006623285356909037\tvalid_loss :0.004626182839274406\n",
      "Adjusting learning rate of group 0 to 6.2013e-03.\n",
      "Adjusting learning rate of group 0 to 6.2013e-03.\n",
      "Epoch : 628,\ttrain_loss : 0.0066270711831748486\tvalid_loss :0.006030383985489607\n",
      "Adjusting learning rate of group 0 to 6.1961e-03.\n",
      "Adjusting learning rate of group 0 to 6.1961e-03.\n",
      "Epoch : 629,\ttrain_loss : 0.006269107572734356\tvalid_loss :0.004714023321866989\n",
      "Adjusting learning rate of group 0 to 6.1908e-03.\n",
      "Adjusting learning rate of group 0 to 6.1908e-03.\n",
      "Epoch : 630,\ttrain_loss : 0.00664629926905036\tvalid_loss :0.0057740891352295876\n",
      "Adjusting learning rate of group 0 to 6.1856e-03.\n",
      "Adjusting learning rate of group 0 to 6.1856e-03.\n",
      "Epoch : 631,\ttrain_loss : 0.007104546297341585\tvalid_loss :0.005756113212555647\n",
      "Adjusting learning rate of group 0 to 6.1803e-03.\n",
      "Adjusting learning rate of group 0 to 6.1803e-03.\n",
      "Epoch : 632,\ttrain_loss : 0.006533756852149963\tvalid_loss :0.0046133119612932205\n",
      "Adjusting learning rate of group 0 to 6.1750e-03.\n",
      "Adjusting learning rate of group 0 to 6.1750e-03.\n",
      "Epoch : 633,\ttrain_loss : 0.006446844432502985\tvalid_loss :0.004960566759109497\n",
      "Adjusting learning rate of group 0 to 6.1698e-03.\n",
      "Adjusting learning rate of group 0 to 6.1698e-03.\n",
      "Epoch : 634,\ttrain_loss : 0.006768851075321436\tvalid_loss :0.004780515097081661\n",
      "Adjusting learning rate of group 0 to 6.1645e-03.\n",
      "Adjusting learning rate of group 0 to 6.1645e-03.\n",
      "Epoch : 635,\ttrain_loss : 0.00646913331001997\tvalid_loss :0.004541535396128893\n",
      "Adjusting learning rate of group 0 to 6.1592e-03.\n",
      "Adjusting learning rate of group 0 to 6.1592e-03.\n",
      "Epoch : 636,\ttrain_loss : 0.006632364820688963\tvalid_loss :0.004460269585251808\n",
      "Adjusting learning rate of group 0 to 6.1539e-03.\n",
      "Adjusting learning rate of group 0 to 6.1539e-03.\n",
      "Epoch : 637,\ttrain_loss : 0.006818024907261133\tvalid_loss :0.005565600469708443\n",
      "Adjusting learning rate of group 0 to 6.1486e-03.\n",
      "Adjusting learning rate of group 0 to 6.1486e-03.\n",
      "Epoch : 638,\ttrain_loss : 0.006355772726237774\tvalid_loss :0.004852956626564264\n",
      "Adjusting learning rate of group 0 to 6.1433e-03.\n",
      "Adjusting learning rate of group 0 to 6.1433e-03.\n",
      "Epoch : 639,\ttrain_loss : 0.006823828909546137\tvalid_loss :0.004988105036318302\n",
      "Adjusting learning rate of group 0 to 6.1380e-03.\n",
      "Adjusting learning rate of group 0 to 6.1380e-03.\n",
      "Epoch : 640,\ttrain_loss : 0.006467452738434076\tvalid_loss :0.0043955338187515736\n",
      "**********Valid loss decreased (0.004425 ==> 0.004396)**********\n",
      "Adjusting learning rate of group 0 to 6.1327e-03.\n",
      "Adjusting learning rate of group 0 to 6.1327e-03.\n",
      "Epoch : 641,\ttrain_loss : 0.006752947811037302\tvalid_loss :0.005367461591959\n",
      "Adjusting learning rate of group 0 to 6.1274e-03.\n",
      "Adjusting learning rate of group 0 to 6.1274e-03.\n",
      "Epoch : 642,\ttrain_loss : 0.0063912346959114075\tvalid_loss :0.00623168982565403\n",
      "Adjusting learning rate of group 0 to 6.1220e-03.\n",
      "Adjusting learning rate of group 0 to 6.1220e-03.\n",
      "Epoch : 643,\ttrain_loss : 0.006976911332458258\tvalid_loss :0.005716430023312569\n",
      "Adjusting learning rate of group 0 to 6.1167e-03.\n",
      "Adjusting learning rate of group 0 to 6.1167e-03.\n",
      "Epoch : 644,\ttrain_loss : 0.0063814134337008\tvalid_loss :0.004769008141011\n",
      "Adjusting learning rate of group 0 to 6.1114e-03.\n",
      "Adjusting learning rate of group 0 to 6.1114e-03.\n",
      "Epoch : 645,\ttrain_loss : 0.0064663998782634735\tvalid_loss :0.004573766142129898\n",
      "Adjusting learning rate of group 0 to 6.1060e-03.\n",
      "Adjusting learning rate of group 0 to 6.1060e-03.\n",
      "Epoch : 646,\ttrain_loss : 0.006624660454690456\tvalid_loss :0.004611465614289045\n",
      "Adjusting learning rate of group 0 to 6.1007e-03.\n",
      "Adjusting learning rate of group 0 to 6.1007e-03.\n",
      "Epoch : 647,\ttrain_loss : 0.006737483199685812\tvalid_loss :0.004697462543845177\n",
      "Adjusting learning rate of group 0 to 6.0953e-03.\n",
      "Adjusting learning rate of group 0 to 6.0953e-03.\n",
      "Epoch : 648,\ttrain_loss : 0.0063575576059520245\tvalid_loss :0.004626247100532055\n",
      "Adjusting learning rate of group 0 to 6.0900e-03.\n",
      "Adjusting learning rate of group 0 to 6.0900e-03.\n",
      "Epoch : 649,\ttrain_loss : 0.006861832924187183\tvalid_loss :0.005291147157549858\n",
      "Adjusting learning rate of group 0 to 6.0846e-03.\n",
      "Adjusting learning rate of group 0 to 6.0846e-03.\n",
      "Epoch : 650,\ttrain_loss : 0.006459818687289953\tvalid_loss :0.0047582597471773624\n",
      "Adjusting learning rate of group 0 to 6.0793e-03.\n",
      "Adjusting learning rate of group 0 to 6.0793e-03.\n",
      "Epoch : 651,\ttrain_loss : 0.006907809060066938\tvalid_loss :0.006642129272222519\n",
      "Adjusting learning rate of group 0 to 6.0739e-03.\n",
      "Adjusting learning rate of group 0 to 6.0739e-03.\n",
      "Epoch : 652,\ttrain_loss : 0.006793104112148285\tvalid_loss :0.0045838612131774426\n",
      "Adjusting learning rate of group 0 to 6.0685e-03.\n",
      "Adjusting learning rate of group 0 to 6.0685e-03.\n",
      "Epoch : 653,\ttrain_loss : 0.006815636996179819\tvalid_loss :0.005141695961356163\n",
      "Adjusting learning rate of group 0 to 6.0631e-03.\n",
      "Adjusting learning rate of group 0 to 6.0631e-03.\n",
      "Epoch : 654,\ttrain_loss : 0.006476216483861208\tvalid_loss :0.0047430251725018024\n",
      "Adjusting learning rate of group 0 to 6.0578e-03.\n",
      "Adjusting learning rate of group 0 to 6.0578e-03.\n",
      "Epoch : 655,\ttrain_loss : 0.006955423392355442\tvalid_loss :0.008910104632377625\n",
      "Adjusting learning rate of group 0 to 6.0524e-03.\n",
      "Adjusting learning rate of group 0 to 6.0524e-03.\n",
      "Epoch : 656,\ttrain_loss : 0.008103364147245884\tvalid_loss :0.004987259395420551\n",
      "Adjusting learning rate of group 0 to 6.0470e-03.\n",
      "Adjusting learning rate of group 0 to 6.0470e-03.\n",
      "Epoch : 657,\ttrain_loss : 0.006466526538133621\tvalid_loss :0.005231145769357681\n",
      "Adjusting learning rate of group 0 to 6.0416e-03.\n",
      "Adjusting learning rate of group 0 to 6.0416e-03.\n",
      "Epoch : 658,\ttrain_loss : 0.006370519287884235\tvalid_loss :0.005701244808733463\n",
      "Adjusting learning rate of group 0 to 6.0362e-03.\n",
      "Adjusting learning rate of group 0 to 6.0362e-03.\n",
      "Epoch : 659,\ttrain_loss : 0.006872653961181641\tvalid_loss :0.004571649245917797\n",
      "Adjusting learning rate of group 0 to 6.0308e-03.\n",
      "Adjusting learning rate of group 0 to 6.0308e-03.\n",
      "Epoch : 660,\ttrain_loss : 0.006832311861217022\tvalid_loss :0.007361965719610453\n",
      "Adjusting learning rate of group 0 to 6.0253e-03.\n",
      "Adjusting learning rate of group 0 to 6.0253e-03.\n",
      "Epoch : 661,\ttrain_loss : 0.007223043590784073\tvalid_loss :0.005245661363005638\n",
      "Adjusting learning rate of group 0 to 6.0199e-03.\n",
      "Adjusting learning rate of group 0 to 6.0199e-03.\n",
      "Epoch : 662,\ttrain_loss : 0.006655989680439234\tvalid_loss :0.004592313431203365\n",
      "Adjusting learning rate of group 0 to 6.0145e-03.\n",
      "Adjusting learning rate of group 0 to 6.0145e-03.\n",
      "Epoch : 663,\ttrain_loss : 0.006423685234040022\tvalid_loss :0.004896180704236031\n",
      "Adjusting learning rate of group 0 to 6.0091e-03.\n",
      "Adjusting learning rate of group 0 to 6.0091e-03.\n",
      "Epoch : 664,\ttrain_loss : 0.006654572673141956\tvalid_loss :0.0054730866104364395\n",
      "Adjusting learning rate of group 0 to 6.0036e-03.\n",
      "Adjusting learning rate of group 0 to 6.0036e-03.\n",
      "Epoch : 665,\ttrain_loss : 0.006339909974485636\tvalid_loss :0.004351253621280193\n",
      "**********Valid loss decreased (0.004396 ==> 0.004351)**********\n",
      "Adjusting learning rate of group 0 to 5.9982e-03.\n",
      "Adjusting learning rate of group 0 to 5.9982e-03.\n",
      "Epoch : 666,\ttrain_loss : 0.006351360119879246\tvalid_loss :0.0052447933703660965\n",
      "Adjusting learning rate of group 0 to 5.9927e-03.\n",
      "Adjusting learning rate of group 0 to 5.9927e-03.\n",
      "Epoch : 667,\ttrain_loss : 0.006480408366769552\tvalid_loss :0.005098300985991955\n",
      "Adjusting learning rate of group 0 to 5.9873e-03.\n",
      "Adjusting learning rate of group 0 to 5.9873e-03.\n",
      "Epoch : 668,\ttrain_loss : 0.006770624779164791\tvalid_loss :0.004415546543896198\n",
      "Adjusting learning rate of group 0 to 5.9818e-03.\n",
      "Adjusting learning rate of group 0 to 5.9818e-03.\n",
      "Epoch : 669,\ttrain_loss : 0.006738012190908194\tvalid_loss :0.0057120914570987225\n",
      "Adjusting learning rate of group 0 to 5.9764e-03.\n",
      "Adjusting learning rate of group 0 to 5.9764e-03.\n",
      "Epoch : 670,\ttrain_loss : 0.0062581999227404594\tvalid_loss :0.00448159035295248\n",
      "Adjusting learning rate of group 0 to 5.9709e-03.\n",
      "Adjusting learning rate of group 0 to 5.9709e-03.\n",
      "Epoch : 671,\ttrain_loss : 0.006347982678562403\tvalid_loss :0.0043136621825397015\n",
      "**********Valid loss decreased (0.004351 ==> 0.004314)**********\n",
      "Adjusting learning rate of group 0 to 5.9654e-03.\n",
      "Adjusting learning rate of group 0 to 5.9654e-03.\n",
      "Epoch : 672,\ttrain_loss : 0.00677284924313426\tvalid_loss :0.005896596238017082\n",
      "Adjusting learning rate of group 0 to 5.9600e-03.\n",
      "Adjusting learning rate of group 0 to 5.9600e-03.\n",
      "Epoch : 673,\ttrain_loss : 0.007067856844514608\tvalid_loss :0.005559109151363373\n",
      "Adjusting learning rate of group 0 to 5.9545e-03.\n",
      "Adjusting learning rate of group 0 to 5.9545e-03.\n",
      "Epoch : 674,\ttrain_loss : 0.007083040662109852\tvalid_loss :0.008393870666623116\n",
      "Adjusting learning rate of group 0 to 5.9490e-03.\n",
      "Adjusting learning rate of group 0 to 5.9490e-03.\n",
      "Epoch : 675,\ttrain_loss : 0.007260579150170088\tvalid_loss :0.00499451719224453\n",
      "Adjusting learning rate of group 0 to 5.9435e-03.\n",
      "Adjusting learning rate of group 0 to 5.9435e-03.\n",
      "Epoch : 676,\ttrain_loss : 0.006461071316152811\tvalid_loss :0.004800151102244854\n",
      "Adjusting learning rate of group 0 to 5.9380e-03.\n",
      "Adjusting learning rate of group 0 to 5.9380e-03.\n",
      "Epoch : 677,\ttrain_loss : 0.0064253294840455055\tvalid_loss :0.0052307890728116035\n",
      "Adjusting learning rate of group 0 to 5.9325e-03.\n",
      "Adjusting learning rate of group 0 to 5.9325e-03.\n",
      "Epoch : 678,\ttrain_loss : 0.006716749630868435\tvalid_loss :0.00450882688164711\n",
      "Adjusting learning rate of group 0 to 5.9270e-03.\n",
      "Adjusting learning rate of group 0 to 5.9270e-03.\n",
      "Epoch : 679,\ttrain_loss : 0.0067845615558326244\tvalid_loss :0.00528799369931221\n",
      "Adjusting learning rate of group 0 to 5.9215e-03.\n",
      "Adjusting learning rate of group 0 to 5.9215e-03.\n",
      "Epoch : 680,\ttrain_loss : 0.007237554062157869\tvalid_loss :0.005298328585922718\n",
      "Adjusting learning rate of group 0 to 5.9160e-03.\n",
      "Adjusting learning rate of group 0 to 5.9160e-03.\n",
      "Epoch : 681,\ttrain_loss : 0.006636977661401033\tvalid_loss :0.004900162108242512\n",
      "Adjusting learning rate of group 0 to 5.9105e-03.\n",
      "Adjusting learning rate of group 0 to 5.9105e-03.\n",
      "Epoch : 682,\ttrain_loss : 0.006482123862951994\tvalid_loss :0.004835142754018307\n",
      "Adjusting learning rate of group 0 to 5.9050e-03.\n",
      "Adjusting learning rate of group 0 to 5.9050e-03.\n",
      "Epoch : 683,\ttrain_loss : 0.006413828581571579\tvalid_loss :0.0044248648919165134\n",
      "Adjusting learning rate of group 0 to 5.8994e-03.\n",
      "Adjusting learning rate of group 0 to 5.8994e-03.\n",
      "Epoch : 684,\ttrain_loss : 0.006636504083871841\tvalid_loss :0.004571019671857357\n",
      "Adjusting learning rate of group 0 to 5.8939e-03.\n",
      "Adjusting learning rate of group 0 to 5.8939e-03.\n",
      "Epoch : 685,\ttrain_loss : 0.006559873931109905\tvalid_loss :0.00527363084256649\n",
      "Adjusting learning rate of group 0 to 5.8884e-03.\n",
      "Adjusting learning rate of group 0 to 5.8884e-03.\n",
      "Epoch : 686,\ttrain_loss : 0.006481618620455265\tvalid_loss :0.004664963111281395\n",
      "Adjusting learning rate of group 0 to 5.8828e-03.\n",
      "Adjusting learning rate of group 0 to 5.8828e-03.\n",
      "Epoch : 687,\ttrain_loss : 0.006337107624858618\tvalid_loss :0.005102162249386311\n",
      "Adjusting learning rate of group 0 to 5.8773e-03.\n",
      "Adjusting learning rate of group 0 to 5.8773e-03.\n",
      "Epoch : 688,\ttrain_loss : 0.006375820841640234\tvalid_loss :0.004609174560755491\n",
      "Adjusting learning rate of group 0 to 5.8717e-03.\n",
      "Adjusting learning rate of group 0 to 5.8717e-03.\n",
      "Epoch : 689,\ttrain_loss : 0.006718347314745188\tvalid_loss :0.004628441296517849\n",
      "Adjusting learning rate of group 0 to 5.8662e-03.\n",
      "Adjusting learning rate of group 0 to 5.8662e-03.\n",
      "Epoch : 690,\ttrain_loss : 0.006761007476598024\tvalid_loss :0.0047287228517234325\n",
      "Adjusting learning rate of group 0 to 5.8606e-03.\n",
      "Adjusting learning rate of group 0 to 5.8606e-03.\n",
      "Epoch : 691,\ttrain_loss : 0.006459302734583616\tvalid_loss :0.004365810193121433\n",
      "Adjusting learning rate of group 0 to 5.8550e-03.\n",
      "Adjusting learning rate of group 0 to 5.8550e-03.\n",
      "Epoch : 692,\ttrain_loss : 0.006334473378956318\tvalid_loss :0.0050936853513121605\n",
      "Adjusting learning rate of group 0 to 5.8495e-03.\n",
      "Adjusting learning rate of group 0 to 5.8495e-03.\n",
      "Epoch : 693,\ttrain_loss : 0.006268944591283798\tvalid_loss :0.005203292705118656\n",
      "Adjusting learning rate of group 0 to 5.8439e-03.\n",
      "Adjusting learning rate of group 0 to 5.8439e-03.\n",
      "Epoch : 694,\ttrain_loss : 0.006770300213247538\tvalid_loss :0.00469938013702631\n",
      "Adjusting learning rate of group 0 to 5.8383e-03.\n",
      "Adjusting learning rate of group 0 to 5.8383e-03.\n",
      "Epoch : 695,\ttrain_loss : 0.006553229875862598\tvalid_loss :0.004511862061917782\n",
      "Adjusting learning rate of group 0 to 5.8327e-03.\n",
      "Adjusting learning rate of group 0 to 5.8327e-03.\n",
      "Epoch : 696,\ttrain_loss : 0.006781506817787886\tvalid_loss :0.004453321918845177\n",
      "Adjusting learning rate of group 0 to 5.8271e-03.\n",
      "Adjusting learning rate of group 0 to 5.8271e-03.\n",
      "Epoch : 697,\ttrain_loss : 0.007551611866801977\tvalid_loss :0.008031707257032394\n",
      "Adjusting learning rate of group 0 to 5.8216e-03.\n",
      "Adjusting learning rate of group 0 to 5.8216e-03.\n",
      "Epoch : 698,\ttrain_loss : 0.0072244033217430115\tvalid_loss :0.008725810796022415\n",
      "Adjusting learning rate of group 0 to 5.8160e-03.\n",
      "Adjusting learning rate of group 0 to 5.8160e-03.\n",
      "Epoch : 699,\ttrain_loss : 0.007649309933185577\tvalid_loss :0.0055815596133470535\n",
      "Adjusting learning rate of group 0 to 5.8104e-03.\n",
      "Adjusting learning rate of group 0 to 5.8104e-03.\n",
      "Epoch : 700,\ttrain_loss : 0.007309225853532553\tvalid_loss :0.0048264507204294205\n",
      "Adjusting learning rate of group 0 to 5.8048e-03.\n",
      "Adjusting learning rate of group 0 to 5.8048e-03.\n",
      "Epoch : 701,\ttrain_loss : 0.006833911407738924\tvalid_loss :0.005233417265117168\n",
      "Adjusting learning rate of group 0 to 5.7991e-03.\n",
      "Adjusting learning rate of group 0 to 5.7991e-03.\n",
      "Epoch : 702,\ttrain_loss : 0.006462779361754656\tvalid_loss :0.0043501537293195724\n",
      "Adjusting learning rate of group 0 to 5.7935e-03.\n",
      "Adjusting learning rate of group 0 to 5.7935e-03.\n",
      "Epoch : 703,\ttrain_loss : 0.006352953612804413\tvalid_loss :0.004915845114737749\n",
      "Adjusting learning rate of group 0 to 5.7879e-03.\n",
      "Adjusting learning rate of group 0 to 5.7879e-03.\n",
      "Epoch : 704,\ttrain_loss : 0.006364169530570507\tvalid_loss :0.00450257770717144\n",
      "Adjusting learning rate of group 0 to 5.7823e-03.\n",
      "Adjusting learning rate of group 0 to 5.7823e-03.\n",
      "Epoch : 705,\ttrain_loss : 0.006642133928835392\tvalid_loss :0.004570860881358385\n",
      "Adjusting learning rate of group 0 to 5.7767e-03.\n",
      "Adjusting learning rate of group 0 to 5.7767e-03.\n",
      "Epoch : 706,\ttrain_loss : 0.006656290031969547\tvalid_loss :0.004693316761404276\n",
      "Adjusting learning rate of group 0 to 5.7710e-03.\n",
      "Adjusting learning rate of group 0 to 5.7710e-03.\n",
      "Epoch : 707,\ttrain_loss : 0.007135302294045687\tvalid_loss :0.0059666940942406654\n",
      "Adjusting learning rate of group 0 to 5.7654e-03.\n",
      "Adjusting learning rate of group 0 to 5.7654e-03.\n",
      "Epoch : 708,\ttrain_loss : 0.0069760531187057495\tvalid_loss :0.007376155816018581\n",
      "Adjusting learning rate of group 0 to 5.7598e-03.\n",
      "Adjusting learning rate of group 0 to 5.7598e-03.\n",
      "Epoch : 709,\ttrain_loss : 0.007112284656614065\tvalid_loss :0.005118728615343571\n",
      "Adjusting learning rate of group 0 to 5.7541e-03.\n",
      "Adjusting learning rate of group 0 to 5.7541e-03.\n",
      "Epoch : 710,\ttrain_loss : 0.0067459940910339355\tvalid_loss :0.004403863567858934\n",
      "Adjusting learning rate of group 0 to 5.7485e-03.\n",
      "Adjusting learning rate of group 0 to 5.7485e-03.\n",
      "Epoch : 711,\ttrain_loss : 0.006397395394742489\tvalid_loss :0.0043405997566878796\n",
      "Adjusting learning rate of group 0 to 5.7428e-03.\n",
      "Adjusting learning rate of group 0 to 5.7428e-03.\n",
      "Epoch : 712,\ttrain_loss : 0.006356725934892893\tvalid_loss :0.004557102918624878\n",
      "Adjusting learning rate of group 0 to 5.7372e-03.\n",
      "Adjusting learning rate of group 0 to 5.7372e-03.\n",
      "Epoch : 713,\ttrain_loss : 0.006430563051253557\tvalid_loss :0.004721997305750847\n",
      "Adjusting learning rate of group 0 to 5.7315e-03.\n",
      "Adjusting learning rate of group 0 to 5.7315e-03.\n",
      "Epoch : 714,\ttrain_loss : 0.006398200523108244\tvalid_loss :0.004364575259387493\n",
      "Adjusting learning rate of group 0 to 5.7258e-03.\n",
      "Adjusting learning rate of group 0 to 5.7258e-03.\n",
      "Epoch : 715,\ttrain_loss : 0.006552543956786394\tvalid_loss :0.0050001246854662895\n",
      "Adjusting learning rate of group 0 to 5.7202e-03.\n",
      "Adjusting learning rate of group 0 to 5.7202e-03.\n",
      "Epoch : 716,\ttrain_loss : 0.006447273772209883\tvalid_loss :0.00456620380282402\n",
      "Adjusting learning rate of group 0 to 5.7145e-03.\n",
      "Adjusting learning rate of group 0 to 5.7145e-03.\n",
      "Epoch : 717,\ttrain_loss : 0.006499794777482748\tvalid_loss :0.004754740744829178\n",
      "Adjusting learning rate of group 0 to 5.7088e-03.\n",
      "Adjusting learning rate of group 0 to 5.7088e-03.\n",
      "Epoch : 718,\ttrain_loss : 0.006567701231688261\tvalid_loss :0.0043871705420315266\n",
      "Adjusting learning rate of group 0 to 5.7031e-03.\n",
      "Adjusting learning rate of group 0 to 5.7031e-03.\n",
      "Epoch : 719,\ttrain_loss : 0.006550497375428677\tvalid_loss :0.004502232652157545\n",
      "Adjusting learning rate of group 0 to 5.6974e-03.\n",
      "Adjusting learning rate of group 0 to 5.6974e-03.\n",
      "Epoch : 720,\ttrain_loss : 0.007472394499927759\tvalid_loss :0.005573437549173832\n",
      "Adjusting learning rate of group 0 to 5.6917e-03.\n",
      "Adjusting learning rate of group 0 to 5.6917e-03.\n",
      "Epoch : 721,\ttrain_loss : 0.006642826367169619\tvalid_loss :0.006306839641183615\n",
      "Adjusting learning rate of group 0 to 5.6860e-03.\n",
      "Adjusting learning rate of group 0 to 5.6860e-03.\n",
      "Epoch : 722,\ttrain_loss : 0.006644986569881439\tvalid_loss :0.005007127299904823\n",
      "Adjusting learning rate of group 0 to 5.6803e-03.\n",
      "Adjusting learning rate of group 0 to 5.6803e-03.\n",
      "Epoch : 723,\ttrain_loss : 0.006543754134327173\tvalid_loss :0.004564947914332151\n",
      "Adjusting learning rate of group 0 to 5.6746e-03.\n",
      "Adjusting learning rate of group 0 to 5.6746e-03.\n",
      "Epoch : 724,\ttrain_loss : 0.006546010263264179\tvalid_loss :0.004693252965807915\n",
      "Adjusting learning rate of group 0 to 5.6689e-03.\n",
      "Adjusting learning rate of group 0 to 5.6689e-03.\n",
      "Epoch : 725,\ttrain_loss : 0.006378762423992157\tvalid_loss :0.004576868377625942\n",
      "Adjusting learning rate of group 0 to 5.6632e-03.\n",
      "Adjusting learning rate of group 0 to 5.6632e-03.\n",
      "Epoch : 726,\ttrain_loss : 0.006329063791781664\tvalid_loss :0.004549884237349033\n",
      "Adjusting learning rate of group 0 to 5.6575e-03.\n",
      "Adjusting learning rate of group 0 to 5.6575e-03.\n",
      "Epoch : 727,\ttrain_loss : 0.006426600273698568\tvalid_loss :0.004523764364421368\n",
      "Adjusting learning rate of group 0 to 5.6518e-03.\n",
      "Adjusting learning rate of group 0 to 5.6518e-03.\n",
      "Epoch : 728,\ttrain_loss : 0.00641044182702899\tvalid_loss :0.004465349484235048\n",
      "Adjusting learning rate of group 0 to 5.6461e-03.\n",
      "Adjusting learning rate of group 0 to 5.6461e-03.\n",
      "Epoch : 729,\ttrain_loss : 0.006587936542928219\tvalid_loss :0.004324303008615971\n",
      "Adjusting learning rate of group 0 to 5.6403e-03.\n",
      "Adjusting learning rate of group 0 to 5.6403e-03.\n",
      "Epoch : 730,\ttrain_loss : 0.00653742253780365\tvalid_loss :0.0046037896536290646\n",
      "Adjusting learning rate of group 0 to 5.6346e-03.\n",
      "Adjusting learning rate of group 0 to 5.6346e-03.\n",
      "Epoch : 731,\ttrain_loss : 0.006474327761679888\tvalid_loss :0.004588678479194641\n",
      "Adjusting learning rate of group 0 to 5.6289e-03.\n",
      "Adjusting learning rate of group 0 to 5.6289e-03.\n",
      "Epoch : 732,\ttrain_loss : 0.006387811619788408\tvalid_loss :0.004290941171348095\n",
      "**********Valid loss decreased (0.004314 ==> 0.004291)**********\n",
      "Adjusting learning rate of group 0 to 5.6231e-03.\n",
      "Adjusting learning rate of group 0 to 5.6231e-03.\n",
      "Epoch : 733,\ttrain_loss : 0.006492759101092815\tvalid_loss :0.0046668085269629955\n",
      "Adjusting learning rate of group 0 to 5.6174e-03.\n",
      "Adjusting learning rate of group 0 to 5.6174e-03.\n",
      "Epoch : 734,\ttrain_loss : 0.006594032049179077\tvalid_loss :0.004486456047743559\n",
      "Adjusting learning rate of group 0 to 5.6116e-03.\n",
      "Adjusting learning rate of group 0 to 5.6116e-03.\n",
      "Epoch : 735,\ttrain_loss : 0.00639632623642683\tvalid_loss :0.004548411816358566\n",
      "Adjusting learning rate of group 0 to 5.6059e-03.\n",
      "Adjusting learning rate of group 0 to 5.6059e-03.\n",
      "Epoch : 736,\ttrain_loss : 0.006579172797501087\tvalid_loss :0.004555298946797848\n",
      "Adjusting learning rate of group 0 to 5.6001e-03.\n",
      "Adjusting learning rate of group 0 to 5.6001e-03.\n",
      "Epoch : 737,\ttrain_loss : 0.006560900714248419\tvalid_loss :0.00458117201924324\n",
      "Adjusting learning rate of group 0 to 5.5944e-03.\n",
      "Adjusting learning rate of group 0 to 5.5944e-03.\n",
      "Epoch : 738,\ttrain_loss : 0.0072172763757407665\tvalid_loss :0.006892364006489515\n",
      "Adjusting learning rate of group 0 to 5.5886e-03.\n",
      "Adjusting learning rate of group 0 to 5.5886e-03.\n",
      "Epoch : 739,\ttrain_loss : 0.007097498048096895\tvalid_loss :0.006825105287134647\n",
      "Adjusting learning rate of group 0 to 5.5828e-03.\n",
      "Adjusting learning rate of group 0 to 5.5828e-03.\n",
      "Epoch : 740,\ttrain_loss : 0.006829480174928904\tvalid_loss :0.005086557939648628\n",
      "Adjusting learning rate of group 0 to 5.5771e-03.\n",
      "Adjusting learning rate of group 0 to 5.5771e-03.\n",
      "Epoch : 741,\ttrain_loss : 0.006544693373143673\tvalid_loss :0.004827274475246668\n",
      "Adjusting learning rate of group 0 to 5.5713e-03.\n",
      "Adjusting learning rate of group 0 to 5.5713e-03.\n",
      "Epoch : 742,\ttrain_loss : 0.006490443833172321\tvalid_loss :0.005405736155807972\n",
      "Adjusting learning rate of group 0 to 5.5655e-03.\n",
      "Adjusting learning rate of group 0 to 5.5655e-03.\n",
      "Epoch : 743,\ttrain_loss : 0.00673642847687006\tvalid_loss :0.005655040964484215\n",
      "Adjusting learning rate of group 0 to 5.5597e-03.\n",
      "Adjusting learning rate of group 0 to 5.5597e-03.\n",
      "Epoch : 744,\ttrain_loss : 0.007072735112160444\tvalid_loss :0.007593782618641853\n",
      "Adjusting learning rate of group 0 to 5.5539e-03.\n",
      "Adjusting learning rate of group 0 to 5.5539e-03.\n",
      "Epoch : 745,\ttrain_loss : 0.006973440293222666\tvalid_loss :0.00491088442504406\n",
      "Adjusting learning rate of group 0 to 5.5481e-03.\n",
      "Adjusting learning rate of group 0 to 5.5481e-03.\n",
      "Epoch : 746,\ttrain_loss : 0.006542531307786703\tvalid_loss :0.004277504980564117\n",
      "**********Valid loss decreased (0.004291 ==> 0.004278)**********\n",
      "Adjusting learning rate of group 0 to 5.5423e-03.\n",
      "Adjusting learning rate of group 0 to 5.5423e-03.\n",
      "Epoch : 747,\ttrain_loss : 0.006398956291377544\tvalid_loss :0.004707774147391319\n",
      "Adjusting learning rate of group 0 to 5.5365e-03.\n",
      "Adjusting learning rate of group 0 to 5.5365e-03.\n",
      "Epoch : 748,\ttrain_loss : 0.006364230066537857\tvalid_loss :0.004241345915943384\n",
      "**********Valid loss decreased (0.004278 ==> 0.004241)**********\n",
      "Adjusting learning rate of group 0 to 5.5307e-03.\n",
      "Adjusting learning rate of group 0 to 5.5307e-03.\n",
      "Epoch : 749,\ttrain_loss : 0.006450365763157606\tvalid_loss :0.004386605694890022\n",
      "Adjusting learning rate of group 0 to 5.5249e-03.\n",
      "Adjusting learning rate of group 0 to 5.5249e-03.\n",
      "Epoch : 750,\ttrain_loss : 0.006295367144048214\tvalid_loss :0.004505009390413761\n",
      "Adjusting learning rate of group 0 to 5.5191e-03.\n",
      "Adjusting learning rate of group 0 to 5.5191e-03.\n",
      "Epoch : 751,\ttrain_loss : 0.006445594597607851\tvalid_loss :0.004514867905527353\n",
      "Adjusting learning rate of group 0 to 5.5133e-03.\n",
      "Adjusting learning rate of group 0 to 5.5133e-03.\n",
      "Epoch : 752,\ttrain_loss : 0.0063962265849113464\tvalid_loss :0.0044023459777235985\n",
      "Adjusting learning rate of group 0 to 5.5075e-03.\n",
      "Adjusting learning rate of group 0 to 5.5075e-03.\n",
      "Epoch : 753,\ttrain_loss : 0.006573353428393602\tvalid_loss :0.004936533980071545\n",
      "Adjusting learning rate of group 0 to 5.5017e-03.\n",
      "Adjusting learning rate of group 0 to 5.5017e-03.\n",
      "Epoch : 754,\ttrain_loss : 0.0069537837989628315\tvalid_loss :0.005144021008163691\n",
      "Adjusting learning rate of group 0 to 5.4958e-03.\n",
      "Adjusting learning rate of group 0 to 5.4958e-03.\n",
      "Epoch : 755,\ttrain_loss : 0.006706139538437128\tvalid_loss :0.005854110233485699\n",
      "Adjusting learning rate of group 0 to 5.4900e-03.\n",
      "Adjusting learning rate of group 0 to 5.4900e-03.\n",
      "Epoch : 756,\ttrain_loss : 0.006664033513516188\tvalid_loss :0.004549537785351276\n",
      "Adjusting learning rate of group 0 to 5.4842e-03.\n",
      "Adjusting learning rate of group 0 to 5.4842e-03.\n",
      "Epoch : 757,\ttrain_loss : 0.006657794117927551\tvalid_loss :0.004625262692570686\n",
      "Adjusting learning rate of group 0 to 5.4783e-03.\n",
      "Adjusting learning rate of group 0 to 5.4783e-03.\n",
      "Epoch : 758,\ttrain_loss : 0.006445474456995726\tvalid_loss :0.004258156754076481\n",
      "Adjusting learning rate of group 0 to 5.4725e-03.\n",
      "Adjusting learning rate of group 0 to 5.4725e-03.\n",
      "Epoch : 759,\ttrain_loss : 0.006463184487074614\tvalid_loss :0.004352828953415155\n",
      "Adjusting learning rate of group 0 to 5.4667e-03.\n",
      "Adjusting learning rate of group 0 to 5.4667e-03.\n",
      "Epoch : 760,\ttrain_loss : 0.006767868530005217\tvalid_loss :0.0055761015973985195\n",
      "Adjusting learning rate of group 0 to 5.4608e-03.\n",
      "Adjusting learning rate of group 0 to 5.4608e-03.\n",
      "Epoch : 761,\ttrain_loss : 0.006875026039779186\tvalid_loss :0.004751134198158979\n",
      "Adjusting learning rate of group 0 to 5.4550e-03.\n",
      "Adjusting learning rate of group 0 to 5.4550e-03.\n",
      "Epoch : 762,\ttrain_loss : 0.006623112130910158\tvalid_loss :0.0043348814360797405\n",
      "Adjusting learning rate of group 0 to 5.4491e-03.\n",
      "Adjusting learning rate of group 0 to 5.4491e-03.\n",
      "Epoch : 763,\ttrain_loss : 0.006582023110240698\tvalid_loss :0.0045829052105546\n",
      "Adjusting learning rate of group 0 to 5.4432e-03.\n",
      "Adjusting learning rate of group 0 to 5.4432e-03.\n",
      "Epoch : 764,\ttrain_loss : 0.0064700329676270485\tvalid_loss :0.004317363724112511\n",
      "Adjusting learning rate of group 0 to 5.4374e-03.\n",
      "Adjusting learning rate of group 0 to 5.4374e-03.\n",
      "Epoch : 765,\ttrain_loss : 0.006302145775407553\tvalid_loss :0.004239535890519619\n",
      "**********Valid loss decreased (0.004241 ==> 0.004240)**********\n",
      "Adjusting learning rate of group 0 to 5.4315e-03.\n",
      "Adjusting learning rate of group 0 to 5.4315e-03.\n",
      "Epoch : 766,\ttrain_loss : 0.006220268551260233\tvalid_loss :0.0043833330273628235\n",
      "Adjusting learning rate of group 0 to 5.4256e-03.\n",
      "Adjusting learning rate of group 0 to 5.4256e-03.\n",
      "Epoch : 767,\ttrain_loss : 0.006236668210476637\tvalid_loss :0.004521484486758709\n",
      "Adjusting learning rate of group 0 to 5.4198e-03.\n",
      "Adjusting learning rate of group 0 to 5.4198e-03.\n",
      "Epoch : 768,\ttrain_loss : 0.006603623274713755\tvalid_loss :0.004536496475338936\n",
      "Adjusting learning rate of group 0 to 5.4139e-03.\n",
      "Adjusting learning rate of group 0 to 5.4139e-03.\n",
      "Epoch : 769,\ttrain_loss : 0.006831334438174963\tvalid_loss :0.004887571092694998\n",
      "Adjusting learning rate of group 0 to 5.4080e-03.\n",
      "Adjusting learning rate of group 0 to 5.4080e-03.\n",
      "Epoch : 770,\ttrain_loss : 0.0065836780704557896\tvalid_loss :0.0046070655807852745\n",
      "Adjusting learning rate of group 0 to 5.4021e-03.\n",
      "Adjusting learning rate of group 0 to 5.4021e-03.\n",
      "Epoch : 771,\ttrain_loss : 0.006207314785569906\tvalid_loss :0.004423841368407011\n",
      "Adjusting learning rate of group 0 to 5.3963e-03.\n",
      "Adjusting learning rate of group 0 to 5.3963e-03.\n",
      "Epoch : 772,\ttrain_loss : 0.006560005713254213\tvalid_loss :0.004400860518217087\n",
      "Adjusting learning rate of group 0 to 5.3904e-03.\n",
      "Adjusting learning rate of group 0 to 5.3904e-03.\n",
      "Epoch : 773,\ttrain_loss : 0.007106281351298094\tvalid_loss :0.005480612628161907\n",
      "Adjusting learning rate of group 0 to 5.3845e-03.\n",
      "Adjusting learning rate of group 0 to 5.3845e-03.\n",
      "Epoch : 774,\ttrain_loss : 0.0064878216944634914\tvalid_loss :0.0053985752165317535\n",
      "Adjusting learning rate of group 0 to 5.3786e-03.\n",
      "Adjusting learning rate of group 0 to 5.3786e-03.\n",
      "Epoch : 775,\ttrain_loss : 0.006359835620969534\tvalid_loss :0.004448491148650646\n",
      "Adjusting learning rate of group 0 to 5.3727e-03.\n",
      "Adjusting learning rate of group 0 to 5.3727e-03.\n",
      "Epoch : 776,\ttrain_loss : 0.0064892456866800785\tvalid_loss :0.005123141221702099\n",
      "Adjusting learning rate of group 0 to 5.3668e-03.\n",
      "Adjusting learning rate of group 0 to 5.3668e-03.\n",
      "Epoch : 777,\ttrain_loss : 0.006758747156709433\tvalid_loss :0.005453815218061209\n",
      "Adjusting learning rate of group 0 to 5.3609e-03.\n",
      "Adjusting learning rate of group 0 to 5.3609e-03.\n",
      "Epoch : 778,\ttrain_loss : 0.006674437317997217\tvalid_loss :0.006265135481953621\n",
      "Adjusting learning rate of group 0 to 5.3550e-03.\n",
      "Adjusting learning rate of group 0 to 5.3550e-03.\n",
      "Epoch : 779,\ttrain_loss : 0.006713810842484236\tvalid_loss :0.004704810678958893\n",
      "Adjusting learning rate of group 0 to 5.3490e-03.\n",
      "Adjusting learning rate of group 0 to 5.3490e-03.\n",
      "Epoch : 780,\ttrain_loss : 0.007040080148726702\tvalid_loss :0.005285997875034809\n",
      "Adjusting learning rate of group 0 to 5.3431e-03.\n",
      "Adjusting learning rate of group 0 to 5.3431e-03.\n",
      "Epoch : 781,\ttrain_loss : 0.006399340927600861\tvalid_loss :0.004333265125751495\n",
      "Adjusting learning rate of group 0 to 5.3372e-03.\n",
      "Adjusting learning rate of group 0 to 5.3372e-03.\n",
      "Epoch : 782,\ttrain_loss : 0.006260561756789684\tvalid_loss :0.004378939978778362\n",
      "Adjusting learning rate of group 0 to 5.3313e-03.\n",
      "Adjusting learning rate of group 0 to 5.3313e-03.\n",
      "Epoch : 783,\ttrain_loss : 0.006762521807104349\tvalid_loss :0.005897080525755882\n",
      "Adjusting learning rate of group 0 to 5.3254e-03.\n",
      "Adjusting learning rate of group 0 to 5.3254e-03.\n",
      "Epoch : 784,\ttrain_loss : 0.0067298077046871185\tvalid_loss :0.004581617657095194\n",
      "Adjusting learning rate of group 0 to 5.3194e-03.\n",
      "Adjusting learning rate of group 0 to 5.3194e-03.\n",
      "Epoch : 785,\ttrain_loss : 0.00660348916426301\tvalid_loss :0.004250464029610157\n",
      "Adjusting learning rate of group 0 to 5.3135e-03.\n",
      "Adjusting learning rate of group 0 to 5.3135e-03.\n",
      "Epoch : 786,\ttrain_loss : 0.006320566404610872\tvalid_loss :0.004360359627753496\n",
      "Adjusting learning rate of group 0 to 5.3076e-03.\n",
      "Adjusting learning rate of group 0 to 5.3076e-03.\n",
      "Epoch : 787,\ttrain_loss : 0.0067771016620099545\tvalid_loss :0.006789073348045349\n",
      "Adjusting learning rate of group 0 to 5.3016e-03.\n",
      "Adjusting learning rate of group 0 to 5.3016e-03.\n",
      "Epoch : 788,\ttrain_loss : 0.007193551864475012\tvalid_loss :0.0043782880529761314\n",
      "Adjusting learning rate of group 0 to 5.2957e-03.\n",
      "Adjusting learning rate of group 0 to 5.2957e-03.\n",
      "Epoch : 789,\ttrain_loss : 0.006290361750870943\tvalid_loss :0.004460436757653952\n",
      "Adjusting learning rate of group 0 to 5.2897e-03.\n",
      "Adjusting learning rate of group 0 to 5.2897e-03.\n",
      "Epoch : 790,\ttrain_loss : 0.006152619142085314\tvalid_loss :0.00476668868213892\n",
      "Adjusting learning rate of group 0 to 5.2838e-03.\n",
      "Adjusting learning rate of group 0 to 5.2838e-03.\n",
      "Epoch : 791,\ttrain_loss : 0.0064647444523870945\tvalid_loss :0.004530961159616709\n",
      "Adjusting learning rate of group 0 to 5.2778e-03.\n",
      "Adjusting learning rate of group 0 to 5.2778e-03.\n",
      "Epoch : 792,\ttrain_loss : 0.006476566661149263\tvalid_loss :0.004646502435207367\n",
      "Adjusting learning rate of group 0 to 5.2719e-03.\n",
      "Adjusting learning rate of group 0 to 5.2719e-03.\n",
      "Epoch : 793,\ttrain_loss : 0.006247592158615589\tvalid_loss :0.004240027628839016\n",
      "Adjusting learning rate of group 0 to 5.2659e-03.\n",
      "Adjusting learning rate of group 0 to 5.2659e-03.\n",
      "Epoch : 794,\ttrain_loss : 0.006442435085773468\tvalid_loss :0.004309195559471846\n",
      "Adjusting learning rate of group 0 to 5.2599e-03.\n",
      "Adjusting learning rate of group 0 to 5.2599e-03.\n",
      "Epoch : 795,\ttrain_loss : 0.006988923996686935\tvalid_loss :0.004690117668360472\n",
      "Adjusting learning rate of group 0 to 5.2540e-03.\n",
      "Adjusting learning rate of group 0 to 5.2540e-03.\n",
      "Epoch : 796,\ttrain_loss : 0.006684948690235615\tvalid_loss :0.005898005329072475\n",
      "Adjusting learning rate of group 0 to 5.2480e-03.\n",
      "Adjusting learning rate of group 0 to 5.2480e-03.\n",
      "Epoch : 797,\ttrain_loss : 0.006650466937571764\tvalid_loss :0.004406679421663284\n",
      "Adjusting learning rate of group 0 to 5.2420e-03.\n",
      "Adjusting learning rate of group 0 to 5.2420e-03.\n",
      "Epoch : 798,\ttrain_loss : 0.00655152415856719\tvalid_loss :0.004490464460104704\n",
      "Adjusting learning rate of group 0 to 5.2361e-03.\n",
      "Adjusting learning rate of group 0 to 5.2361e-03.\n",
      "Epoch : 799,\ttrain_loss : 0.006717238109558821\tvalid_loss :0.004928325768560171\n",
      "Adjusting learning rate of group 0 to 5.2301e-03.\n",
      "Adjusting learning rate of group 0 to 5.2301e-03.\n",
      "Epoch : 800,\ttrain_loss : 0.006491167936474085\tvalid_loss :0.0078491922467947\n",
      "Adjusting learning rate of group 0 to 5.2241e-03.\n",
      "Adjusting learning rate of group 0 to 5.2241e-03.\n",
      "Epoch : 801,\ttrain_loss : 0.007097831927239895\tvalid_loss :0.006190069019794464\n",
      "Adjusting learning rate of group 0 to 5.2181e-03.\n",
      "Adjusting learning rate of group 0 to 5.2181e-03.\n",
      "Epoch : 802,\ttrain_loss : 0.006829846650362015\tvalid_loss :0.006765843369066715\n",
      "Adjusting learning rate of group 0 to 5.2121e-03.\n",
      "Adjusting learning rate of group 0 to 5.2121e-03.\n",
      "Epoch : 803,\ttrain_loss : 0.006935920566320419\tvalid_loss :0.006276032887399197\n",
      "Adjusting learning rate of group 0 to 5.2062e-03.\n",
      "Adjusting learning rate of group 0 to 5.2062e-03.\n",
      "Epoch : 804,\ttrain_loss : 0.00678135035559535\tvalid_loss :0.007131087593734264\n",
      "Adjusting learning rate of group 0 to 5.2002e-03.\n",
      "Adjusting learning rate of group 0 to 5.2002e-03.\n",
      "Epoch : 805,\ttrain_loss : 0.006930239498615265\tvalid_loss :0.005654543172568083\n",
      "Adjusting learning rate of group 0 to 5.1942e-03.\n",
      "Adjusting learning rate of group 0 to 5.1942e-03.\n",
      "Epoch : 806,\ttrain_loss : 0.006908383220434189\tvalid_loss :0.005270287860184908\n",
      "Adjusting learning rate of group 0 to 5.1882e-03.\n",
      "Adjusting learning rate of group 0 to 5.1882e-03.\n",
      "Epoch : 807,\ttrain_loss : 0.006628880742937326\tvalid_loss :0.005330946762114763\n",
      "Adjusting learning rate of group 0 to 5.1822e-03.\n",
      "Adjusting learning rate of group 0 to 5.1822e-03.\n",
      "Epoch : 808,\ttrain_loss : 0.006409596651792526\tvalid_loss :0.0043717180378735065\n",
      "Adjusting learning rate of group 0 to 5.1762e-03.\n",
      "Adjusting learning rate of group 0 to 5.1762e-03.\n",
      "Epoch : 809,\ttrain_loss : 0.006465394515544176\tvalid_loss :0.005780495703220367\n",
      "Adjusting learning rate of group 0 to 5.1702e-03.\n",
      "Adjusting learning rate of group 0 to 5.1702e-03.\n",
      "Epoch : 810,\ttrain_loss : 0.006927380803972483\tvalid_loss :0.004905167035758495\n",
      "Adjusting learning rate of group 0 to 5.1641e-03.\n",
      "Adjusting learning rate of group 0 to 5.1641e-03.\n",
      "Epoch : 811,\ttrain_loss : 0.006424104329198599\tvalid_loss :0.0048510730266571045\n",
      "Adjusting learning rate of group 0 to 5.1581e-03.\n",
      "Adjusting learning rate of group 0 to 5.1581e-03.\n",
      "Epoch : 812,\ttrain_loss : 0.006368555594235659\tvalid_loss :0.004232105799019337\n",
      "**********Valid loss decreased (0.004240 ==> 0.004232)**********\n",
      "Adjusting learning rate of group 0 to 5.1521e-03.\n",
      "Adjusting learning rate of group 0 to 5.1521e-03.\n",
      "Epoch : 813,\ttrain_loss : 0.006865467876195908\tvalid_loss :0.00461494131013751\n",
      "Adjusting learning rate of group 0 to 5.1461e-03.\n",
      "Adjusting learning rate of group 0 to 5.1461e-03.\n",
      "Epoch : 814,\ttrain_loss : 0.006716418545693159\tvalid_loss :0.005011344328522682\n",
      "Adjusting learning rate of group 0 to 5.1401e-03.\n",
      "Adjusting learning rate of group 0 to 5.1401e-03.\n",
      "Epoch : 815,\ttrain_loss : 0.006414598785340786\tvalid_loss :0.005373998079448938\n",
      "Adjusting learning rate of group 0 to 5.1341e-03.\n",
      "Adjusting learning rate of group 0 to 5.1341e-03.\n",
      "Epoch : 816,\ttrain_loss : 0.00651031406596303\tvalid_loss :0.004225690383464098\n",
      "**********Valid loss decreased (0.004232 ==> 0.004226)**********\n",
      "Adjusting learning rate of group 0 to 5.1280e-03.\n",
      "Adjusting learning rate of group 0 to 5.1280e-03.\n",
      "Epoch : 817,\ttrain_loss : 0.00692069623619318\tvalid_loss :0.005867502652108669\n",
      "Adjusting learning rate of group 0 to 5.1220e-03.\n",
      "Adjusting learning rate of group 0 to 5.1220e-03.\n",
      "Epoch : 818,\ttrain_loss : 0.006608420051634312\tvalid_loss :0.0050094774924218655\n",
      "Adjusting learning rate of group 0 to 5.1160e-03.\n",
      "Adjusting learning rate of group 0 to 5.1160e-03.\n",
      "Epoch : 819,\ttrain_loss : 0.006298658438026905\tvalid_loss :0.004393658600747585\n",
      "Adjusting learning rate of group 0 to 5.1099e-03.\n",
      "Adjusting learning rate of group 0 to 5.1099e-03.\n",
      "Epoch : 820,\ttrain_loss : 0.006972894538193941\tvalid_loss :0.005397878121584654\n",
      "Adjusting learning rate of group 0 to 5.1039e-03.\n",
      "Adjusting learning rate of group 0 to 5.1039e-03.\n",
      "Epoch : 821,\ttrain_loss : 0.006549588404595852\tvalid_loss :0.0069421022199094296\n",
      "Adjusting learning rate of group 0 to 5.0979e-03.\n",
      "Adjusting learning rate of group 0 to 5.0979e-03.\n",
      "Epoch : 822,\ttrain_loss : 0.006451568100601435\tvalid_loss :0.006952456198632717\n",
      "Adjusting learning rate of group 0 to 5.0918e-03.\n",
      "Adjusting learning rate of group 0 to 5.0918e-03.\n",
      "Epoch : 823,\ttrain_loss : 0.006722731981426477\tvalid_loss :0.0054195234552025795\n",
      "Adjusting learning rate of group 0 to 5.0858e-03.\n",
      "Adjusting learning rate of group 0 to 5.0858e-03.\n",
      "Epoch : 824,\ttrain_loss : 0.006518151145428419\tvalid_loss :0.004514035768806934\n",
      "Adjusting learning rate of group 0 to 5.0797e-03.\n",
      "Adjusting learning rate of group 0 to 5.0797e-03.\n",
      "Epoch : 825,\ttrain_loss : 0.006435375194996595\tvalid_loss :0.004296149127185345\n",
      "Adjusting learning rate of group 0 to 5.0737e-03.\n",
      "Adjusting learning rate of group 0 to 5.0737e-03.\n",
      "Epoch : 826,\ttrain_loss : 0.006793079432100058\tvalid_loss :0.00556165399029851\n",
      "Adjusting learning rate of group 0 to 5.0676e-03.\n",
      "Adjusting learning rate of group 0 to 5.0676e-03.\n",
      "Epoch : 827,\ttrain_loss : 0.006646317895501852\tvalid_loss :0.006857160944491625\n",
      "Adjusting learning rate of group 0 to 5.0616e-03.\n",
      "Adjusting learning rate of group 0 to 5.0616e-03.\n",
      "Epoch : 828,\ttrain_loss : 0.00656880671158433\tvalid_loss :0.006121654994785786\n",
      "Adjusting learning rate of group 0 to 5.0555e-03.\n",
      "Adjusting learning rate of group 0 to 5.0555e-03.\n",
      "Epoch : 829,\ttrain_loss : 0.006495026405900717\tvalid_loss :0.004226404707878828\n",
      "Adjusting learning rate of group 0 to 5.0494e-03.\n",
      "Adjusting learning rate of group 0 to 5.0494e-03.\n",
      "Epoch : 830,\ttrain_loss : 0.0069212159141898155\tvalid_loss :0.005740932654589415\n",
      "Adjusting learning rate of group 0 to 5.0434e-03.\n",
      "Adjusting learning rate of group 0 to 5.0434e-03.\n",
      "Epoch : 831,\ttrain_loss : 0.006660386919975281\tvalid_loss :0.004816888831555843\n",
      "Adjusting learning rate of group 0 to 5.0373e-03.\n",
      "Adjusting learning rate of group 0 to 5.0373e-03.\n",
      "Epoch : 832,\ttrain_loss : 0.006378257181495428\tvalid_loss :0.006040438078343868\n",
      "Adjusting learning rate of group 0 to 5.0312e-03.\n",
      "Adjusting learning rate of group 0 to 5.0312e-03.\n",
      "Epoch : 833,\ttrain_loss : 0.006857776083052158\tvalid_loss :0.004409153945744038\n",
      "Adjusting learning rate of group 0 to 5.0252e-03.\n",
      "Adjusting learning rate of group 0 to 5.0252e-03.\n",
      "Epoch : 834,\ttrain_loss : 0.006475039292126894\tvalid_loss :0.004551466554403305\n",
      "Adjusting learning rate of group 0 to 5.0191e-03.\n",
      "Adjusting learning rate of group 0 to 5.0191e-03.\n",
      "Epoch : 835,\ttrain_loss : 0.0062976558692753315\tvalid_loss :0.004352373071014881\n",
      "Adjusting learning rate of group 0 to 5.0130e-03.\n",
      "Adjusting learning rate of group 0 to 5.0130e-03.\n",
      "Epoch : 836,\ttrain_loss : 0.006823767442256212\tvalid_loss :0.004971812944859266\n",
      "Adjusting learning rate of group 0 to 5.0069e-03.\n",
      "Adjusting learning rate of group 0 to 5.0069e-03.\n",
      "Epoch : 837,\ttrain_loss : 0.00657431036233902\tvalid_loss :0.006130191497504711\n",
      "Adjusting learning rate of group 0 to 5.0008e-03.\n",
      "Adjusting learning rate of group 0 to 5.0008e-03.\n",
      "Epoch : 838,\ttrain_loss : 0.006518864072859287\tvalid_loss :0.005677465815097094\n",
      "Adjusting learning rate of group 0 to 4.9948e-03.\n",
      "Adjusting learning rate of group 0 to 4.9948e-03.\n",
      "Epoch : 839,\ttrain_loss : 0.0063743069767951965\tvalid_loss :0.004481919575482607\n",
      "Adjusting learning rate of group 0 to 4.9887e-03.\n",
      "Adjusting learning rate of group 0 to 4.9887e-03.\n",
      "Epoch : 840,\ttrain_loss : 0.0064770556055009365\tvalid_loss :0.006322770845144987\n",
      "Adjusting learning rate of group 0 to 4.9826e-03.\n",
      "Adjusting learning rate of group 0 to 4.9826e-03.\n",
      "Epoch : 841,\ttrain_loss : 0.006990486290305853\tvalid_loss :0.004741457756608725\n",
      "Adjusting learning rate of group 0 to 4.9765e-03.\n",
      "Adjusting learning rate of group 0 to 4.9765e-03.\n",
      "Epoch : 842,\ttrain_loss : 0.006424189545214176\tvalid_loss :0.0050896406173706055\n",
      "Adjusting learning rate of group 0 to 4.9704e-03.\n",
      "Adjusting learning rate of group 0 to 4.9704e-03.\n",
      "Epoch : 843,\ttrain_loss : 0.00643713865429163\tvalid_loss :0.004258579108864069\n",
      "Adjusting learning rate of group 0 to 4.9643e-03.\n",
      "Adjusting learning rate of group 0 to 4.9643e-03.\n",
      "Epoch : 844,\ttrain_loss : 0.006931119132786989\tvalid_loss :0.006193502806127071\n",
      "Adjusting learning rate of group 0 to 4.9582e-03.\n",
      "Adjusting learning rate of group 0 to 4.9582e-03.\n",
      "Epoch : 845,\ttrain_loss : 0.006808632053434849\tvalid_loss :0.004895472899079323\n",
      "Adjusting learning rate of group 0 to 4.9521e-03.\n",
      "Adjusting learning rate of group 0 to 4.9521e-03.\n",
      "Epoch : 846,\ttrain_loss : 0.006568321026861668\tvalid_loss :0.00494221830740571\n",
      "Adjusting learning rate of group 0 to 4.9460e-03.\n",
      "Adjusting learning rate of group 0 to 4.9460e-03.\n",
      "Epoch : 847,\ttrain_loss : 0.006595217157155275\tvalid_loss :0.0076375482603907585\n",
      "Adjusting learning rate of group 0 to 4.9399e-03.\n",
      "Adjusting learning rate of group 0 to 4.9399e-03.\n",
      "Epoch : 848,\ttrain_loss : 0.007692009210586548\tvalid_loss :0.005494831595569849\n",
      "Adjusting learning rate of group 0 to 4.9338e-03.\n",
      "Adjusting learning rate of group 0 to 4.9338e-03.\n",
      "Epoch : 849,\ttrain_loss : 0.006511983461678028\tvalid_loss :0.00440254807472229\n",
      "Adjusting learning rate of group 0 to 4.9277e-03.\n",
      "Adjusting learning rate of group 0 to 4.9277e-03.\n",
      "Epoch : 850,\ttrain_loss : 0.006985709071159363\tvalid_loss :0.004713932517915964\n",
      "Adjusting learning rate of group 0 to 4.9216e-03.\n",
      "Adjusting learning rate of group 0 to 4.9216e-03.\n",
      "Epoch : 851,\ttrain_loss : 0.006432234775274992\tvalid_loss :0.006392988376319408\n",
      "Adjusting learning rate of group 0 to 4.9154e-03.\n",
      "Adjusting learning rate of group 0 to 4.9154e-03.\n",
      "Epoch : 852,\ttrain_loss : 0.006663865875452757\tvalid_loss :0.005840126425027847\n",
      "Adjusting learning rate of group 0 to 4.9093e-03.\n",
      "Adjusting learning rate of group 0 to 4.9093e-03.\n",
      "Epoch : 853,\ttrain_loss : 0.006894584745168686\tvalid_loss :0.0046985335648059845\n",
      "Adjusting learning rate of group 0 to 4.9032e-03.\n",
      "Adjusting learning rate of group 0 to 4.9032e-03.\n",
      "Epoch : 854,\ttrain_loss : 0.006584798451513052\tvalid_loss :0.005258684046566486\n",
      "Adjusting learning rate of group 0 to 4.8971e-03.\n",
      "Adjusting learning rate of group 0 to 4.8971e-03.\n",
      "Epoch : 855,\ttrain_loss : 0.0068478332832455635\tvalid_loss :0.004517296329140663\n",
      "Adjusting learning rate of group 0 to 4.8910e-03.\n",
      "Adjusting learning rate of group 0 to 4.8910e-03.\n",
      "Epoch : 856,\ttrain_loss : 0.0066007934510707855\tvalid_loss :0.004559251479804516\n",
      "Adjusting learning rate of group 0 to 4.8848e-03.\n",
      "Adjusting learning rate of group 0 to 4.8848e-03.\n",
      "Epoch : 857,\ttrain_loss : 0.0065011833794415\tvalid_loss :0.00519654480740428\n",
      "Adjusting learning rate of group 0 to 4.8787e-03.\n",
      "Adjusting learning rate of group 0 to 4.8787e-03.\n",
      "Epoch : 858,\ttrain_loss : 0.006786107551306486\tvalid_loss :0.0045679179020226\n",
      "Adjusting learning rate of group 0 to 4.8726e-03.\n",
      "Adjusting learning rate of group 0 to 4.8726e-03.\n",
      "Epoch : 859,\ttrain_loss : 0.006594367325305939\tvalid_loss :0.0051598455756902695\n",
      "Adjusting learning rate of group 0 to 4.8664e-03.\n",
      "Adjusting learning rate of group 0 to 4.8664e-03.\n",
      "Epoch : 860,\ttrain_loss : 0.006858114153146744\tvalid_loss :0.004511557519435883\n",
      "Adjusting learning rate of group 0 to 4.8603e-03.\n",
      "Adjusting learning rate of group 0 to 4.8603e-03.\n",
      "Epoch : 861,\ttrain_loss : 0.006485820282250643\tvalid_loss :0.005314822308719158\n",
      "Adjusting learning rate of group 0 to 4.8542e-03.\n",
      "Adjusting learning rate of group 0 to 4.8542e-03.\n",
      "Epoch : 862,\ttrain_loss : 0.006796892732381821\tvalid_loss :0.0044702510349452496\n",
      "Adjusting learning rate of group 0 to 4.8480e-03.\n",
      "Adjusting learning rate of group 0 to 4.8480e-03.\n",
      "Epoch : 863,\ttrain_loss : 0.006575964856892824\tvalid_loss :0.004941941238939762\n",
      "Adjusting learning rate of group 0 to 4.8419e-03.\n",
      "Adjusting learning rate of group 0 to 4.8419e-03.\n",
      "Epoch : 864,\ttrain_loss : 0.006678494159132242\tvalid_loss :0.004404574166983366\n",
      "Adjusting learning rate of group 0 to 4.8357e-03.\n",
      "Adjusting learning rate of group 0 to 4.8357e-03.\n",
      "Epoch : 865,\ttrain_loss : 0.006398049648851156\tvalid_loss :0.005867599043995142\n",
      "Adjusting learning rate of group 0 to 4.8296e-03.\n",
      "Adjusting learning rate of group 0 to 4.8296e-03.\n",
      "Epoch : 866,\ttrain_loss : 0.006807859521359205\tvalid_loss :0.004494881723076105\n",
      "Adjusting learning rate of group 0 to 4.8235e-03.\n",
      "Adjusting learning rate of group 0 to 4.8235e-03.\n",
      "Epoch : 867,\ttrain_loss : 0.006465965416282415\tvalid_loss :0.005824117921292782\n",
      "Adjusting learning rate of group 0 to 4.8173e-03.\n",
      "Adjusting learning rate of group 0 to 4.8173e-03.\n",
      "Epoch : 868,\ttrain_loss : 0.006787207443267107\tvalid_loss :0.004606037866324186\n",
      "Adjusting learning rate of group 0 to 4.8111e-03.\n",
      "Adjusting learning rate of group 0 to 4.8111e-03.\n",
      "Epoch : 869,\ttrain_loss : 0.006613908801227808\tvalid_loss :0.0049718706868588924\n",
      "Adjusting learning rate of group 0 to 4.8050e-03.\n",
      "Adjusting learning rate of group 0 to 4.8050e-03.\n",
      "Epoch : 870,\ttrain_loss : 0.006735553499311209\tvalid_loss :0.0052122632041573524\n",
      "Adjusting learning rate of group 0 to 4.7988e-03.\n",
      "Adjusting learning rate of group 0 to 4.7988e-03.\n",
      "Epoch : 871,\ttrain_loss : 0.006787965539842844\tvalid_loss :0.0050986516289412975\n",
      "Adjusting learning rate of group 0 to 4.7927e-03.\n",
      "Adjusting learning rate of group 0 to 4.7927e-03.\n",
      "Epoch : 872,\ttrain_loss : 0.006816613022238016\tvalid_loss :0.0051032076589763165\n",
      "Adjusting learning rate of group 0 to 4.7865e-03.\n",
      "Adjusting learning rate of group 0 to 4.7865e-03.\n",
      "Epoch : 873,\ttrain_loss : 0.006762233562767506\tvalid_loss :0.004745355807244778\n",
      "Adjusting learning rate of group 0 to 4.7804e-03.\n",
      "Adjusting learning rate of group 0 to 4.7804e-03.\n",
      "Epoch : 874,\ttrain_loss : 0.006685736123472452\tvalid_loss :0.00488806888461113\n",
      "Adjusting learning rate of group 0 to 4.7742e-03.\n",
      "Adjusting learning rate of group 0 to 4.7742e-03.\n",
      "Epoch : 875,\ttrain_loss : 0.006834839005023241\tvalid_loss :0.005687250755727291\n",
      "Adjusting learning rate of group 0 to 4.7680e-03.\n",
      "Adjusting learning rate of group 0 to 4.7680e-03.\n",
      "Epoch : 876,\ttrain_loss : 0.006959095131605864\tvalid_loss :0.006404582876712084\n",
      "Adjusting learning rate of group 0 to 4.7619e-03.\n",
      "Adjusting learning rate of group 0 to 4.7619e-03.\n",
      "Epoch : 877,\ttrain_loss : 0.0071431295946240425\tvalid_loss :0.004944170359522104\n",
      "Adjusting learning rate of group 0 to 4.7557e-03.\n",
      "Adjusting learning rate of group 0 to 4.7557e-03.\n",
      "Epoch : 878,\ttrain_loss : 0.006686457432806492\tvalid_loss :0.00583474850282073\n",
      "Adjusting learning rate of group 0 to 4.7495e-03.\n",
      "Adjusting learning rate of group 0 to 4.7495e-03.\n",
      "Epoch : 879,\ttrain_loss : 0.007038234267383814\tvalid_loss :0.006331217475235462\n",
      "Adjusting learning rate of group 0 to 4.7434e-03.\n",
      "Adjusting learning rate of group 0 to 4.7434e-03.\n",
      "Epoch : 880,\ttrain_loss : 0.007111005950719118\tvalid_loss :0.00467267632484436\n",
      "Adjusting learning rate of group 0 to 4.7372e-03.\n",
      "Adjusting learning rate of group 0 to 4.7372e-03.\n",
      "Epoch : 881,\ttrain_loss : 0.006875352468341589\tvalid_loss :0.004481096286326647\n",
      "Adjusting learning rate of group 0 to 4.7310e-03.\n",
      "Adjusting learning rate of group 0 to 4.7310e-03.\n",
      "Epoch : 882,\ttrain_loss : 0.007034730166196823\tvalid_loss :0.00444002915173769\n",
      "Adjusting learning rate of group 0 to 4.7248e-03.\n",
      "Adjusting learning rate of group 0 to 4.7248e-03.\n",
      "Epoch : 883,\ttrain_loss : 0.006812764797359705\tvalid_loss :0.0044091749005019665\n",
      "Adjusting learning rate of group 0 to 4.7186e-03.\n",
      "Adjusting learning rate of group 0 to 4.7186e-03.\n",
      "Epoch : 884,\ttrain_loss : 0.006648663897067308\tvalid_loss :0.004370150621980429\n",
      "Adjusting learning rate of group 0 to 4.7125e-03.\n",
      "Adjusting learning rate of group 0 to 4.7125e-03.\n",
      "Epoch : 885,\ttrain_loss : 0.006625074427574873\tvalid_loss :0.004307982511818409\n",
      "Adjusting learning rate of group 0 to 4.7063e-03.\n",
      "Adjusting learning rate of group 0 to 4.7063e-03.\n",
      "Epoch : 886,\ttrain_loss : 0.006553861312568188\tvalid_loss :0.004728911444544792\n",
      "Adjusting learning rate of group 0 to 4.7001e-03.\n",
      "Adjusting learning rate of group 0 to 4.7001e-03.\n",
      "Epoch : 887,\ttrain_loss : 0.006380298640578985\tvalid_loss :0.00610906258225441\n",
      "Adjusting learning rate of group 0 to 4.6939e-03.\n",
      "Adjusting learning rate of group 0 to 4.6939e-03.\n",
      "Epoch : 888,\ttrain_loss : 0.006554985884577036\tvalid_loss :0.005994886159896851\n",
      "Adjusting learning rate of group 0 to 4.6877e-03.\n",
      "Adjusting learning rate of group 0 to 4.6877e-03.\n",
      "Epoch : 889,\ttrain_loss : 0.006686490494757891\tvalid_loss :0.0048136101104319096\n",
      "Adjusting learning rate of group 0 to 4.6815e-03.\n",
      "Adjusting learning rate of group 0 to 4.6815e-03.\n",
      "Epoch : 890,\ttrain_loss : 0.00636512553319335\tvalid_loss :0.005790228024125099\n",
      "Adjusting learning rate of group 0 to 4.6753e-03.\n",
      "Adjusting learning rate of group 0 to 4.6753e-03.\n",
      "Epoch : 891,\ttrain_loss : 0.006656393874436617\tvalid_loss :0.004705994855612516\n",
      "Adjusting learning rate of group 0 to 4.6691e-03.\n",
      "Adjusting learning rate of group 0 to 4.6691e-03.\n",
      "Epoch : 892,\ttrain_loss : 0.006482730153948069\tvalid_loss :0.005511659197509289\n",
      "Adjusting learning rate of group 0 to 4.6629e-03.\n",
      "Adjusting learning rate of group 0 to 4.6629e-03.\n",
      "Epoch : 893,\ttrain_loss : 0.006405244581401348\tvalid_loss :0.005451602395623922\n",
      "Adjusting learning rate of group 0 to 4.6567e-03.\n",
      "Adjusting learning rate of group 0 to 4.6567e-03.\n",
      "Epoch : 894,\ttrain_loss : 0.0066283708438277245\tvalid_loss :0.0048525771126151085\n",
      "Adjusting learning rate of group 0 to 4.6505e-03.\n",
      "Adjusting learning rate of group 0 to 4.6505e-03.\n",
      "Epoch : 895,\ttrain_loss : 0.006414559669792652\tvalid_loss :0.005473980214446783\n",
      "Adjusting learning rate of group 0 to 4.6443e-03.\n",
      "Adjusting learning rate of group 0 to 4.6443e-03.\n",
      "Epoch : 896,\ttrain_loss : 0.006393516901880503\tvalid_loss :0.005726831965148449\n",
      "Adjusting learning rate of group 0 to 4.6381e-03.\n",
      "Adjusting learning rate of group 0 to 4.6381e-03.\n",
      "Epoch : 897,\ttrain_loss : 0.006528053432703018\tvalid_loss :0.005145435221493244\n",
      "Adjusting learning rate of group 0 to 4.6319e-03.\n",
      "Adjusting learning rate of group 0 to 4.6319e-03.\n",
      "Epoch : 898,\ttrain_loss : 0.006463773548603058\tvalid_loss :0.00494174100458622\n",
      "Adjusting learning rate of group 0 to 4.6257e-03.\n",
      "Adjusting learning rate of group 0 to 4.6257e-03.\n",
      "Epoch : 899,\ttrain_loss : 0.006436617579311132\tvalid_loss :0.005394347012042999\n",
      "Adjusting learning rate of group 0 to 4.6195e-03.\n",
      "Adjusting learning rate of group 0 to 4.6195e-03.\n",
      "Epoch : 900,\ttrain_loss : 0.006393297575414181\tvalid_loss :0.005377671681344509\n",
      "Adjusting learning rate of group 0 to 4.6133e-03.\n",
      "Adjusting learning rate of group 0 to 4.6133e-03.\n",
      "Epoch : 901,\ttrain_loss : 0.00660121813416481\tvalid_loss :0.005081958137452602\n",
      "Adjusting learning rate of group 0 to 4.6071e-03.\n",
      "Adjusting learning rate of group 0 to 4.6071e-03.\n",
      "Epoch : 902,\ttrain_loss : 0.00641896715387702\tvalid_loss :0.00502450205385685\n",
      "Adjusting learning rate of group 0 to 4.6009e-03.\n",
      "Adjusting learning rate of group 0 to 4.6009e-03.\n",
      "Epoch : 903,\ttrain_loss : 0.006473011802881956\tvalid_loss :0.005202156491577625\n",
      "Adjusting learning rate of group 0 to 4.5947e-03.\n",
      "Adjusting learning rate of group 0 to 4.5947e-03.\n",
      "Epoch : 904,\ttrain_loss : 0.006521825212985277\tvalid_loss :0.004742290824651718\n",
      "Adjusting learning rate of group 0 to 4.5885e-03.\n",
      "Adjusting learning rate of group 0 to 4.5885e-03.\n",
      "Epoch : 905,\ttrain_loss : 0.006360033992677927\tvalid_loss :0.006371453404426575\n",
      "Adjusting learning rate of group 0 to 4.5823e-03.\n",
      "Adjusting learning rate of group 0 to 4.5823e-03.\n",
      "Epoch : 906,\ttrain_loss : 0.0065291812643408775\tvalid_loss :0.00584654975682497\n",
      "Adjusting learning rate of group 0 to 4.5760e-03.\n",
      "Adjusting learning rate of group 0 to 4.5760e-03.\n",
      "Epoch : 907,\ttrain_loss : 0.006596148479729891\tvalid_loss :0.004705199506133795\n",
      "Adjusting learning rate of group 0 to 4.5698e-03.\n",
      "Adjusting learning rate of group 0 to 4.5698e-03.\n",
      "Epoch : 908,\ttrain_loss : 0.006400850135833025\tvalid_loss :0.005629639606922865\n",
      "Adjusting learning rate of group 0 to 4.5636e-03.\n",
      "Adjusting learning rate of group 0 to 4.5636e-03.\n",
      "Epoch : 909,\ttrain_loss : 0.006498996634036303\tvalid_loss :0.004788839723914862\n",
      "Adjusting learning rate of group 0 to 4.5574e-03.\n",
      "Adjusting learning rate of group 0 to 4.5574e-03.\n",
      "Epoch : 910,\ttrain_loss : 0.006642422638833523\tvalid_loss :0.0047612437047064304\n",
      "Adjusting learning rate of group 0 to 4.5512e-03.\n",
      "Adjusting learning rate of group 0 to 4.5512e-03.\n",
      "Epoch : 911,\ttrain_loss : 0.006252963561564684\tvalid_loss :0.00570640666410327\n",
      "Adjusting learning rate of group 0 to 4.5449e-03.\n",
      "Adjusting learning rate of group 0 to 4.5449e-03.\n",
      "Epoch : 912,\ttrain_loss : 0.006462202873080969\tvalid_loss :0.0061649433337152\n",
      "Adjusting learning rate of group 0 to 4.5387e-03.\n",
      "Adjusting learning rate of group 0 to 4.5387e-03.\n",
      "Epoch : 913,\ttrain_loss : 0.006560417357832193\tvalid_loss :0.005065450444817543\n",
      "Adjusting learning rate of group 0 to 4.5325e-03.\n",
      "Adjusting learning rate of group 0 to 4.5325e-03.\n",
      "Epoch : 914,\ttrain_loss : 0.006426401901990175\tvalid_loss :0.005255924537777901\n",
      "Adjusting learning rate of group 0 to 4.5263e-03.\n",
      "Adjusting learning rate of group 0 to 4.5263e-03.\n",
      "Epoch : 915,\ttrain_loss : 0.006616073194891214\tvalid_loss :0.0045409598387777805\n",
      "Adjusting learning rate of group 0 to 4.5200e-03.\n",
      "Adjusting learning rate of group 0 to 4.5200e-03.\n",
      "Epoch : 916,\ttrain_loss : 0.006558922119438648\tvalid_loss :0.004634300246834755\n",
      "Adjusting learning rate of group 0 to 4.5138e-03.\n",
      "Adjusting learning rate of group 0 to 4.5138e-03.\n",
      "Epoch : 917,\ttrain_loss : 0.006401676684617996\tvalid_loss :0.005666299723088741\n",
      "Adjusting learning rate of group 0 to 4.5076e-03.\n",
      "Adjusting learning rate of group 0 to 4.5076e-03.\n",
      "Epoch : 918,\ttrain_loss : 0.00631787208840251\tvalid_loss :0.005425971932709217\n",
      "Adjusting learning rate of group 0 to 4.5013e-03.\n",
      "Adjusting learning rate of group 0 to 4.5013e-03.\n",
      "Epoch : 919,\ttrain_loss : 0.006380249280482531\tvalid_loss :0.004610187374055386\n",
      "Adjusting learning rate of group 0 to 4.4951e-03.\n",
      "Adjusting learning rate of group 0 to 4.4951e-03.\n",
      "Epoch : 920,\ttrain_loss : 0.006475505419075489\tvalid_loss :0.0054316590540111065\n",
      "Adjusting learning rate of group 0 to 4.4889e-03.\n",
      "Adjusting learning rate of group 0 to 4.4889e-03.\n",
      "Epoch : 921,\ttrain_loss : 0.006538350600749254\tvalid_loss :0.005174283403903246\n",
      "Adjusting learning rate of group 0 to 4.4826e-03.\n",
      "Adjusting learning rate of group 0 to 4.4826e-03.\n",
      "Epoch : 922,\ttrain_loss : 0.006396861281245947\tvalid_loss :0.00546621810644865\n",
      "Adjusting learning rate of group 0 to 4.4764e-03.\n",
      "Adjusting learning rate of group 0 to 4.4764e-03.\n",
      "Epoch : 923,\ttrain_loss : 0.006457544397562742\tvalid_loss :0.004935052711516619\n",
      "Adjusting learning rate of group 0 to 4.4701e-03.\n",
      "Adjusting learning rate of group 0 to 4.4701e-03.\n",
      "Epoch : 924,\ttrain_loss : 0.006419024430215359\tvalid_loss :0.0054181721061468124\n",
      "Adjusting learning rate of group 0 to 4.4639e-03.\n",
      "Adjusting learning rate of group 0 to 4.4639e-03.\n",
      "Epoch : 925,\ttrain_loss : 0.006415958981961012\tvalid_loss :0.005726290866732597\n",
      "Adjusting learning rate of group 0 to 4.4577e-03.\n",
      "Adjusting learning rate of group 0 to 4.4577e-03.\n",
      "Epoch : 926,\ttrain_loss : 0.006509200669825077\tvalid_loss :0.00480970973148942\n",
      "Adjusting learning rate of group 0 to 4.4514e-03.\n",
      "Adjusting learning rate of group 0 to 4.4514e-03.\n",
      "Epoch : 927,\ttrain_loss : 0.00620904890820384\tvalid_loss :0.005074653774499893\n",
      "Adjusting learning rate of group 0 to 4.4452e-03.\n",
      "Adjusting learning rate of group 0 to 4.4452e-03.\n",
      "Epoch : 928,\ttrain_loss : 0.006254374980926514\tvalid_loss :0.005717966705560684\n",
      "Adjusting learning rate of group 0 to 4.4389e-03.\n",
      "Adjusting learning rate of group 0 to 4.4389e-03.\n",
      "Epoch : 929,\ttrain_loss : 0.0065016113221645355\tvalid_loss :0.004625537432730198\n",
      "Adjusting learning rate of group 0 to 4.4327e-03.\n",
      "Adjusting learning rate of group 0 to 4.4327e-03.\n",
      "Epoch : 930,\ttrain_loss : 0.006627348251640797\tvalid_loss :0.004445195663720369\n",
      "Adjusting learning rate of group 0 to 4.4264e-03.\n",
      "Adjusting learning rate of group 0 to 4.4264e-03.\n",
      "Epoch : 931,\ttrain_loss : 0.006631318014115095\tvalid_loss :0.004370225127786398\n",
      "Adjusting learning rate of group 0 to 4.4202e-03.\n",
      "Adjusting learning rate of group 0 to 4.4202e-03.\n",
      "Epoch : 932,\ttrain_loss : 0.006632461212575436\tvalid_loss :0.004934045486152172\n",
      "Adjusting learning rate of group 0 to 4.4139e-03.\n",
      "Adjusting learning rate of group 0 to 4.4139e-03.\n",
      "Epoch : 933,\ttrain_loss : 0.006424910854548216\tvalid_loss :0.005260860547423363\n",
      "Adjusting learning rate of group 0 to 4.4077e-03.\n",
      "Adjusting learning rate of group 0 to 4.4077e-03.\n",
      "Epoch : 934,\ttrain_loss : 0.00631572213023901\tvalid_loss :0.005595377180725336\n",
      "Adjusting learning rate of group 0 to 4.4014e-03.\n",
      "Adjusting learning rate of group 0 to 4.4014e-03.\n",
      "Epoch : 935,\ttrain_loss : 0.0064248512499034405\tvalid_loss :0.005007244646549225\n",
      "Adjusting learning rate of group 0 to 4.3952e-03.\n",
      "Adjusting learning rate of group 0 to 4.3952e-03.\n",
      "Epoch : 936,\ttrain_loss : 0.006108057219535112\tvalid_loss :0.004806963726878166\n",
      "Adjusting learning rate of group 0 to 4.3889e-03.\n",
      "Adjusting learning rate of group 0 to 4.3889e-03.\n",
      "Epoch : 937,\ttrain_loss : 0.006380781531333923\tvalid_loss :0.005148967262357473\n",
      "Adjusting learning rate of group 0 to 4.3827e-03.\n",
      "Adjusting learning rate of group 0 to 4.3827e-03.\n",
      "Epoch : 938,\ttrain_loss : 0.006366181652992964\tvalid_loss :0.005448716692626476\n",
      "Adjusting learning rate of group 0 to 4.3764e-03.\n",
      "Adjusting learning rate of group 0 to 4.3764e-03.\n",
      "Epoch : 939,\ttrain_loss : 0.0063480488024652\tvalid_loss :0.004612204618752003\n",
      "Adjusting learning rate of group 0 to 4.3702e-03.\n",
      "Adjusting learning rate of group 0 to 4.3702e-03.\n",
      "Epoch : 940,\ttrain_loss : 0.0064394804649055\tvalid_loss :0.0042869932949543\n",
      "Adjusting learning rate of group 0 to 4.3639e-03.\n",
      "Adjusting learning rate of group 0 to 4.3639e-03.\n",
      "Epoch : 941,\ttrain_loss : 0.006545285228639841\tvalid_loss :0.005214998498558998\n",
      "Adjusting learning rate of group 0 to 4.3577e-03.\n",
      "Adjusting learning rate of group 0 to 4.3577e-03.\n",
      "Epoch : 942,\ttrain_loss : 0.006381531246006489\tvalid_loss :0.004719169810414314\n",
      "Adjusting learning rate of group 0 to 4.3514e-03.\n",
      "Adjusting learning rate of group 0 to 4.3514e-03.\n",
      "Epoch : 943,\ttrain_loss : 0.006371942348778248\tvalid_loss :0.004253651946783066\n",
      "Adjusting learning rate of group 0 to 4.3451e-03.\n",
      "Adjusting learning rate of group 0 to 4.3451e-03.\n",
      "Epoch : 944,\ttrain_loss : 0.006293526850640774\tvalid_loss :0.004201214760541916\n",
      "**********Valid loss decreased (0.004226 ==> 0.004201)**********\n",
      "Adjusting learning rate of group 0 to 4.3389e-03.\n",
      "Adjusting learning rate of group 0 to 4.3389e-03.\n",
      "Epoch : 945,\ttrain_loss : 0.006241002585738897\tvalid_loss :0.005460171028971672\n",
      "Adjusting learning rate of group 0 to 4.3326e-03.\n",
      "Adjusting learning rate of group 0 to 4.3326e-03.\n",
      "Epoch : 946,\ttrain_loss : 0.006281633861362934\tvalid_loss :0.004536119289696217\n",
      "Adjusting learning rate of group 0 to 4.3264e-03.\n",
      "Adjusting learning rate of group 0 to 4.3264e-03.\n",
      "Epoch : 947,\ttrain_loss : 0.006146729923784733\tvalid_loss :0.005138047970831394\n",
      "Adjusting learning rate of group 0 to 4.3201e-03.\n",
      "Adjusting learning rate of group 0 to 4.3201e-03.\n",
      "Epoch : 948,\ttrain_loss : 0.006241037975996733\tvalid_loss :0.004571095108985901\n",
      "Adjusting learning rate of group 0 to 4.3138e-03.\n",
      "Adjusting learning rate of group 0 to 4.3138e-03.\n",
      "Epoch : 949,\ttrain_loss : 0.006207624915987253\tvalid_loss :0.004798601847141981\n",
      "Adjusting learning rate of group 0 to 4.3076e-03.\n",
      "Adjusting learning rate of group 0 to 4.3076e-03.\n",
      "Epoch : 950,\ttrain_loss : 0.006650944706052542\tvalid_loss :0.0043256026692688465\n",
      "Adjusting learning rate of group 0 to 4.3013e-03.\n",
      "Adjusting learning rate of group 0 to 4.3013e-03.\n",
      "Epoch : 951,\ttrain_loss : 0.006612295750528574\tvalid_loss :0.004861772060394287\n",
      "Adjusting learning rate of group 0 to 4.2950e-03.\n",
      "Adjusting learning rate of group 0 to 4.2950e-03.\n",
      "Epoch : 952,\ttrain_loss : 0.0068291365168988705\tvalid_loss :0.00531758414581418\n",
      "Adjusting learning rate of group 0 to 4.2888e-03.\n",
      "Adjusting learning rate of group 0 to 4.2888e-03.\n",
      "Epoch : 953,\ttrain_loss : 0.0063276453875005245\tvalid_loss :0.004455364774912596\n",
      "Adjusting learning rate of group 0 to 4.2825e-03.\n",
      "Adjusting learning rate of group 0 to 4.2825e-03.\n",
      "Epoch : 954,\ttrain_loss : 0.006280710455030203\tvalid_loss :0.004582004155963659\n",
      "Adjusting learning rate of group 0 to 4.2762e-03.\n",
      "Adjusting learning rate of group 0 to 4.2762e-03.\n",
      "Epoch : 955,\ttrain_loss : 0.006298084743320942\tvalid_loss :0.00431098323315382\n",
      "Adjusting learning rate of group 0 to 4.2700e-03.\n",
      "Adjusting learning rate of group 0 to 4.2700e-03.\n",
      "Epoch : 956,\ttrain_loss : 0.006270005367696285\tvalid_loss :0.004518914967775345\n",
      "Adjusting learning rate of group 0 to 4.2637e-03.\n",
      "Adjusting learning rate of group 0 to 4.2637e-03.\n",
      "Epoch : 957,\ttrain_loss : 0.00622693682089448\tvalid_loss :0.0054244683124125\n",
      "Adjusting learning rate of group 0 to 4.2574e-03.\n",
      "Adjusting learning rate of group 0 to 4.2574e-03.\n",
      "Epoch : 958,\ttrain_loss : 0.006690907292068005\tvalid_loss :0.005094969179481268\n",
      "Adjusting learning rate of group 0 to 4.2512e-03.\n",
      "Adjusting learning rate of group 0 to 4.2512e-03.\n",
      "Epoch : 959,\ttrain_loss : 0.006510660517960787\tvalid_loss :0.004504643380641937\n",
      "Adjusting learning rate of group 0 to 4.2449e-03.\n",
      "Adjusting learning rate of group 0 to 4.2449e-03.\n",
      "Epoch : 960,\ttrain_loss : 0.006219364237040281\tvalid_loss :0.004644592292606831\n",
      "Adjusting learning rate of group 0 to 4.2386e-03.\n",
      "Adjusting learning rate of group 0 to 4.2386e-03.\n",
      "Epoch : 961,\ttrain_loss : 0.006337926723062992\tvalid_loss :0.004336915910243988\n",
      "Adjusting learning rate of group 0 to 4.2323e-03.\n",
      "Adjusting learning rate of group 0 to 4.2323e-03.\n",
      "Epoch : 962,\ttrain_loss : 0.006405963562428951\tvalid_loss :0.004295509308576584\n",
      "Adjusting learning rate of group 0 to 4.2261e-03.\n",
      "Adjusting learning rate of group 0 to 4.2261e-03.\n",
      "Epoch : 963,\ttrain_loss : 0.006544006522744894\tvalid_loss :0.005263932514935732\n",
      "Adjusting learning rate of group 0 to 4.2198e-03.\n",
      "Adjusting learning rate of group 0 to 4.2198e-03.\n",
      "Epoch : 964,\ttrain_loss : 0.006570424418896437\tvalid_loss :0.004550103563815355\n",
      "Adjusting learning rate of group 0 to 4.2135e-03.\n",
      "Adjusting learning rate of group 0 to 4.2135e-03.\n",
      "Epoch : 965,\ttrain_loss : 0.006312769837677479\tvalid_loss :0.004269080236554146\n",
      "Adjusting learning rate of group 0 to 4.2073e-03.\n",
      "Adjusting learning rate of group 0 to 4.2073e-03.\n",
      "Epoch : 966,\ttrain_loss : 0.006387167610228062\tvalid_loss :0.00515710050240159\n",
      "Adjusting learning rate of group 0 to 4.2010e-03.\n",
      "Adjusting learning rate of group 0 to 4.2010e-03.\n",
      "Epoch : 967,\ttrain_loss : 0.006498279515653849\tvalid_loss :0.0047710491344332695\n",
      "Adjusting learning rate of group 0 to 4.1947e-03.\n",
      "Adjusting learning rate of group 0 to 4.1947e-03.\n",
      "Epoch : 968,\ttrain_loss : 0.0062206513248384\tvalid_loss :0.00423766952008009\n",
      "Adjusting learning rate of group 0 to 4.1884e-03.\n",
      "Adjusting learning rate of group 0 to 4.1884e-03.\n",
      "Epoch : 969,\ttrain_loss : 0.006304196547716856\tvalid_loss :0.004330199211835861\n",
      "Adjusting learning rate of group 0 to 4.1821e-03.\n",
      "Adjusting learning rate of group 0 to 4.1821e-03.\n",
      "Epoch : 970,\ttrain_loss : 0.006600361317396164\tvalid_loss :0.004804547876119614\n",
      "Adjusting learning rate of group 0 to 4.1759e-03.\n",
      "Adjusting learning rate of group 0 to 4.1759e-03.\n",
      "Epoch : 971,\ttrain_loss : 0.006278017070144415\tvalid_loss :0.0043841018341481686\n",
      "Adjusting learning rate of group 0 to 4.1696e-03.\n",
      "Adjusting learning rate of group 0 to 4.1696e-03.\n",
      "Epoch : 972,\ttrain_loss : 0.006209197919815779\tvalid_loss :0.004434104077517986\n",
      "Adjusting learning rate of group 0 to 4.1633e-03.\n",
      "Adjusting learning rate of group 0 to 4.1633e-03.\n",
      "Epoch : 973,\ttrain_loss : 0.006173467729240656\tvalid_loss :0.004238251596689224\n",
      "Adjusting learning rate of group 0 to 4.1570e-03.\n",
      "Adjusting learning rate of group 0 to 4.1570e-03.\n",
      "Epoch : 974,\ttrain_loss : 0.006170176900923252\tvalid_loss :0.0042199972085654736\n",
      "Adjusting learning rate of group 0 to 4.1508e-03.\n",
      "Adjusting learning rate of group 0 to 4.1508e-03.\n",
      "Epoch : 975,\ttrain_loss : 0.006386269815266132\tvalid_loss :0.00443436112254858\n",
      "Adjusting learning rate of group 0 to 4.1445e-03.\n",
      "Adjusting learning rate of group 0 to 4.1445e-03.\n",
      "Epoch : 976,\ttrain_loss : 0.006599217187613249\tvalid_loss :0.00595173379406333\n",
      "Adjusting learning rate of group 0 to 4.1382e-03.\n",
      "Adjusting learning rate of group 0 to 4.1382e-03.\n",
      "Epoch : 977,\ttrain_loss : 0.006487867329269648\tvalid_loss :0.0050756013952195644\n",
      "Adjusting learning rate of group 0 to 4.1319e-03.\n",
      "Adjusting learning rate of group 0 to 4.1319e-03.\n",
      "Epoch : 978,\ttrain_loss : 0.00632114103063941\tvalid_loss :0.004906069487333298\n",
      "Adjusting learning rate of group 0 to 4.1256e-03.\n",
      "Adjusting learning rate of group 0 to 4.1256e-03.\n",
      "Epoch : 979,\ttrain_loss : 0.006197567097842693\tvalid_loss :0.004999264609068632\n",
      "Adjusting learning rate of group 0 to 4.1194e-03.\n",
      "Adjusting learning rate of group 0 to 4.1194e-03.\n",
      "Epoch : 980,\ttrain_loss : 0.00634553050622344\tvalid_loss :0.004529333673417568\n",
      "Adjusting learning rate of group 0 to 4.1131e-03.\n",
      "Adjusting learning rate of group 0 to 4.1131e-03.\n",
      "Epoch : 981,\ttrain_loss : 0.006179336458444595\tvalid_loss :0.004628618713468313\n",
      "Adjusting learning rate of group 0 to 4.1068e-03.\n",
      "Adjusting learning rate of group 0 to 4.1068e-03.\n",
      "Epoch : 982,\ttrain_loss : 0.00611790781840682\tvalid_loss :0.004371423739939928\n",
      "Adjusting learning rate of group 0 to 4.1005e-03.\n",
      "Adjusting learning rate of group 0 to 4.1005e-03.\n",
      "Epoch : 983,\ttrain_loss : 0.006181441247463226\tvalid_loss :0.004211446736007929\n",
      "Adjusting learning rate of group 0 to 4.0942e-03.\n",
      "Adjusting learning rate of group 0 to 4.0942e-03.\n",
      "Epoch : 984,\ttrain_loss : 0.0062580471858382225\tvalid_loss :0.00424128957092762\n",
      "Adjusting learning rate of group 0 to 4.0880e-03.\n",
      "Adjusting learning rate of group 0 to 4.0880e-03.\n",
      "Epoch : 985,\ttrain_loss : 0.006319641135632992\tvalid_loss :0.004330557771027088\n",
      "Adjusting learning rate of group 0 to 4.0817e-03.\n",
      "Adjusting learning rate of group 0 to 4.0817e-03.\n",
      "Epoch : 986,\ttrain_loss : 0.006488739978522062\tvalid_loss :0.004795266781002283\n",
      "Adjusting learning rate of group 0 to 4.0754e-03.\n",
      "Adjusting learning rate of group 0 to 4.0754e-03.\n",
      "Epoch : 987,\ttrain_loss : 0.006346056703478098\tvalid_loss :0.004363520070910454\n",
      "Adjusting learning rate of group 0 to 4.0691e-03.\n",
      "Adjusting learning rate of group 0 to 4.0691e-03.\n",
      "Epoch : 988,\ttrain_loss : 0.006358583923429251\tvalid_loss :0.0045701805502176285\n",
      "Adjusting learning rate of group 0 to 4.0628e-03.\n",
      "Adjusting learning rate of group 0 to 4.0628e-03.\n",
      "Epoch : 989,\ttrain_loss : 0.0064140004105865955\tvalid_loss :0.005348192527890205\n",
      "Adjusting learning rate of group 0 to 4.0565e-03.\n",
      "Adjusting learning rate of group 0 to 4.0565e-03.\n",
      "Epoch : 990,\ttrain_loss : 0.006517676170915365\tvalid_loss :0.0047262609004974365\n",
      "Adjusting learning rate of group 0 to 4.0503e-03.\n",
      "Adjusting learning rate of group 0 to 4.0503e-03.\n",
      "Epoch : 991,\ttrain_loss : 0.006249400787055492\tvalid_loss :0.005104465410113335\n",
      "Adjusting learning rate of group 0 to 4.0440e-03.\n",
      "Adjusting learning rate of group 0 to 4.0440e-03.\n",
      "Epoch : 992,\ttrain_loss : 0.006391317583620548\tvalid_loss :0.005251882132142782\n",
      "Adjusting learning rate of group 0 to 4.0377e-03.\n",
      "Adjusting learning rate of group 0 to 4.0377e-03.\n",
      "Epoch : 993,\ttrain_loss : 0.006350730545818806\tvalid_loss :0.005523344501852989\n",
      "Adjusting learning rate of group 0 to 4.0314e-03.\n",
      "Adjusting learning rate of group 0 to 4.0314e-03.\n",
      "Epoch : 994,\ttrain_loss : 0.006401869002729654\tvalid_loss :0.0055424985475838184\n",
      "Adjusting learning rate of group 0 to 4.0251e-03.\n",
      "Adjusting learning rate of group 0 to 4.0251e-03.\n",
      "Epoch : 995,\ttrain_loss : 0.006324282847344875\tvalid_loss :0.005126883275806904\n",
      "Adjusting learning rate of group 0 to 4.0188e-03.\n",
      "Adjusting learning rate of group 0 to 4.0188e-03.\n",
      "Epoch : 996,\ttrain_loss : 0.006349507719278336\tvalid_loss :0.005324978847056627\n",
      "Adjusting learning rate of group 0 to 4.0126e-03.\n",
      "Adjusting learning rate of group 0 to 4.0126e-03.\n",
      "Epoch : 997,\ttrain_loss : 0.0064053721725940704\tvalid_loss :0.005430449265986681\n",
      "Adjusting learning rate of group 0 to 4.0063e-03.\n",
      "Adjusting learning rate of group 0 to 4.0063e-03.\n",
      "Epoch : 998,\ttrain_loss : 0.006361042615026236\tvalid_loss :0.0049512810073792934\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "Epoch : 999,\ttrain_loss : 0.006348283030092716\tvalid_loss :0.0045881737023591995\n",
      "Adjusting learning rate of group 0 to 3.9937e-03.\n",
      "Adjusting learning rate of group 0 to 3.9937e-03.\n",
      "Epoch : 1000,\ttrain_loss : 0.006226903758943081\tvalid_loss :0.004588599316775799\n",
      "Adjusting learning rate of group 0 to 3.9874e-03.\n",
      "Adjusting learning rate of group 0 to 3.9874e-03.\n",
      "Epoch : 1001,\ttrain_loss : 0.00622264388948679\tvalid_loss :0.0043976944871246815\n",
      "Adjusting learning rate of group 0 to 3.9812e-03.\n",
      "Adjusting learning rate of group 0 to 3.9812e-03.\n",
      "Epoch : 1002,\ttrain_loss : 0.0062280455604195595\tvalid_loss :0.004196604713797569\n",
      "**********Valid loss decreased (0.004201 ==> 0.004197)**********\n",
      "Adjusting learning rate of group 0 to 3.9749e-03.\n",
      "Adjusting learning rate of group 0 to 3.9749e-03.\n",
      "Epoch : 1003,\ttrain_loss : 0.006455286871641874\tvalid_loss :0.005075728055089712\n",
      "Adjusting learning rate of group 0 to 3.9686e-03.\n",
      "Adjusting learning rate of group 0 to 3.9686e-03.\n",
      "Epoch : 1004,\ttrain_loss : 0.00630386034026742\tvalid_loss :0.004474824294447899\n",
      "Adjusting learning rate of group 0 to 3.9623e-03.\n",
      "Adjusting learning rate of group 0 to 3.9623e-03.\n",
      "Epoch : 1005,\ttrain_loss : 0.006311801262199879\tvalid_loss :0.004305693320930004\n",
      "Adjusting learning rate of group 0 to 3.9560e-03.\n",
      "Adjusting learning rate of group 0 to 3.9560e-03.\n",
      "Epoch : 1006,\ttrain_loss : 0.006427283398807049\tvalid_loss :0.004966326057910919\n",
      "Adjusting learning rate of group 0 to 3.9497e-03.\n",
      "Adjusting learning rate of group 0 to 3.9497e-03.\n",
      "Epoch : 1007,\ttrain_loss : 0.006282956805080175\tvalid_loss :0.004553857259452343\n",
      "Adjusting learning rate of group 0 to 3.9435e-03.\n",
      "Adjusting learning rate of group 0 to 3.9435e-03.\n",
      "Epoch : 1008,\ttrain_loss : 0.0061750272288918495\tvalid_loss :0.004507371690124273\n",
      "Adjusting learning rate of group 0 to 3.9372e-03.\n",
      "Adjusting learning rate of group 0 to 3.9372e-03.\n",
      "Epoch : 1009,\ttrain_loss : 0.006268903613090515\tvalid_loss :0.004398210905492306\n",
      "Adjusting learning rate of group 0 to 3.9309e-03.\n",
      "Adjusting learning rate of group 0 to 3.9309e-03.\n",
      "Epoch : 1010,\ttrain_loss : 0.006341463886201382\tvalid_loss :0.004600817337632179\n",
      "Adjusting learning rate of group 0 to 3.9246e-03.\n",
      "Adjusting learning rate of group 0 to 3.9246e-03.\n",
      "Epoch : 1011,\ttrain_loss : 0.006293554790318012\tvalid_loss :0.004281657747924328\n",
      "Adjusting learning rate of group 0 to 3.9183e-03.\n",
      "Adjusting learning rate of group 0 to 3.9183e-03.\n",
      "Epoch : 1012,\ttrain_loss : 0.006286397110670805\tvalid_loss :0.004354285076260567\n",
      "Adjusting learning rate of group 0 to 3.9120e-03.\n",
      "Adjusting learning rate of group 0 to 3.9120e-03.\n",
      "Epoch : 1013,\ttrain_loss : 0.006357341073453426\tvalid_loss :0.004798709414899349\n",
      "Adjusting learning rate of group 0 to 3.9058e-03.\n",
      "Adjusting learning rate of group 0 to 3.9058e-03.\n",
      "Epoch : 1014,\ttrain_loss : 0.006272425409406424\tvalid_loss :0.005001584533601999\n",
      "Adjusting learning rate of group 0 to 3.8995e-03.\n",
      "Adjusting learning rate of group 0 to 3.8995e-03.\n",
      "Epoch : 1015,\ttrain_loss : 0.006310391705483198\tvalid_loss :0.004933988209813833\n",
      "Adjusting learning rate of group 0 to 3.8932e-03.\n",
      "Adjusting learning rate of group 0 to 3.8932e-03.\n",
      "Epoch : 1016,\ttrain_loss : 0.006262630689889193\tvalid_loss :0.004746541380882263\n",
      "Adjusting learning rate of group 0 to 3.8869e-03.\n",
      "Adjusting learning rate of group 0 to 3.8869e-03.\n",
      "Epoch : 1017,\ttrain_loss : 0.006276193540543318\tvalid_loss :0.004717227071523666\n",
      "Adjusting learning rate of group 0 to 3.8806e-03.\n",
      "Adjusting learning rate of group 0 to 3.8806e-03.\n",
      "Epoch : 1018,\ttrain_loss : 0.006349620409309864\tvalid_loss :0.005178413353860378\n",
      "Adjusting learning rate of group 0 to 3.8744e-03.\n",
      "Adjusting learning rate of group 0 to 3.8744e-03.\n",
      "Epoch : 1019,\ttrain_loss : 0.006376036908477545\tvalid_loss :0.0053269946947693825\n",
      "Adjusting learning rate of group 0 to 3.8681e-03.\n",
      "Adjusting learning rate of group 0 to 3.8681e-03.\n",
      "Epoch : 1020,\ttrain_loss : 0.00634706811979413\tvalid_loss :0.005188985262066126\n",
      "Adjusting learning rate of group 0 to 3.8618e-03.\n",
      "Adjusting learning rate of group 0 to 3.8618e-03.\n",
      "Epoch : 1021,\ttrain_loss : 0.006284787319600582\tvalid_loss :0.005202200263738632\n",
      "Adjusting learning rate of group 0 to 3.8555e-03.\n",
      "Adjusting learning rate of group 0 to 3.8555e-03.\n",
      "Epoch : 1022,\ttrain_loss : 0.006405312102288008\tvalid_loss :0.005706582218408585\n",
      "Adjusting learning rate of group 0 to 3.8492e-03.\n",
      "Adjusting learning rate of group 0 to 3.8492e-03.\n",
      "Epoch : 1023,\ttrain_loss : 0.006291674915701151\tvalid_loss :0.004949914291501045\n",
      "Adjusting learning rate of group 0 to 3.8430e-03.\n",
      "Adjusting learning rate of group 0 to 3.8430e-03.\n",
      "Epoch : 1024,\ttrain_loss : 0.006309553515166044\tvalid_loss :0.0045898291282355785\n",
      "Adjusting learning rate of group 0 to 3.8367e-03.\n",
      "Adjusting learning rate of group 0 to 3.8367e-03.\n",
      "Epoch : 1025,\ttrain_loss : 0.00620950385928154\tvalid_loss :0.004466195125132799\n",
      "Adjusting learning rate of group 0 to 3.8304e-03.\n",
      "Adjusting learning rate of group 0 to 3.8304e-03.\n",
      "Epoch : 1026,\ttrain_loss : 0.0062447888776659966\tvalid_loss :0.004493357613682747\n",
      "Adjusting learning rate of group 0 to 3.8241e-03.\n",
      "Adjusting learning rate of group 0 to 3.8241e-03.\n",
      "Epoch : 1027,\ttrain_loss : 0.006420283578336239\tvalid_loss :0.005670096725225449\n",
      "Adjusting learning rate of group 0 to 3.8179e-03.\n",
      "Adjusting learning rate of group 0 to 3.8179e-03.\n",
      "Epoch : 1028,\ttrain_loss : 0.006476443260908127\tvalid_loss :0.00560941593721509\n",
      "Adjusting learning rate of group 0 to 3.8116e-03.\n",
      "Adjusting learning rate of group 0 to 3.8116e-03.\n",
      "Epoch : 1029,\ttrain_loss : 0.006349712144583464\tvalid_loss :0.004669325426220894\n",
      "Adjusting learning rate of group 0 to 3.8053e-03.\n",
      "Adjusting learning rate of group 0 to 3.8053e-03.\n",
      "Epoch : 1030,\ttrain_loss : 0.006205898243933916\tvalid_loss :0.004876985214650631\n",
      "Adjusting learning rate of group 0 to 3.7990e-03.\n",
      "Adjusting learning rate of group 0 to 3.7990e-03.\n",
      "Epoch : 1031,\ttrain_loss : 0.006267826538532972\tvalid_loss :0.005009837448596954\n",
      "Adjusting learning rate of group 0 to 3.7927e-03.\n",
      "Adjusting learning rate of group 0 to 3.7927e-03.\n",
      "Epoch : 1032,\ttrain_loss : 0.0062548150308430195\tvalid_loss :0.0045478567481040955\n",
      "Adjusting learning rate of group 0 to 3.7865e-03.\n",
      "Adjusting learning rate of group 0 to 3.7865e-03.\n",
      "Epoch : 1033,\ttrain_loss : 0.006320972461253405\tvalid_loss :0.004243910312652588\n",
      "Adjusting learning rate of group 0 to 3.7802e-03.\n",
      "Adjusting learning rate of group 0 to 3.7802e-03.\n",
      "Epoch : 1034,\ttrain_loss : 0.0064446586184203625\tvalid_loss :0.005373772699385881\n",
      "Adjusting learning rate of group 0 to 3.7739e-03.\n",
      "Adjusting learning rate of group 0 to 3.7739e-03.\n",
      "Epoch : 1035,\ttrain_loss : 0.006444965023547411\tvalid_loss :0.005143014248460531\n",
      "Adjusting learning rate of group 0 to 3.7677e-03.\n",
      "Adjusting learning rate of group 0 to 3.7677e-03.\n",
      "Epoch : 1036,\ttrain_loss : 0.0063162995502352715\tvalid_loss :0.004931975621730089\n",
      "Adjusting learning rate of group 0 to 3.7614e-03.\n",
      "Adjusting learning rate of group 0 to 3.7614e-03.\n",
      "Epoch : 1037,\ttrain_loss : 0.006247933022677898\tvalid_loss :0.004906159359961748\n",
      "Adjusting learning rate of group 0 to 3.7551e-03.\n",
      "Adjusting learning rate of group 0 to 3.7551e-03.\n",
      "Epoch : 1038,\ttrain_loss : 0.0062750219367444515\tvalid_loss :0.004855240695178509\n",
      "Adjusting learning rate of group 0 to 3.7488e-03.\n",
      "Adjusting learning rate of group 0 to 3.7488e-03.\n",
      "Epoch : 1039,\ttrain_loss : 0.006242767442017794\tvalid_loss :0.004664437845349312\n",
      "Adjusting learning rate of group 0 to 3.7426e-03.\n",
      "Adjusting learning rate of group 0 to 3.7426e-03.\n",
      "Epoch : 1040,\ttrain_loss : 0.006150782108306885\tvalid_loss :0.004260033834725618\n",
      "Adjusting learning rate of group 0 to 3.7363e-03.\n",
      "Adjusting learning rate of group 0 to 3.7363e-03.\n",
      "Epoch : 1041,\ttrain_loss : 0.006348143331706524\tvalid_loss :0.004570897668600082\n",
      "Adjusting learning rate of group 0 to 3.7300e-03.\n",
      "Adjusting learning rate of group 0 to 3.7300e-03.\n",
      "Epoch : 1042,\ttrain_loss : 0.006286950316280127\tvalid_loss :0.004665153566747904\n",
      "Adjusting learning rate of group 0 to 3.7238e-03.\n",
      "Adjusting learning rate of group 0 to 3.7238e-03.\n",
      "Epoch : 1043,\ttrain_loss : 0.006236271467059851\tvalid_loss :0.004298441112041473\n",
      "Adjusting learning rate of group 0 to 3.7175e-03.\n",
      "Adjusting learning rate of group 0 to 3.7175e-03.\n",
      "Epoch : 1044,\ttrain_loss : 0.0062124175019562244\tvalid_loss :0.004305293783545494\n",
      "Adjusting learning rate of group 0 to 3.7112e-03.\n",
      "Adjusting learning rate of group 0 to 3.7112e-03.\n",
      "Epoch : 1045,\ttrain_loss : 0.006442599929869175\tvalid_loss :0.005943610332906246\n",
      "Adjusting learning rate of group 0 to 3.7050e-03.\n",
      "Adjusting learning rate of group 0 to 3.7050e-03.\n",
      "Epoch : 1046,\ttrain_loss : 0.006441427860409021\tvalid_loss :0.004918995313346386\n",
      "Adjusting learning rate of group 0 to 3.6987e-03.\n",
      "Adjusting learning rate of group 0 to 3.6987e-03.\n",
      "Epoch : 1047,\ttrain_loss : 0.006166534032672644\tvalid_loss :0.004392974078655243\n",
      "Adjusting learning rate of group 0 to 3.6924e-03.\n",
      "Adjusting learning rate of group 0 to 3.6924e-03.\n",
      "Epoch : 1048,\ttrain_loss : 0.006420443300157785\tvalid_loss :0.004490266554057598\n",
      "Adjusting learning rate of group 0 to 3.6862e-03.\n",
      "Adjusting learning rate of group 0 to 3.6862e-03.\n",
      "Epoch : 1049,\ttrain_loss : 0.00623385701328516\tvalid_loss :0.005145232658833265\n",
      "Adjusting learning rate of group 0 to 3.6799e-03.\n",
      "Adjusting learning rate of group 0 to 3.6799e-03.\n",
      "Epoch : 1050,\ttrain_loss : 0.006458251271396875\tvalid_loss :0.0048053450882434845\n",
      "Adjusting learning rate of group 0 to 3.6736e-03.\n",
      "Adjusting learning rate of group 0 to 3.6736e-03.\n",
      "Epoch : 1051,\ttrain_loss : 0.006232555955648422\tvalid_loss :0.0046468935906887054\n",
      "Adjusting learning rate of group 0 to 3.6674e-03.\n",
      "Adjusting learning rate of group 0 to 3.6674e-03.\n",
      "Epoch : 1052,\ttrain_loss : 0.006230260711163282\tvalid_loss :0.004730404820293188\n",
      "Adjusting learning rate of group 0 to 3.6611e-03.\n",
      "Adjusting learning rate of group 0 to 3.6611e-03.\n",
      "Epoch : 1053,\ttrain_loss : 0.006284717936068773\tvalid_loss :0.005148014053702354\n",
      "Adjusting learning rate of group 0 to 3.6549e-03.\n",
      "Adjusting learning rate of group 0 to 3.6549e-03.\n",
      "Epoch : 1054,\ttrain_loss : 0.006360945291817188\tvalid_loss :0.005432127043604851\n",
      "Adjusting learning rate of group 0 to 3.6486e-03.\n",
      "Adjusting learning rate of group 0 to 3.6486e-03.\n",
      "Epoch : 1055,\ttrain_loss : 0.006288853008300066\tvalid_loss :0.005156626924872398\n",
      "Adjusting learning rate of group 0 to 3.6423e-03.\n",
      "Adjusting learning rate of group 0 to 3.6423e-03.\n",
      "Epoch : 1056,\ttrain_loss : 0.006310678087174892\tvalid_loss :0.005250952206552029\n",
      "Adjusting learning rate of group 0 to 3.6361e-03.\n",
      "Adjusting learning rate of group 0 to 3.6361e-03.\n",
      "Epoch : 1057,\ttrain_loss : 0.006293354090303183\tvalid_loss :0.0049244072288274765\n",
      "Adjusting learning rate of group 0 to 3.6298e-03.\n",
      "Adjusting learning rate of group 0 to 3.6298e-03.\n",
      "Epoch : 1058,\ttrain_loss : 0.006289455574005842\tvalid_loss :0.005252565257251263\n",
      "Adjusting learning rate of group 0 to 3.6236e-03.\n",
      "Adjusting learning rate of group 0 to 3.6236e-03.\n",
      "Epoch : 1059,\ttrain_loss : 0.006254679523408413\tvalid_loss :0.004999382421374321\n",
      "Adjusting learning rate of group 0 to 3.6173e-03.\n",
      "Adjusting learning rate of group 0 to 3.6173e-03.\n",
      "Epoch : 1060,\ttrain_loss : 0.0063361553475260735\tvalid_loss :0.005814394447952509\n",
      "Adjusting learning rate of group 0 to 3.6111e-03.\n",
      "Adjusting learning rate of group 0 to 3.6111e-03.\n",
      "Epoch : 1061,\ttrain_loss : 0.006343119777739048\tvalid_loss :0.004708915948867798\n",
      "Adjusting learning rate of group 0 to 3.6048e-03.\n",
      "Adjusting learning rate of group 0 to 3.6048e-03.\n",
      "Epoch : 1062,\ttrain_loss : 0.006311583332717419\tvalid_loss :0.004914774559438229\n",
      "Adjusting learning rate of group 0 to 3.5986e-03.\n",
      "Adjusting learning rate of group 0 to 3.5986e-03.\n",
      "Epoch : 1063,\ttrain_loss : 0.006339418701827526\tvalid_loss :0.005166272167116404\n",
      "Adjusting learning rate of group 0 to 3.5923e-03.\n",
      "Adjusting learning rate of group 0 to 3.5923e-03.\n",
      "Epoch : 1064,\ttrain_loss : 0.006240065209567547\tvalid_loss :0.0048638032749295235\n",
      "Adjusting learning rate of group 0 to 3.5861e-03.\n",
      "Adjusting learning rate of group 0 to 3.5861e-03.\n",
      "Epoch : 1065,\ttrain_loss : 0.006203104741871357\tvalid_loss :0.00492969062179327\n",
      "Adjusting learning rate of group 0 to 3.5798e-03.\n",
      "Adjusting learning rate of group 0 to 3.5798e-03.\n",
      "Epoch : 1066,\ttrain_loss : 0.006283413618803024\tvalid_loss :0.004878593143075705\n",
      "Adjusting learning rate of group 0 to 3.5736e-03.\n",
      "Adjusting learning rate of group 0 to 3.5736e-03.\n",
      "Epoch : 1067,\ttrain_loss : 0.006296765059232712\tvalid_loss :0.005146089941263199\n",
      "Adjusting learning rate of group 0 to 3.5673e-03.\n",
      "Adjusting learning rate of group 0 to 3.5673e-03.\n",
      "Epoch : 1068,\ttrain_loss : 0.006434720009565353\tvalid_loss :0.005274525843560696\n",
      "Adjusting learning rate of group 0 to 3.5611e-03.\n",
      "Adjusting learning rate of group 0 to 3.5611e-03.\n",
      "Epoch : 1069,\ttrain_loss : 0.006299941800534725\tvalid_loss :0.005096616223454475\n",
      "Adjusting learning rate of group 0 to 3.5548e-03.\n",
      "Adjusting learning rate of group 0 to 3.5548e-03.\n",
      "Epoch : 1070,\ttrain_loss : 0.006292317528277636\tvalid_loss :0.004528074059635401\n",
      "Adjusting learning rate of group 0 to 3.5486e-03.\n",
      "Adjusting learning rate of group 0 to 3.5486e-03.\n",
      "Epoch : 1071,\ttrain_loss : 0.006129672285169363\tvalid_loss :0.004222620278596878\n",
      "Adjusting learning rate of group 0 to 3.5423e-03.\n",
      "Adjusting learning rate of group 0 to 3.5423e-03.\n",
      "Epoch : 1072,\ttrain_loss : 0.006382620427757502\tvalid_loss :0.004812377039343119\n",
      "Adjusting learning rate of group 0 to 3.5361e-03.\n",
      "Adjusting learning rate of group 0 to 3.5361e-03.\n",
      "Epoch : 1073,\ttrain_loss : 0.006290449760854244\tvalid_loss :0.0055967941880226135\n",
      "Adjusting learning rate of group 0 to 3.5299e-03.\n",
      "Adjusting learning rate of group 0 to 3.5299e-03.\n",
      "Epoch : 1074,\ttrain_loss : 0.006218540482223034\tvalid_loss :0.004949652589857578\n",
      "Adjusting learning rate of group 0 to 3.5236e-03.\n",
      "Adjusting learning rate of group 0 to 3.5236e-03.\n",
      "Epoch : 1075,\ttrain_loss : 0.006212622858583927\tvalid_loss :0.0047041685320436954\n",
      "Adjusting learning rate of group 0 to 3.5174e-03.\n",
      "Adjusting learning rate of group 0 to 3.5174e-03.\n",
      "Epoch : 1076,\ttrain_loss : 0.006234022323042154\tvalid_loss :0.004203252959996462\n",
      "Adjusting learning rate of group 0 to 3.5111e-03.\n",
      "Adjusting learning rate of group 0 to 3.5111e-03.\n",
      "Epoch : 1077,\ttrain_loss : 0.006680131424218416\tvalid_loss :0.004511890932917595\n",
      "Adjusting learning rate of group 0 to 3.5049e-03.\n",
      "Adjusting learning rate of group 0 to 3.5049e-03.\n",
      "Epoch : 1078,\ttrain_loss : 0.006321561522781849\tvalid_loss :0.00488576665520668\n",
      "Adjusting learning rate of group 0 to 3.4987e-03.\n",
      "Adjusting learning rate of group 0 to 3.4987e-03.\n",
      "Epoch : 1079,\ttrain_loss : 0.006264856550842524\tvalid_loss :0.005162473302334547\n",
      "Adjusting learning rate of group 0 to 3.4924e-03.\n",
      "Adjusting learning rate of group 0 to 3.4924e-03.\n",
      "Epoch : 1080,\ttrain_loss : 0.006178248208016157\tvalid_loss :0.0045966883189976215\n",
      "Adjusting learning rate of group 0 to 3.4862e-03.\n",
      "Adjusting learning rate of group 0 to 3.4862e-03.\n",
      "Epoch : 1081,\ttrain_loss : 0.006203788332641125\tvalid_loss :0.004409261047840118\n",
      "Adjusting learning rate of group 0 to 3.4800e-03.\n",
      "Adjusting learning rate of group 0 to 3.4800e-03.\n",
      "Epoch : 1082,\ttrain_loss : 0.006532988511025906\tvalid_loss :0.005366756580770016\n",
      "Adjusting learning rate of group 0 to 3.4737e-03.\n",
      "Adjusting learning rate of group 0 to 3.4737e-03.\n",
      "Epoch : 1083,\ttrain_loss : 0.006444201804697514\tvalid_loss :0.004633963108062744\n",
      "Adjusting learning rate of group 0 to 3.4675e-03.\n",
      "Adjusting learning rate of group 0 to 3.4675e-03.\n",
      "Epoch : 1084,\ttrain_loss : 0.006342701148241758\tvalid_loss :0.004535674583166838\n",
      "Adjusting learning rate of group 0 to 3.4613e-03.\n",
      "Adjusting learning rate of group 0 to 3.4613e-03.\n",
      "Epoch : 1085,\ttrain_loss : 0.0062208217568695545\tvalid_loss :0.004996384959667921\n",
      "Adjusting learning rate of group 0 to 3.4551e-03.\n",
      "Adjusting learning rate of group 0 to 3.4551e-03.\n",
      "Epoch : 1086,\ttrain_loss : 0.006380106322467327\tvalid_loss :0.004951858893036842\n",
      "Adjusting learning rate of group 0 to 3.4488e-03.\n",
      "Adjusting learning rate of group 0 to 3.4488e-03.\n",
      "Epoch : 1087,\ttrain_loss : 0.006213776301592588\tvalid_loss :0.004764944314956665\n",
      "Adjusting learning rate of group 0 to 3.4426e-03.\n",
      "Adjusting learning rate of group 0 to 3.4426e-03.\n",
      "Epoch : 1088,\ttrain_loss : 0.00622175307944417\tvalid_loss :0.004929620306938887\n",
      "Adjusting learning rate of group 0 to 3.4364e-03.\n",
      "Adjusting learning rate of group 0 to 3.4364e-03.\n",
      "Epoch : 1089,\ttrain_loss : 0.006395281292498112\tvalid_loss :0.004830325488001108\n",
      "Adjusting learning rate of group 0 to 3.4302e-03.\n",
      "Adjusting learning rate of group 0 to 3.4302e-03.\n",
      "Epoch : 1090,\ttrain_loss : 0.00620803190395236\tvalid_loss :0.004586016293615103\n",
      "Adjusting learning rate of group 0 to 3.4240e-03.\n",
      "Adjusting learning rate of group 0 to 3.4240e-03.\n",
      "Epoch : 1091,\ttrain_loss : 0.006372310221195221\tvalid_loss :0.006114976014941931\n",
      "Adjusting learning rate of group 0 to 3.4177e-03.\n",
      "Adjusting learning rate of group 0 to 3.4177e-03.\n",
      "Epoch : 1092,\ttrain_loss : 0.006439687218517065\tvalid_loss :0.004901548847556114\n",
      "Adjusting learning rate of group 0 to 3.4115e-03.\n",
      "Adjusting learning rate of group 0 to 3.4115e-03.\n",
      "Epoch : 1093,\ttrain_loss : 0.006273719482123852\tvalid_loss :0.005748191848397255\n",
      "Adjusting learning rate of group 0 to 3.4053e-03.\n",
      "Adjusting learning rate of group 0 to 3.4053e-03.\n",
      "Epoch : 1094,\ttrain_loss : 0.006399151869118214\tvalid_loss :0.004706718027591705\n",
      "Adjusting learning rate of group 0 to 3.3991e-03.\n",
      "Adjusting learning rate of group 0 to 3.3991e-03.\n",
      "Epoch : 1095,\ttrain_loss : 0.006251263432204723\tvalid_loss :0.004948267247527838\n",
      "Adjusting learning rate of group 0 to 3.3929e-03.\n",
      "Adjusting learning rate of group 0 to 3.3929e-03.\n",
      "Epoch : 1096,\ttrain_loss : 0.0062202890403568745\tvalid_loss :0.0046865250915288925\n",
      "Adjusting learning rate of group 0 to 3.3867e-03.\n",
      "Adjusting learning rate of group 0 to 3.3867e-03.\n",
      "Epoch : 1097,\ttrain_loss : 0.006179166026413441\tvalid_loss :0.0044929455034434795\n",
      "Adjusting learning rate of group 0 to 3.3805e-03.\n",
      "Adjusting learning rate of group 0 to 3.3805e-03.\n",
      "Epoch : 1098,\ttrain_loss : 0.006171137560158968\tvalid_loss :0.004687033593654633\n",
      "Adjusting learning rate of group 0 to 3.3743e-03.\n",
      "Adjusting learning rate of group 0 to 3.3743e-03.\n",
      "Epoch : 1099,\ttrain_loss : 0.006143360398709774\tvalid_loss :0.004546798765659332\n",
      "Adjusting learning rate of group 0 to 3.3681e-03.\n",
      "Adjusting learning rate of group 0 to 3.3681e-03.\n",
      "Epoch : 1100,\ttrain_loss : 0.006145153194665909\tvalid_loss :0.0049613360315561295\n",
      "Adjusting learning rate of group 0 to 3.3619e-03.\n",
      "Adjusting learning rate of group 0 to 3.3619e-03.\n",
      "Epoch : 1101,\ttrain_loss : 0.006241980008780956\tvalid_loss :0.004709803499281406\n",
      "Adjusting learning rate of group 0 to 3.3557e-03.\n",
      "Adjusting learning rate of group 0 to 3.3557e-03.\n",
      "Epoch : 1102,\ttrain_loss : 0.006186347454786301\tvalid_loss :0.005226819310337305\n",
      "Adjusting learning rate of group 0 to 3.3495e-03.\n",
      "Adjusting learning rate of group 0 to 3.3495e-03.\n",
      "Epoch : 1103,\ttrain_loss : 0.006295398343354464\tvalid_loss :0.004982413724064827\n",
      "Adjusting learning rate of group 0 to 3.3433e-03.\n",
      "Adjusting learning rate of group 0 to 3.3433e-03.\n",
      "Epoch : 1104,\ttrain_loss : 0.006249493453651667\tvalid_loss :0.0045997342094779015\n",
      "Adjusting learning rate of group 0 to 3.3371e-03.\n",
      "Adjusting learning rate of group 0 to 3.3371e-03.\n",
      "Epoch : 1105,\ttrain_loss : 0.0062154605984687805\tvalid_loss :0.0050894757732748985\n",
      "Adjusting learning rate of group 0 to 3.3309e-03.\n",
      "Adjusting learning rate of group 0 to 3.3309e-03.\n",
      "Epoch : 1106,\ttrain_loss : 0.0062755015678703785\tvalid_loss :0.004904984496533871\n",
      "Adjusting learning rate of group 0 to 3.3247e-03.\n",
      "Adjusting learning rate of group 0 to 3.3247e-03.\n",
      "Epoch : 1107,\ttrain_loss : 0.006210898514837027\tvalid_loss :0.004684095270931721\n",
      "Adjusting learning rate of group 0 to 3.3185e-03.\n",
      "Adjusting learning rate of group 0 to 3.3185e-03.\n",
      "Epoch : 1108,\ttrain_loss : 0.006273103412240744\tvalid_loss :0.0044609964825212955\n",
      "Adjusting learning rate of group 0 to 3.3123e-03.\n",
      "Adjusting learning rate of group 0 to 3.3123e-03.\n",
      "Epoch : 1109,\ttrain_loss : 0.006235099397599697\tvalid_loss :0.00463395519182086\n",
      "Adjusting learning rate of group 0 to 3.3061e-03.\n",
      "Adjusting learning rate of group 0 to 3.3061e-03.\n",
      "Epoch : 1110,\ttrain_loss : 0.006152235437184572\tvalid_loss :0.004824130795896053\n",
      "Adjusting learning rate of group 0 to 3.2999e-03.\n",
      "Adjusting learning rate of group 0 to 3.2999e-03.\n",
      "Epoch : 1111,\ttrain_loss : 0.0062683806754648685\tvalid_loss :0.005346343386918306\n",
      "Adjusting learning rate of group 0 to 3.2937e-03.\n",
      "Adjusting learning rate of group 0 to 3.2937e-03.\n",
      "Epoch : 1112,\ttrain_loss : 0.006265839096158743\tvalid_loss :0.0045846193097531796\n",
      "Adjusting learning rate of group 0 to 3.2875e-03.\n",
      "Adjusting learning rate of group 0 to 3.2875e-03.\n",
      "Epoch : 1113,\ttrain_loss : 0.0062283361330628395\tvalid_loss :0.004368931986391544\n",
      "Adjusting learning rate of group 0 to 3.2814e-03.\n",
      "Adjusting learning rate of group 0 to 3.2814e-03.\n",
      "Epoch : 1114,\ttrain_loss : 0.006377061363309622\tvalid_loss :0.005165888927876949\n",
      "Adjusting learning rate of group 0 to 3.2752e-03.\n",
      "Adjusting learning rate of group 0 to 3.2752e-03.\n",
      "Epoch : 1115,\ttrain_loss : 0.006266939919441938\tvalid_loss :0.004653903655707836\n",
      "Adjusting learning rate of group 0 to 3.2690e-03.\n",
      "Adjusting learning rate of group 0 to 3.2690e-03.\n",
      "Epoch : 1116,\ttrain_loss : 0.006215881556272507\tvalid_loss :0.005021544639021158\n",
      "Adjusting learning rate of group 0 to 3.2628e-03.\n",
      "Adjusting learning rate of group 0 to 3.2628e-03.\n",
      "Epoch : 1117,\ttrain_loss : 0.006255469750612974\tvalid_loss :0.0047951629385352135\n",
      "Adjusting learning rate of group 0 to 3.2566e-03.\n",
      "Adjusting learning rate of group 0 to 3.2566e-03.\n",
      "Epoch : 1118,\ttrain_loss : 0.006201078649610281\tvalid_loss :0.004669761750847101\n",
      "Adjusting learning rate of group 0 to 3.2505e-03.\n",
      "Adjusting learning rate of group 0 to 3.2505e-03.\n",
      "Epoch : 1119,\ttrain_loss : 0.006126824300736189\tvalid_loss :0.004703565500676632\n",
      "Adjusting learning rate of group 0 to 3.2443e-03.\n",
      "Adjusting learning rate of group 0 to 3.2443e-03.\n",
      "Epoch : 1120,\ttrain_loss : 0.006380557082593441\tvalid_loss :0.005446732044219971\n",
      "Adjusting learning rate of group 0 to 3.2381e-03.\n",
      "Adjusting learning rate of group 0 to 3.2381e-03.\n",
      "Epoch : 1121,\ttrain_loss : 0.006313295569270849\tvalid_loss :0.004440520890057087\n",
      "Adjusting learning rate of group 0 to 3.2320e-03.\n",
      "Adjusting learning rate of group 0 to 3.2320e-03.\n",
      "Epoch : 1122,\ttrain_loss : 0.006357365287840366\tvalid_loss :0.004998306278139353\n",
      "Adjusting learning rate of group 0 to 3.2258e-03.\n",
      "Adjusting learning rate of group 0 to 3.2258e-03.\n",
      "Epoch : 1123,\ttrain_loss : 0.006273056846112013\tvalid_loss :0.004815730266273022\n",
      "Adjusting learning rate of group 0 to 3.2196e-03.\n",
      "Adjusting learning rate of group 0 to 3.2196e-03.\n",
      "Epoch : 1124,\ttrain_loss : 0.00632057199254632\tvalid_loss :0.005289243534207344\n",
      "Adjusting learning rate of group 0 to 3.2135e-03.\n",
      "Adjusting learning rate of group 0 to 3.2135e-03.\n",
      "Epoch : 1125,\ttrain_loss : 0.0062826103530824184\tvalid_loss :0.004380897618830204\n",
      "Adjusting learning rate of group 0 to 3.2073e-03.\n",
      "Adjusting learning rate of group 0 to 3.2073e-03.\n",
      "Epoch : 1126,\ttrain_loss : 0.006340004038065672\tvalid_loss :0.0048965876922011375\n",
      "Adjusting learning rate of group 0 to 3.2012e-03.\n",
      "Adjusting learning rate of group 0 to 3.2012e-03.\n",
      "Epoch : 1127,\ttrain_loss : 0.006194281857460737\tvalid_loss :0.004588170908391476\n",
      "Adjusting learning rate of group 0 to 3.1950e-03.\n",
      "Adjusting learning rate of group 0 to 3.1950e-03.\n",
      "Epoch : 1128,\ttrain_loss : 0.0062577794305980206\tvalid_loss :0.0051894416101276875\n",
      "Adjusting learning rate of group 0 to 3.1889e-03.\n",
      "Adjusting learning rate of group 0 to 3.1889e-03.\n",
      "Epoch : 1129,\ttrain_loss : 0.006227691657841206\tvalid_loss :0.004312591161578894\n",
      "Adjusting learning rate of group 0 to 3.1827e-03.\n",
      "Adjusting learning rate of group 0 to 3.1827e-03.\n",
      "Epoch : 1130,\ttrain_loss : 0.006516092922538519\tvalid_loss :0.004251115024089813\n",
      "Adjusting learning rate of group 0 to 3.1765e-03.\n",
      "Adjusting learning rate of group 0 to 3.1765e-03.\n",
      "Epoch : 1131,\ttrain_loss : 0.006128669250756502\tvalid_loss :0.004898676183074713\n",
      "Adjusting learning rate of group 0 to 3.1704e-03.\n",
      "Adjusting learning rate of group 0 to 3.1704e-03.\n",
      "Epoch : 1132,\ttrain_loss : 0.006246012169867754\tvalid_loss :0.005202042404562235\n",
      "Adjusting learning rate of group 0 to 3.1643e-03.\n",
      "Adjusting learning rate of group 0 to 3.1643e-03.\n",
      "Epoch : 1133,\ttrain_loss : 0.006250548176467419\tvalid_loss :0.00576743483543396\n",
      "Adjusting learning rate of group 0 to 3.1581e-03.\n",
      "Adjusting learning rate of group 0 to 3.1581e-03.\n",
      "Epoch : 1134,\ttrain_loss : 0.006237057503312826\tvalid_loss :0.004710710607469082\n",
      "Adjusting learning rate of group 0 to 3.1520e-03.\n",
      "Adjusting learning rate of group 0 to 3.1520e-03.\n",
      "Epoch : 1135,\ttrain_loss : 0.00628649303689599\tvalid_loss :0.004752105101943016\n",
      "Adjusting learning rate of group 0 to 3.1458e-03.\n",
      "Adjusting learning rate of group 0 to 3.1458e-03.\n",
      "Epoch : 1136,\ttrain_loss : 0.0061970059759914875\tvalid_loss :0.005014656111598015\n",
      "Adjusting learning rate of group 0 to 3.1397e-03.\n",
      "Adjusting learning rate of group 0 to 3.1397e-03.\n",
      "Epoch : 1137,\ttrain_loss : 0.0061713457107543945\tvalid_loss :0.0056618391536176205\n",
      "Adjusting learning rate of group 0 to 3.1336e-03.\n",
      "Adjusting learning rate of group 0 to 3.1336e-03.\n",
      "Epoch : 1138,\ttrain_loss : 0.006295084487646818\tvalid_loss :0.004774857312440872\n",
      "Adjusting learning rate of group 0 to 3.1274e-03.\n",
      "Adjusting learning rate of group 0 to 3.1274e-03.\n",
      "Epoch : 1139,\ttrain_loss : 0.0063294158317148685\tvalid_loss :0.0050312792882323265\n",
      "Adjusting learning rate of group 0 to 3.1213e-03.\n",
      "Adjusting learning rate of group 0 to 3.1213e-03.\n",
      "Epoch : 1140,\ttrain_loss : 0.006207183003425598\tvalid_loss :0.004691699519753456\n",
      "Adjusting learning rate of group 0 to 3.1152e-03.\n",
      "Adjusting learning rate of group 0 to 3.1152e-03.\n",
      "Epoch : 1141,\ttrain_loss : 0.0063275001011788845\tvalid_loss :0.0047346921637654305\n",
      "Adjusting learning rate of group 0 to 3.1090e-03.\n",
      "Adjusting learning rate of group 0 to 3.1090e-03.\n",
      "Epoch : 1142,\ttrain_loss : 0.00612446665763855\tvalid_loss :0.004409833811223507\n",
      "Adjusting learning rate of group 0 to 3.1029e-03.\n",
      "Adjusting learning rate of group 0 to 3.1029e-03.\n",
      "Epoch : 1143,\ttrain_loss : 0.006258311681449413\tvalid_loss :0.004888752941042185\n",
      "Adjusting learning rate of group 0 to 3.0968e-03.\n",
      "Adjusting learning rate of group 0 to 3.0968e-03.\n",
      "Epoch : 1144,\ttrain_loss : 0.006163180805742741\tvalid_loss :0.004555274732410908\n",
      "Adjusting learning rate of group 0 to 3.0907e-03.\n",
      "Adjusting learning rate of group 0 to 3.0907e-03.\n",
      "Epoch : 1145,\ttrain_loss : 0.006232217885553837\tvalid_loss :0.004764742683619261\n",
      "Adjusting learning rate of group 0 to 3.0846e-03.\n",
      "Adjusting learning rate of group 0 to 3.0846e-03.\n",
      "Epoch : 1146,\ttrain_loss : 0.006178074050694704\tvalid_loss :0.0051398263312876225\n",
      "Adjusting learning rate of group 0 to 3.0784e-03.\n",
      "Adjusting learning rate of group 0 to 3.0784e-03.\n",
      "Epoch : 1147,\ttrain_loss : 0.006227623671293259\tvalid_loss :0.004917144775390625\n",
      "Adjusting learning rate of group 0 to 3.0723e-03.\n",
      "Adjusting learning rate of group 0 to 3.0723e-03.\n",
      "Epoch : 1148,\ttrain_loss : 0.006190571002662182\tvalid_loss :0.005330995190888643\n",
      "Adjusting learning rate of group 0 to 3.0662e-03.\n",
      "Adjusting learning rate of group 0 to 3.0662e-03.\n",
      "Epoch : 1149,\ttrain_loss : 0.0061724488623440266\tvalid_loss :0.004577771760523319\n",
      "Adjusting learning rate of group 0 to 3.0601e-03.\n",
      "Adjusting learning rate of group 0 to 3.0601e-03.\n",
      "Epoch : 1150,\ttrain_loss : 0.006353309843689203\tvalid_loss :0.004524389281868935\n",
      "Adjusting learning rate of group 0 to 3.0540e-03.\n",
      "Adjusting learning rate of group 0 to 3.0540e-03.\n",
      "Epoch : 1151,\ttrain_loss : 0.0061592767015099525\tvalid_loss :0.004445187747478485\n",
      "Adjusting learning rate of group 0 to 3.0479e-03.\n",
      "Adjusting learning rate of group 0 to 3.0479e-03.\n",
      "Epoch : 1152,\ttrain_loss : 0.006195164751261473\tvalid_loss :0.005055596120655537\n",
      "Adjusting learning rate of group 0 to 3.0418e-03.\n",
      "Adjusting learning rate of group 0 to 3.0418e-03.\n",
      "Epoch : 1153,\ttrain_loss : 0.006157431751489639\tvalid_loss :0.00444116210564971\n",
      "Adjusting learning rate of group 0 to 3.0357e-03.\n",
      "Adjusting learning rate of group 0 to 3.0357e-03.\n",
      "Epoch : 1154,\ttrain_loss : 0.006233156658709049\tvalid_loss :0.004479498602449894\n",
      "Adjusting learning rate of group 0 to 3.0296e-03.\n",
      "Adjusting learning rate of group 0 to 3.0296e-03.\n",
      "Epoch : 1155,\ttrain_loss : 0.006104329135268927\tvalid_loss :0.004571724683046341\n",
      "Adjusting learning rate of group 0 to 3.0235e-03.\n",
      "Adjusting learning rate of group 0 to 3.0235e-03.\n",
      "Epoch : 1156,\ttrain_loss : 0.00612721499055624\tvalid_loss :0.004827903117984533\n",
      "Adjusting learning rate of group 0 to 3.0174e-03.\n",
      "Adjusting learning rate of group 0 to 3.0174e-03.\n",
      "Epoch : 1157,\ttrain_loss : 0.006183616351336241\tvalid_loss :0.005256799980998039\n",
      "Adjusting learning rate of group 0 to 3.0113e-03.\n",
      "Adjusting learning rate of group 0 to 3.0113e-03.\n",
      "Epoch : 1158,\ttrain_loss : 0.00622191047295928\tvalid_loss :0.005071008577942848\n",
      "Adjusting learning rate of group 0 to 3.0052e-03.\n",
      "Adjusting learning rate of group 0 to 3.0052e-03.\n",
      "Epoch : 1159,\ttrain_loss : 0.006163461599498987\tvalid_loss :0.004723356105387211\n",
      "Adjusting learning rate of group 0 to 2.9992e-03.\n",
      "Adjusting learning rate of group 0 to 2.9992e-03.\n",
      "Epoch : 1160,\ttrain_loss : 0.006115755997598171\tvalid_loss :0.005088409874588251\n",
      "Adjusting learning rate of group 0 to 2.9931e-03.\n",
      "Adjusting learning rate of group 0 to 2.9931e-03.\n",
      "Epoch : 1161,\ttrain_loss : 0.0061333924531936646\tvalid_loss :0.005401456728577614\n",
      "Adjusting learning rate of group 0 to 2.9870e-03.\n",
      "Adjusting learning rate of group 0 to 2.9870e-03.\n",
      "Epoch : 1162,\ttrain_loss : 0.006218330003321171\tvalid_loss :0.004718123469501734\n",
      "Adjusting learning rate of group 0 to 2.9809e-03.\n",
      "Adjusting learning rate of group 0 to 2.9809e-03.\n",
      "Epoch : 1163,\ttrain_loss : 0.00627503264695406\tvalid_loss :0.004572163335978985\n",
      "Adjusting learning rate of group 0 to 2.9748e-03.\n",
      "Adjusting learning rate of group 0 to 2.9748e-03.\n",
      "Epoch : 1164,\ttrain_loss : 0.006179950200021267\tvalid_loss :0.005038902163505554\n",
      "Adjusting learning rate of group 0 to 2.9688e-03.\n",
      "Adjusting learning rate of group 0 to 2.9688e-03.\n",
      "Epoch : 1165,\ttrain_loss : 0.006144460756331682\tvalid_loss :0.0046958657912909985\n",
      "Adjusting learning rate of group 0 to 2.9627e-03.\n",
      "Adjusting learning rate of group 0 to 2.9627e-03.\n",
      "Epoch : 1166,\ttrain_loss : 0.006199178751558065\tvalid_loss :0.004461344331502914\n",
      "Adjusting learning rate of group 0 to 2.9566e-03.\n",
      "Adjusting learning rate of group 0 to 2.9566e-03.\n",
      "Epoch : 1167,\ttrain_loss : 0.006148864980787039\tvalid_loss :0.005075997207313776\n",
      "Adjusting learning rate of group 0 to 2.9506e-03.\n",
      "Adjusting learning rate of group 0 to 2.9506e-03.\n",
      "Epoch : 1168,\ttrain_loss : 0.0061651309952139854\tvalid_loss :0.004999195691198111\n",
      "Adjusting learning rate of group 0 to 2.9445e-03.\n",
      "Adjusting learning rate of group 0 to 2.9445e-03.\n",
      "Epoch : 1169,\ttrain_loss : 0.0062072123400866985\tvalid_loss :0.004666942171752453\n",
      "Adjusting learning rate of group 0 to 2.9384e-03.\n",
      "Adjusting learning rate of group 0 to 2.9384e-03.\n",
      "Epoch : 1170,\ttrain_loss : 0.006119940429925919\tvalid_loss :0.004823385737836361\n",
      "Adjusting learning rate of group 0 to 2.9324e-03.\n",
      "Adjusting learning rate of group 0 to 2.9324e-03.\n",
      "Epoch : 1171,\ttrain_loss : 0.006173306610435247\tvalid_loss :0.00469715241342783\n",
      "Adjusting learning rate of group 0 to 2.9263e-03.\n",
      "Adjusting learning rate of group 0 to 2.9263e-03.\n",
      "Epoch : 1172,\ttrain_loss : 0.006177535746246576\tvalid_loss :0.004548863507807255\n",
      "Adjusting learning rate of group 0 to 2.9203e-03.\n",
      "Adjusting learning rate of group 0 to 2.9203e-03.\n",
      "Epoch : 1173,\ttrain_loss : 0.006122606806457043\tvalid_loss :0.004906086716800928\n",
      "Adjusting learning rate of group 0 to 2.9142e-03.\n",
      "Adjusting learning rate of group 0 to 2.9142e-03.\n",
      "Epoch : 1174,\ttrain_loss : 0.006182337645441294\tvalid_loss :0.004730430897325277\n",
      "Adjusting learning rate of group 0 to 2.9082e-03.\n",
      "Adjusting learning rate of group 0 to 2.9082e-03.\n",
      "Epoch : 1175,\ttrain_loss : 0.00609790300950408\tvalid_loss :0.0050806570798158646\n",
      "Adjusting learning rate of group 0 to 2.9021e-03.\n",
      "Adjusting learning rate of group 0 to 2.9021e-03.\n",
      "Epoch : 1176,\ttrain_loss : 0.0062995413318276405\tvalid_loss :0.004465083591639996\n",
      "Adjusting learning rate of group 0 to 2.8961e-03.\n",
      "Adjusting learning rate of group 0 to 2.8961e-03.\n",
      "Epoch : 1177,\ttrain_loss : 0.0061294627375900745\tvalid_loss :0.004496731795370579\n",
      "Adjusting learning rate of group 0 to 2.8901e-03.\n",
      "Adjusting learning rate of group 0 to 2.8901e-03.\n",
      "Epoch : 1178,\ttrain_loss : 0.00616966700181365\tvalid_loss :0.004332043696194887\n",
      "Adjusting learning rate of group 0 to 2.8840e-03.\n",
      "Adjusting learning rate of group 0 to 2.8840e-03.\n",
      "Epoch : 1179,\ttrain_loss : 0.0061072418466210365\tvalid_loss :0.00451018987223506\n",
      "Adjusting learning rate of group 0 to 2.8780e-03.\n",
      "Adjusting learning rate of group 0 to 2.8780e-03.\n",
      "Epoch : 1180,\ttrain_loss : 0.006167254876345396\tvalid_loss :0.004508357495069504\n",
      "Adjusting learning rate of group 0 to 2.8720e-03.\n",
      "Adjusting learning rate of group 0 to 2.8720e-03.\n",
      "Epoch : 1181,\ttrain_loss : 0.0060966224409639835\tvalid_loss :0.004636676050722599\n",
      "Adjusting learning rate of group 0 to 2.8659e-03.\n",
      "Adjusting learning rate of group 0 to 2.8659e-03.\n",
      "Epoch : 1182,\ttrain_loss : 0.00626163138076663\tvalid_loss :0.004311511293053627\n",
      "Adjusting learning rate of group 0 to 2.8599e-03.\n",
      "Adjusting learning rate of group 0 to 2.8599e-03.\n",
      "Epoch : 1183,\ttrain_loss : 0.006141900084912777\tvalid_loss :0.004361921455711126\n",
      "Adjusting learning rate of group 0 to 2.8539e-03.\n",
      "Adjusting learning rate of group 0 to 2.8539e-03.\n",
      "Epoch : 1184,\ttrain_loss : 0.006062875501811504\tvalid_loss :0.004537105094641447\n",
      "Adjusting learning rate of group 0 to 2.8479e-03.\n",
      "Adjusting learning rate of group 0 to 2.8479e-03.\n",
      "Epoch : 1185,\ttrain_loss : 0.006215775851160288\tvalid_loss :0.004186962731182575\n",
      "**********Valid loss decreased (0.004197 ==> 0.004187)**********\n",
      "Adjusting learning rate of group 0 to 2.8419e-03.\n",
      "Adjusting learning rate of group 0 to 2.8419e-03.\n",
      "Epoch : 1186,\ttrain_loss : 0.006091677118092775\tvalid_loss :0.004262653179466724\n",
      "Adjusting learning rate of group 0 to 2.8359e-03.\n",
      "Adjusting learning rate of group 0 to 2.8359e-03.\n",
      "Epoch : 1187,\ttrain_loss : 0.006104522850364447\tvalid_loss :0.004175487440079451\n",
      "**********Valid loss decreased (0.004187 ==> 0.004175)**********\n",
      "Adjusting learning rate of group 0 to 2.8298e-03.\n",
      "Adjusting learning rate of group 0 to 2.8298e-03.\n",
      "Epoch : 1188,\ttrain_loss : 0.006122536491602659\tvalid_loss :0.004279776476323605\n",
      "Adjusting learning rate of group 0 to 2.8238e-03.\n",
      "Adjusting learning rate of group 0 to 2.8238e-03.\n",
      "Epoch : 1189,\ttrain_loss : 0.0060991691425442696\tvalid_loss :0.004358399659395218\n",
      "Adjusting learning rate of group 0 to 2.8178e-03.\n",
      "Adjusting learning rate of group 0 to 2.8178e-03.\n",
      "Epoch : 1190,\ttrain_loss : 0.006160042714327574\tvalid_loss :0.004259561654180288\n",
      "Adjusting learning rate of group 0 to 2.8118e-03.\n",
      "Adjusting learning rate of group 0 to 2.8118e-03.\n",
      "Epoch : 1191,\ttrain_loss : 0.006127629894763231\tvalid_loss :0.0042408830486238\n",
      "Adjusting learning rate of group 0 to 2.8058e-03.\n",
      "Adjusting learning rate of group 0 to 2.8058e-03.\n",
      "Epoch : 1192,\ttrain_loss : 0.006072781980037689\tvalid_loss :0.0042726341634988785\n",
      "Adjusting learning rate of group 0 to 2.7998e-03.\n",
      "Adjusting learning rate of group 0 to 2.7998e-03.\n",
      "Epoch : 1193,\ttrain_loss : 0.006149097345769405\tvalid_loss :0.004347352311015129\n",
      "Adjusting learning rate of group 0 to 2.7938e-03.\n",
      "Adjusting learning rate of group 0 to 2.7938e-03.\n",
      "Epoch : 1194,\ttrain_loss : 0.006146719213575125\tvalid_loss :0.00428959634155035\n",
      "Adjusting learning rate of group 0 to 2.7879e-03.\n",
      "Adjusting learning rate of group 0 to 2.7879e-03.\n",
      "Epoch : 1195,\ttrain_loss : 0.006095505319535732\tvalid_loss :0.004233665764331818\n",
      "Adjusting learning rate of group 0 to 2.7819e-03.\n",
      "Adjusting learning rate of group 0 to 2.7819e-03.\n",
      "Epoch : 1196,\ttrain_loss : 0.006135431583970785\tvalid_loss :0.004223331343382597\n",
      "Adjusting learning rate of group 0 to 2.7759e-03.\n",
      "Adjusting learning rate of group 0 to 2.7759e-03.\n",
      "Epoch : 1197,\ttrain_loss : 0.006107457913458347\tvalid_loss :0.00430171936750412\n",
      "Adjusting learning rate of group 0 to 2.7699e-03.\n",
      "Adjusting learning rate of group 0 to 2.7699e-03.\n",
      "Epoch : 1198,\ttrain_loss : 0.0061312573961913586\tvalid_loss :0.0042627472430467606\n",
      "Adjusting learning rate of group 0 to 2.7639e-03.\n",
      "Adjusting learning rate of group 0 to 2.7639e-03.\n",
      "Epoch : 1199,\ttrain_loss : 0.006153923459351063\tvalid_loss :0.004296599887311459\n",
      "Adjusting learning rate of group 0 to 2.7580e-03.\n",
      "Adjusting learning rate of group 0 to 2.7580e-03.\n",
      "Epoch : 1200,\ttrain_loss : 0.0061087519861757755\tvalid_loss :0.004225434735417366\n",
      "Adjusting learning rate of group 0 to 2.7520e-03.\n",
      "Adjusting learning rate of group 0 to 2.7520e-03.\n",
      "Epoch : 1201,\ttrain_loss : 0.006064336281269789\tvalid_loss :0.004324308596551418\n",
      "Adjusting learning rate of group 0 to 2.7460e-03.\n",
      "Adjusting learning rate of group 0 to 2.7460e-03.\n",
      "Epoch : 1202,\ttrain_loss : 0.006154916249215603\tvalid_loss :0.00428659375756979\n",
      "Adjusting learning rate of group 0 to 2.7401e-03.\n",
      "Adjusting learning rate of group 0 to 2.7401e-03.\n",
      "Epoch : 1203,\ttrain_loss : 0.006098538637161255\tvalid_loss :0.004196748603135347\n",
      "Adjusting learning rate of group 0 to 2.7341e-03.\n",
      "Adjusting learning rate of group 0 to 2.7341e-03.\n",
      "Epoch : 1204,\ttrain_loss : 0.006059885490685701\tvalid_loss :0.004299312364310026\n",
      "Adjusting learning rate of group 0 to 2.7281e-03.\n",
      "Adjusting learning rate of group 0 to 2.7281e-03.\n",
      "Epoch : 1205,\ttrain_loss : 0.006114962510764599\tvalid_loss :0.004253866616636515\n",
      "Adjusting learning rate of group 0 to 2.7222e-03.\n",
      "Adjusting learning rate of group 0 to 2.7222e-03.\n",
      "Epoch : 1206,\ttrain_loss : 0.006148638669401407\tvalid_loss :0.004375196527689695\n",
      "Adjusting learning rate of group 0 to 2.7162e-03.\n",
      "Adjusting learning rate of group 0 to 2.7162e-03.\n",
      "Epoch : 1207,\ttrain_loss : 0.0061522419564425945\tvalid_loss :0.004341457039117813\n",
      "Adjusting learning rate of group 0 to 2.7103e-03.\n",
      "Adjusting learning rate of group 0 to 2.7103e-03.\n",
      "Epoch : 1208,\ttrain_loss : 0.0061570447869598866\tvalid_loss :0.004397999495267868\n",
      "Adjusting learning rate of group 0 to 2.7043e-03.\n",
      "Adjusting learning rate of group 0 to 2.7043e-03.\n",
      "Epoch : 1209,\ttrain_loss : 0.006113007664680481\tvalid_loss :0.004355750046670437\n",
      "Adjusting learning rate of group 0 to 2.6984e-03.\n",
      "Adjusting learning rate of group 0 to 2.6984e-03.\n",
      "Epoch : 1210,\ttrain_loss : 0.006128508131951094\tvalid_loss :0.004386335611343384\n",
      "Adjusting learning rate of group 0 to 2.6924e-03.\n",
      "Adjusting learning rate of group 0 to 2.6924e-03.\n",
      "Epoch : 1211,\ttrain_loss : 0.006145563442260027\tvalid_loss :0.004542120732367039\n",
      "Adjusting learning rate of group 0 to 2.6865e-03.\n",
      "Adjusting learning rate of group 0 to 2.6865e-03.\n",
      "Epoch : 1212,\ttrain_loss : 0.006215647328644991\tvalid_loss :0.004794082138687372\n",
      "Adjusting learning rate of group 0 to 2.6806e-03.\n",
      "Adjusting learning rate of group 0 to 2.6806e-03.\n",
      "Epoch : 1213,\ttrain_loss : 0.006338578183203936\tvalid_loss :0.004624791443347931\n",
      "Adjusting learning rate of group 0 to 2.6746e-03.\n",
      "Adjusting learning rate of group 0 to 2.6746e-03.\n",
      "Epoch : 1214,\ttrain_loss : 0.0062665194272994995\tvalid_loss :0.004778007045388222\n",
      "Adjusting learning rate of group 0 to 2.6687e-03.\n",
      "Adjusting learning rate of group 0 to 2.6687e-03.\n",
      "Epoch : 1215,\ttrain_loss : 0.006266665179282427\tvalid_loss :0.004995889496058226\n",
      "Adjusting learning rate of group 0 to 2.6628e-03.\n",
      "Adjusting learning rate of group 0 to 2.6628e-03.\n",
      "Epoch : 1216,\ttrain_loss : 0.0063627646304667\tvalid_loss :0.004867784678936005\n",
      "Adjusting learning rate of group 0 to 2.6569e-03.\n",
      "Adjusting learning rate of group 0 to 2.6569e-03.\n",
      "Epoch : 1217,\ttrain_loss : 0.006279502995312214\tvalid_loss :0.004658068995922804\n",
      "Adjusting learning rate of group 0 to 2.6510e-03.\n",
      "Adjusting learning rate of group 0 to 2.6510e-03.\n",
      "Epoch : 1218,\ttrain_loss : 0.006312427576631308\tvalid_loss :0.004805539269000292\n",
      "Adjusting learning rate of group 0 to 2.6450e-03.\n",
      "Adjusting learning rate of group 0 to 2.6450e-03.\n",
      "Epoch : 1219,\ttrain_loss : 0.006248497869819403\tvalid_loss :0.004464681260287762\n",
      "Adjusting learning rate of group 0 to 2.6391e-03.\n",
      "Adjusting learning rate of group 0 to 2.6391e-03.\n",
      "Epoch : 1220,\ttrain_loss : 0.006201163865625858\tvalid_loss :0.004805142991244793\n",
      "Adjusting learning rate of group 0 to 2.6332e-03.\n",
      "Adjusting learning rate of group 0 to 2.6332e-03.\n",
      "Epoch : 1221,\ttrain_loss : 0.0063665215857326984\tvalid_loss :0.0047384630888700485\n",
      "Adjusting learning rate of group 0 to 2.6273e-03.\n",
      "Adjusting learning rate of group 0 to 2.6273e-03.\n",
      "Epoch : 1222,\ttrain_loss : 0.006463353522121906\tvalid_loss :0.004246685188263655\n",
      "Adjusting learning rate of group 0 to 2.6214e-03.\n",
      "Adjusting learning rate of group 0 to 2.6214e-03.\n",
      "Epoch : 1223,\ttrain_loss : 0.006172310095280409\tvalid_loss :0.004496867768466473\n",
      "Adjusting learning rate of group 0 to 2.6155e-03.\n",
      "Adjusting learning rate of group 0 to 2.6155e-03.\n",
      "Epoch : 1224,\ttrain_loss : 0.006195863243192434\tvalid_loss :0.004589467775076628\n",
      "Adjusting learning rate of group 0 to 2.6096e-03.\n",
      "Adjusting learning rate of group 0 to 2.6096e-03.\n",
      "Epoch : 1225,\ttrain_loss : 0.006222385913133621\tvalid_loss :0.0044698212295770645\n",
      "Adjusting learning rate of group 0 to 2.6037e-03.\n",
      "Adjusting learning rate of group 0 to 2.6037e-03.\n",
      "Epoch : 1226,\ttrain_loss : 0.006154988426715136\tvalid_loss :0.004516997374594212\n",
      "Adjusting learning rate of group 0 to 2.5979e-03.\n",
      "Adjusting learning rate of group 0 to 2.5979e-03.\n",
      "Epoch : 1227,\ttrain_loss : 0.006163196172565222\tvalid_loss :0.004534660838544369\n",
      "Adjusting learning rate of group 0 to 2.5920e-03.\n",
      "Adjusting learning rate of group 0 to 2.5920e-03.\n",
      "Epoch : 1228,\ttrain_loss : 0.006118456833064556\tvalid_loss :0.004521403461694717\n",
      "Adjusting learning rate of group 0 to 2.5861e-03.\n",
      "Adjusting learning rate of group 0 to 2.5861e-03.\n",
      "Epoch : 1229,\ttrain_loss : 0.006090917158871889\tvalid_loss :0.0046367403119802475\n",
      "Adjusting learning rate of group 0 to 2.5802e-03.\n",
      "Adjusting learning rate of group 0 to 2.5802e-03.\n",
      "Epoch : 1230,\ttrain_loss : 0.006301818881183863\tvalid_loss :0.004427624866366386\n",
      "Adjusting learning rate of group 0 to 2.5744e-03.\n",
      "Adjusting learning rate of group 0 to 2.5744e-03.\n",
      "Epoch : 1231,\ttrain_loss : 0.006108751054853201\tvalid_loss :0.004537544678896666\n",
      "Adjusting learning rate of group 0 to 2.5685e-03.\n",
      "Adjusting learning rate of group 0 to 2.5685e-03.\n",
      "Epoch : 1232,\ttrain_loss : 0.006262509617954493\tvalid_loss :0.004506617318838835\n",
      "Adjusting learning rate of group 0 to 2.5626e-03.\n",
      "Adjusting learning rate of group 0 to 2.5626e-03.\n",
      "Epoch : 1233,\ttrain_loss : 0.006168539170175791\tvalid_loss :0.004243060480803251\n",
      "Adjusting learning rate of group 0 to 2.5568e-03.\n",
      "Adjusting learning rate of group 0 to 2.5568e-03.\n",
      "Epoch : 1234,\ttrain_loss : 0.0060431999154388905\tvalid_loss :0.004527375102043152\n",
      "Adjusting learning rate of group 0 to 2.5509e-03.\n",
      "Adjusting learning rate of group 0 to 2.5509e-03.\n",
      "Epoch : 1235,\ttrain_loss : 0.006085577420890331\tvalid_loss :0.004499691538512707\n",
      "Adjusting learning rate of group 0 to 2.5450e-03.\n",
      "Adjusting learning rate of group 0 to 2.5450e-03.\n",
      "Epoch : 1236,\ttrain_loss : 0.006165601778775454\tvalid_loss :0.004383597988635302\n",
      "Adjusting learning rate of group 0 to 2.5392e-03.\n",
      "Adjusting learning rate of group 0 to 2.5392e-03.\n",
      "Epoch : 1237,\ttrain_loss : 0.006098293699324131\tvalid_loss :0.004369545262306929\n",
      "Adjusting learning rate of group 0 to 2.5333e-03.\n",
      "Adjusting learning rate of group 0 to 2.5333e-03.\n",
      "Epoch : 1238,\ttrain_loss : 0.0060924505814909935\tvalid_loss :0.004477297887206078\n",
      "Adjusting learning rate of group 0 to 2.5275e-03.\n",
      "Adjusting learning rate of group 0 to 2.5275e-03.\n",
      "Epoch : 1239,\ttrain_loss : 0.006112969014793634\tvalid_loss :0.004349755123257637\n",
      "Adjusting learning rate of group 0 to 2.5217e-03.\n",
      "Adjusting learning rate of group 0 to 2.5217e-03.\n",
      "Epoch : 1240,\ttrain_loss : 0.00610328558832407\tvalid_loss :0.004498887341469526\n",
      "Adjusting learning rate of group 0 to 2.5158e-03.\n",
      "Adjusting learning rate of group 0 to 2.5158e-03.\n",
      "Epoch : 1241,\ttrain_loss : 0.006176254246383905\tvalid_loss :0.004292291589081287\n",
      "Adjusting learning rate of group 0 to 2.5100e-03.\n",
      "Adjusting learning rate of group 0 to 2.5100e-03.\n",
      "Epoch : 1242,\ttrain_loss : 0.005990008823573589\tvalid_loss :0.0044792285189032555\n",
      "Adjusting learning rate of group 0 to 2.5042e-03.\n",
      "Adjusting learning rate of group 0 to 2.5042e-03.\n",
      "Epoch : 1243,\ttrain_loss : 0.0060518947429955006\tvalid_loss :0.004429345019161701\n",
      "Adjusting learning rate of group 0 to 2.4983e-03.\n",
      "Adjusting learning rate of group 0 to 2.4983e-03.\n",
      "Epoch : 1244,\ttrain_loss : 0.006088952533900738\tvalid_loss :0.004319075029343367\n",
      "Adjusting learning rate of group 0 to 2.4925e-03.\n",
      "Adjusting learning rate of group 0 to 2.4925e-03.\n",
      "Epoch : 1245,\ttrain_loss : 0.006121295504271984\tvalid_loss :0.004504750017076731\n",
      "Adjusting learning rate of group 0 to 2.4867e-03.\n",
      "Adjusting learning rate of group 0 to 2.4867e-03.\n",
      "Epoch : 1246,\ttrain_loss : 0.00610939459875226\tvalid_loss :0.004290725104510784\n",
      "Adjusting learning rate of group 0 to 2.4809e-03.\n",
      "Adjusting learning rate of group 0 to 2.4809e-03.\n",
      "Epoch : 1247,\ttrain_loss : 0.006088079418987036\tvalid_loss :0.0044023459777235985\n",
      "Adjusting learning rate of group 0 to 2.4751e-03.\n",
      "Adjusting learning rate of group 0 to 2.4751e-03.\n",
      "Epoch : 1248,\ttrain_loss : 0.006043580360710621\tvalid_loss :0.0043449848890304565\n",
      "Adjusting learning rate of group 0 to 2.4693e-03.\n",
      "Adjusting learning rate of group 0 to 2.4693e-03.\n",
      "Epoch : 1249,\ttrain_loss : 0.006106086075305939\tvalid_loss :0.004358882550150156\n",
      "Adjusting learning rate of group 0 to 2.4635e-03.\n",
      "Adjusting learning rate of group 0 to 2.4635e-03.\n",
      "Epoch : 1250,\ttrain_loss : 0.00602060928940773\tvalid_loss :0.004431520123034716\n",
      "Adjusting learning rate of group 0 to 2.4577e-03.\n",
      "Adjusting learning rate of group 0 to 2.4577e-03.\n",
      "Epoch : 1251,\ttrain_loss : 0.006051052361726761\tvalid_loss :0.00429847976192832\n",
      "Adjusting learning rate of group 0 to 2.4519e-03.\n",
      "Adjusting learning rate of group 0 to 2.4519e-03.\n",
      "Epoch : 1252,\ttrain_loss : 0.006098824553191662\tvalid_loss :0.004420967772603035\n",
      "Adjusting learning rate of group 0 to 2.4461e-03.\n",
      "Adjusting learning rate of group 0 to 2.4461e-03.\n",
      "Epoch : 1253,\ttrain_loss : 0.006033741869032383\tvalid_loss :0.004336081445217133\n",
      "Adjusting learning rate of group 0 to 2.4403e-03.\n",
      "Adjusting learning rate of group 0 to 2.4403e-03.\n",
      "Epoch : 1254,\ttrain_loss : 0.0060645220801234245\tvalid_loss :0.00425676116719842\n",
      "Adjusting learning rate of group 0 to 2.4345e-03.\n",
      "Adjusting learning rate of group 0 to 2.4345e-03.\n",
      "Epoch : 1255,\ttrain_loss : 0.006012479308992624\tvalid_loss :0.004265079274773598\n",
      "Adjusting learning rate of group 0 to 2.4287e-03.\n",
      "Adjusting learning rate of group 0 to 2.4287e-03.\n",
      "Epoch : 1256,\ttrain_loss : 0.00601563137024641\tvalid_loss :0.004320948384702206\n",
      "Adjusting learning rate of group 0 to 2.4229e-03.\n",
      "Adjusting learning rate of group 0 to 2.4229e-03.\n",
      "Epoch : 1257,\ttrain_loss : 0.006039116065949202\tvalid_loss :0.0042754788883030415\n",
      "Adjusting learning rate of group 0 to 2.4172e-03.\n",
      "Adjusting learning rate of group 0 to 2.4172e-03.\n",
      "Epoch : 1258,\ttrain_loss : 0.00608435645699501\tvalid_loss :0.0043022846803069115\n",
      "Adjusting learning rate of group 0 to 2.4114e-03.\n",
      "Adjusting learning rate of group 0 to 2.4114e-03.\n",
      "Epoch : 1259,\ttrain_loss : 0.005973347928375006\tvalid_loss :0.004427447449415922\n",
      "Adjusting learning rate of group 0 to 2.4056e-03.\n",
      "Adjusting learning rate of group 0 to 2.4056e-03.\n",
      "Epoch : 1260,\ttrain_loss : 0.006142487283796072\tvalid_loss :0.004295988008379936\n",
      "Adjusting learning rate of group 0 to 2.3999e-03.\n",
      "Adjusting learning rate of group 0 to 2.3999e-03.\n",
      "Epoch : 1261,\ttrain_loss : 0.006012546364217997\tvalid_loss :0.004214190412312746\n",
      "Adjusting learning rate of group 0 to 2.3941e-03.\n",
      "Adjusting learning rate of group 0 to 2.3941e-03.\n",
      "Epoch : 1262,\ttrain_loss : 0.006069458555430174\tvalid_loss :0.004366043023765087\n",
      "Adjusting learning rate of group 0 to 2.3884e-03.\n",
      "Adjusting learning rate of group 0 to 2.3884e-03.\n",
      "Epoch : 1263,\ttrain_loss : 0.006005673669278622\tvalid_loss :0.004283210262656212\n",
      "Adjusting learning rate of group 0 to 2.3826e-03.\n",
      "Adjusting learning rate of group 0 to 2.3826e-03.\n",
      "Epoch : 1264,\ttrain_loss : 0.0059756808914244175\tvalid_loss :0.004416740499436855\n",
      "Adjusting learning rate of group 0 to 2.3769e-03.\n",
      "Adjusting learning rate of group 0 to 2.3769e-03.\n",
      "Epoch : 1265,\ttrain_loss : 0.006051220931112766\tvalid_loss :0.004192239139229059\n",
      "Adjusting learning rate of group 0 to 2.3711e-03.\n",
      "Adjusting learning rate of group 0 to 2.3711e-03.\n",
      "Epoch : 1266,\ttrain_loss : 0.005998673383146524\tvalid_loss :0.004375346004962921\n",
      "Adjusting learning rate of group 0 to 2.3654e-03.\n",
      "Adjusting learning rate of group 0 to 2.3654e-03.\n",
      "Epoch : 1267,\ttrain_loss : 0.00604515615850687\tvalid_loss :0.0042375801131129265\n",
      "Adjusting learning rate of group 0 to 2.3597e-03.\n",
      "Adjusting learning rate of group 0 to 2.3597e-03.\n",
      "Epoch : 1268,\ttrain_loss : 0.005988712422549725\tvalid_loss :0.004226881545037031\n",
      "Adjusting learning rate of group 0 to 2.3539e-03.\n",
      "Adjusting learning rate of group 0 to 2.3539e-03.\n",
      "Epoch : 1269,\ttrain_loss : 0.005992935039103031\tvalid_loss :0.00436188792809844\n",
      "Adjusting learning rate of group 0 to 2.3482e-03.\n",
      "Adjusting learning rate of group 0 to 2.3482e-03.\n",
      "Epoch : 1270,\ttrain_loss : 0.006027901079505682\tvalid_loss :0.004161595366895199\n",
      "**********Valid loss decreased (0.004175 ==> 0.004162)**********\n",
      "Adjusting learning rate of group 0 to 2.3425e-03.\n",
      "Adjusting learning rate of group 0 to 2.3425e-03.\n",
      "Epoch : 1271,\ttrain_loss : 0.006015014834702015\tvalid_loss :0.0042927805334329605\n",
      "Adjusting learning rate of group 0 to 2.3368e-03.\n",
      "Adjusting learning rate of group 0 to 2.3368e-03.\n",
      "Epoch : 1272,\ttrain_loss : 0.006002263631671667\tvalid_loss :0.0042054723016917706\n",
      "Adjusting learning rate of group 0 to 2.3311e-03.\n",
      "Adjusting learning rate of group 0 to 2.3311e-03.\n",
      "Epoch : 1273,\ttrain_loss : 0.006092166993767023\tvalid_loss :0.004288967698812485\n",
      "Adjusting learning rate of group 0 to 2.3254e-03.\n",
      "Adjusting learning rate of group 0 to 2.3254e-03.\n",
      "Epoch : 1274,\ttrain_loss : 0.006085312459617853\tvalid_loss :0.004285356029868126\n",
      "Adjusting learning rate of group 0 to 2.3197e-03.\n",
      "Adjusting learning rate of group 0 to 2.3197e-03.\n",
      "Epoch : 1275,\ttrain_loss : 0.006021968554705381\tvalid_loss :0.004192219581454992\n",
      "Adjusting learning rate of group 0 to 2.3140e-03.\n",
      "Adjusting learning rate of group 0 to 2.3140e-03.\n",
      "Epoch : 1276,\ttrain_loss : 0.006113895680755377\tvalid_loss :0.004338237456977367\n",
      "Adjusting learning rate of group 0 to 2.3083e-03.\n",
      "Adjusting learning rate of group 0 to 2.3083e-03.\n",
      "Epoch : 1277,\ttrain_loss : 0.006130121182650328\tvalid_loss :0.0042715659365057945\n",
      "Adjusting learning rate of group 0 to 2.3026e-03.\n",
      "Adjusting learning rate of group 0 to 2.3026e-03.\n",
      "Epoch : 1278,\ttrain_loss : 0.006033044308423996\tvalid_loss :0.004157237708568573\n",
      "**********Valid loss decreased (0.004162 ==> 0.004157)**********\n",
      "Adjusting learning rate of group 0 to 2.2969e-03.\n",
      "Adjusting learning rate of group 0 to 2.2969e-03.\n",
      "Epoch : 1279,\ttrain_loss : 0.005984738003462553\tvalid_loss :0.004163994453847408\n",
      "Adjusting learning rate of group 0 to 2.2912e-03.\n",
      "Adjusting learning rate of group 0 to 2.2912e-03.\n",
      "Epoch : 1280,\ttrain_loss : 0.005981379188597202\tvalid_loss :0.004131654277443886\n",
      "**********Valid loss decreased (0.004157 ==> 0.004132)**********\n",
      "Adjusting learning rate of group 0 to 2.2855e-03.\n",
      "Adjusting learning rate of group 0 to 2.2855e-03.\n",
      "Epoch : 1281,\ttrain_loss : 0.00596716720610857\tvalid_loss :0.004225131124258041\n",
      "Adjusting learning rate of group 0 to 2.2798e-03.\n",
      "Adjusting learning rate of group 0 to 2.2798e-03.\n",
      "Epoch : 1282,\ttrain_loss : 0.006039162632077932\tvalid_loss :0.0041684252209961414\n",
      "Adjusting learning rate of group 0 to 2.2742e-03.\n",
      "Adjusting learning rate of group 0 to 2.2742e-03.\n",
      "Epoch : 1283,\ttrain_loss : 0.006038972642272711\tvalid_loss :0.0041758958250284195\n",
      "Adjusting learning rate of group 0 to 2.2685e-03.\n",
      "Adjusting learning rate of group 0 to 2.2685e-03.\n",
      "Epoch : 1284,\ttrain_loss : 0.005998141597956419\tvalid_loss :0.004137943498790264\n",
      "Adjusting learning rate of group 0 to 2.2628e-03.\n",
      "Adjusting learning rate of group 0 to 2.2628e-03.\n",
      "Epoch : 1285,\ttrain_loss : 0.005957978777587414\tvalid_loss :0.004201601259410381\n",
      "Adjusting learning rate of group 0 to 2.2572e-03.\n",
      "Adjusting learning rate of group 0 to 2.2572e-03.\n",
      "Epoch : 1286,\ttrain_loss : 0.0060461899265646935\tvalid_loss :0.004204744938760996\n",
      "Adjusting learning rate of group 0 to 2.2515e-03.\n",
      "Adjusting learning rate of group 0 to 2.2515e-03.\n",
      "Epoch : 1287,\ttrain_loss : 0.006030777934938669\tvalid_loss :0.0041968743316829205\n",
      "Adjusting learning rate of group 0 to 2.2459e-03.\n",
      "Adjusting learning rate of group 0 to 2.2459e-03.\n",
      "Epoch : 1288,\ttrain_loss : 0.0059931958094239235\tvalid_loss :0.004145137500017881\n",
      "Adjusting learning rate of group 0 to 2.2402e-03.\n",
      "Adjusting learning rate of group 0 to 2.2402e-03.\n",
      "Epoch : 1289,\ttrain_loss : 0.00597419822588563\tvalid_loss :0.00416105380281806\n",
      "Adjusting learning rate of group 0 to 2.2346e-03.\n",
      "Adjusting learning rate of group 0 to 2.2346e-03.\n",
      "Epoch : 1290,\ttrain_loss : 0.0060351197607815266\tvalid_loss :0.004137237556278706\n",
      "Adjusting learning rate of group 0 to 2.2290e-03.\n",
      "Adjusting learning rate of group 0 to 2.2290e-03.\n",
      "Epoch : 1291,\ttrain_loss : 0.006027395371347666\tvalid_loss :0.004180237650871277\n",
      "Adjusting learning rate of group 0 to 2.2233e-03.\n",
      "Adjusting learning rate of group 0 to 2.2233e-03.\n",
      "Epoch : 1292,\ttrain_loss : 0.005998064763844013\tvalid_loss :0.004166021011769772\n",
      "Adjusting learning rate of group 0 to 2.2177e-03.\n",
      "Adjusting learning rate of group 0 to 2.2177e-03.\n",
      "Epoch : 1293,\ttrain_loss : 0.006074804347008467\tvalid_loss :0.004173940047621727\n",
      "Adjusting learning rate of group 0 to 2.2121e-03.\n",
      "Adjusting learning rate of group 0 to 2.2121e-03.\n",
      "Epoch : 1294,\ttrain_loss : 0.00601809611544013\tvalid_loss :0.004246010910719633\n",
      "Adjusting learning rate of group 0 to 2.2065e-03.\n",
      "Adjusting learning rate of group 0 to 2.2065e-03.\n",
      "Epoch : 1295,\ttrain_loss : 0.006020150613039732\tvalid_loss :0.004124797880649567\n",
      "**********Valid loss decreased (0.004132 ==> 0.004125)**********\n",
      "Adjusting learning rate of group 0 to 2.2009e-03.\n",
      "Adjusting learning rate of group 0 to 2.2009e-03.\n",
      "Epoch : 1296,\ttrain_loss : 0.006038710009306669\tvalid_loss :0.004223283845931292\n",
      "Adjusting learning rate of group 0 to 2.1952e-03.\n",
      "Adjusting learning rate of group 0 to 2.1952e-03.\n",
      "Epoch : 1297,\ttrain_loss : 0.006016521714627743\tvalid_loss :0.004313095007091761\n",
      "Adjusting learning rate of group 0 to 2.1896e-03.\n",
      "Adjusting learning rate of group 0 to 2.1896e-03.\n",
      "Epoch : 1298,\ttrain_loss : 0.005964882206171751\tvalid_loss :0.004153767600655556\n",
      "Adjusting learning rate of group 0 to 2.1840e-03.\n",
      "Adjusting learning rate of group 0 to 2.1840e-03.\n",
      "Epoch : 1299,\ttrain_loss : 0.006080626975744963\tvalid_loss :0.004287998657673597\n",
      "Adjusting learning rate of group 0 to 2.1784e-03.\n",
      "Adjusting learning rate of group 0 to 2.1784e-03.\n",
      "Epoch : 1300,\ttrain_loss : 0.006002294365316629\tvalid_loss :0.004234672524034977\n",
      "Adjusting learning rate of group 0 to 2.1729e-03.\n",
      "Adjusting learning rate of group 0 to 2.1729e-03.\n",
      "Epoch : 1301,\ttrain_loss : 0.005982666742056608\tvalid_loss :0.004120382945984602\n",
      "**********Valid loss decreased (0.004125 ==> 0.004120)**********\n",
      "Adjusting learning rate of group 0 to 2.1673e-03.\n",
      "Adjusting learning rate of group 0 to 2.1673e-03.\n",
      "Epoch : 1302,\ttrain_loss : 0.006001348607242107\tvalid_loss :0.004116974771022797\n",
      "**********Valid loss decreased (0.004120 ==> 0.004117)**********\n",
      "Adjusting learning rate of group 0 to 2.1617e-03.\n",
      "Adjusting learning rate of group 0 to 2.1617e-03.\n",
      "Epoch : 1303,\ttrain_loss : 0.005952149163931608\tvalid_loss :0.004126821644604206\n",
      "Adjusting learning rate of group 0 to 2.1561e-03.\n",
      "Adjusting learning rate of group 0 to 2.1561e-03.\n",
      "Epoch : 1304,\ttrain_loss : 0.005984453950077295\tvalid_loss :0.004125376231968403\n",
      "Adjusting learning rate of group 0 to 2.1505e-03.\n",
      "Adjusting learning rate of group 0 to 2.1505e-03.\n",
      "Epoch : 1305,\ttrain_loss : 0.006047758739441633\tvalid_loss :0.004219883121550083\n",
      "Adjusting learning rate of group 0 to 2.1450e-03.\n",
      "Adjusting learning rate of group 0 to 2.1450e-03.\n",
      "Epoch : 1306,\ttrain_loss : 0.006055302452296019\tvalid_loss :0.004290001001209021\n",
      "Adjusting learning rate of group 0 to 2.1394e-03.\n",
      "Adjusting learning rate of group 0 to 2.1394e-03.\n",
      "Epoch : 1307,\ttrain_loss : 0.005985167808830738\tvalid_loss :0.004290853161364794\n",
      "Adjusting learning rate of group 0 to 2.1338e-03.\n",
      "Adjusting learning rate of group 0 to 2.1338e-03.\n",
      "Epoch : 1308,\ttrain_loss : 0.006059884559363127\tvalid_loss :0.004135835915803909\n",
      "Adjusting learning rate of group 0 to 2.1283e-03.\n",
      "Adjusting learning rate of group 0 to 2.1283e-03.\n",
      "Epoch : 1309,\ttrain_loss : 0.006071661598980427\tvalid_loss :0.004325943533331156\n",
      "Adjusting learning rate of group 0 to 2.1227e-03.\n",
      "Adjusting learning rate of group 0 to 2.1227e-03.\n",
      "Epoch : 1310,\ttrain_loss : 0.006013392936438322\tvalid_loss :0.004118884447962046\n",
      "Adjusting learning rate of group 0 to 2.1172e-03.\n",
      "Adjusting learning rate of group 0 to 2.1172e-03.\n",
      "Epoch : 1311,\ttrain_loss : 0.006058032158762217\tvalid_loss :0.00417109252884984\n",
      "Adjusting learning rate of group 0 to 2.1116e-03.\n",
      "Adjusting learning rate of group 0 to 2.1116e-03.\n",
      "Epoch : 1312,\ttrain_loss : 0.00604222621768713\tvalid_loss :0.004370817914605141\n",
      "Adjusting learning rate of group 0 to 2.1061e-03.\n",
      "Adjusting learning rate of group 0 to 2.1061e-03.\n",
      "Epoch : 1313,\ttrain_loss : 0.0060686529614031315\tvalid_loss :0.004326450638473034\n",
      "Adjusting learning rate of group 0 to 2.1006e-03.\n",
      "Adjusting learning rate of group 0 to 2.1006e-03.\n",
      "Epoch : 1314,\ttrain_loss : 0.006093498319387436\tvalid_loss :0.004202739335596561\n",
      "Adjusting learning rate of group 0 to 2.0950e-03.\n",
      "Adjusting learning rate of group 0 to 2.0950e-03.\n",
      "Epoch : 1315,\ttrain_loss : 0.0060347686521708965\tvalid_loss :0.0041956109926104546\n",
      "Adjusting learning rate of group 0 to 2.0895e-03.\n",
      "Adjusting learning rate of group 0 to 2.0895e-03.\n",
      "Epoch : 1316,\ttrain_loss : 0.006081987172365189\tvalid_loss :0.00414699362590909\n",
      "Adjusting learning rate of group 0 to 2.0840e-03.\n",
      "Adjusting learning rate of group 0 to 2.0840e-03.\n",
      "Epoch : 1317,\ttrain_loss : 0.006048053037375212\tvalid_loss :0.004125794395804405\n",
      "Adjusting learning rate of group 0 to 2.0785e-03.\n",
      "Adjusting learning rate of group 0 to 2.0785e-03.\n",
      "Epoch : 1318,\ttrain_loss : 0.006023869384080172\tvalid_loss :0.004148419015109539\n",
      "Adjusting learning rate of group 0 to 2.0730e-03.\n",
      "Adjusting learning rate of group 0 to 2.0730e-03.\n",
      "Epoch : 1319,\ttrain_loss : 0.005993199069052935\tvalid_loss :0.004125123843550682\n",
      "Adjusting learning rate of group 0 to 2.0675e-03.\n",
      "Adjusting learning rate of group 0 to 2.0675e-03.\n",
      "Epoch : 1320,\ttrain_loss : 0.0060166786424815655\tvalid_loss :0.004140842705965042\n",
      "Adjusting learning rate of group 0 to 2.0620e-03.\n",
      "Adjusting learning rate of group 0 to 2.0620e-03.\n",
      "Epoch : 1321,\ttrain_loss : 0.006032928824424744\tvalid_loss :0.004139277618378401\n",
      "Adjusting learning rate of group 0 to 2.0565e-03.\n",
      "Adjusting learning rate of group 0 to 2.0565e-03.\n",
      "Epoch : 1322,\ttrain_loss : 0.006027009338140488\tvalid_loss :0.004094684962183237\n",
      "**********Valid loss decreased (0.004117 ==> 0.004095)**********\n",
      "Adjusting learning rate of group 0 to 2.0510e-03.\n",
      "Adjusting learning rate of group 0 to 2.0510e-03.\n",
      "Epoch : 1323,\ttrain_loss : 0.006092751864343882\tvalid_loss :0.0041429223492741585\n",
      "Adjusting learning rate of group 0 to 2.0455e-03.\n",
      "Adjusting learning rate of group 0 to 2.0455e-03.\n",
      "Epoch : 1324,\ttrain_loss : 0.006031055934727192\tvalid_loss :0.004147793166339397\n",
      "Adjusting learning rate of group 0 to 2.0400e-03.\n",
      "Adjusting learning rate of group 0 to 2.0400e-03.\n",
      "Epoch : 1325,\ttrain_loss : 0.0060457815416157246\tvalid_loss :0.004115965683013201\n",
      "Adjusting learning rate of group 0 to 2.0346e-03.\n",
      "Adjusting learning rate of group 0 to 2.0346e-03.\n",
      "Epoch : 1326,\ttrain_loss : 0.005940890870988369\tvalid_loss :0.004155950620770454\n",
      "Adjusting learning rate of group 0 to 2.0291e-03.\n",
      "Adjusting learning rate of group 0 to 2.0291e-03.\n",
      "Epoch : 1327,\ttrain_loss : 0.006092576775699854\tvalid_loss :0.004227625206112862\n",
      "Adjusting learning rate of group 0 to 2.0236e-03.\n",
      "Adjusting learning rate of group 0 to 2.0236e-03.\n",
      "Epoch : 1328,\ttrain_loss : 0.005938005167990923\tvalid_loss :0.004425065591931343\n",
      "Adjusting learning rate of group 0 to 2.0182e-03.\n",
      "Adjusting learning rate of group 0 to 2.0182e-03.\n",
      "Epoch : 1329,\ttrain_loss : 0.005996309220790863\tvalid_loss :0.00424521928653121\n",
      "Adjusting learning rate of group 0 to 2.0127e-03.\n",
      "Adjusting learning rate of group 0 to 2.0127e-03.\n",
      "Epoch : 1330,\ttrain_loss : 0.005944273434579372\tvalid_loss :0.004234662279486656\n",
      "Adjusting learning rate of group 0 to 2.0073e-03.\n",
      "Adjusting learning rate of group 0 to 2.0073e-03.\n",
      "Epoch : 1331,\ttrain_loss : 0.005953525193035603\tvalid_loss :0.004233642015606165\n",
      "Adjusting learning rate of group 0 to 2.0018e-03.\n",
      "Adjusting learning rate of group 0 to 2.0018e-03.\n",
      "Epoch : 1332,\ttrain_loss : 0.005949958693236113\tvalid_loss :0.004567547235637903\n",
      "Adjusting learning rate of group 0 to 1.9964e-03.\n",
      "Adjusting learning rate of group 0 to 1.9964e-03.\n",
      "Epoch : 1333,\ttrain_loss : 0.005932461004704237\tvalid_loss :0.004356927238404751\n",
      "Adjusting learning rate of group 0 to 1.9909e-03.\n",
      "Adjusting learning rate of group 0 to 1.9909e-03.\n",
      "Epoch : 1334,\ttrain_loss : 0.005971555598080158\tvalid_loss :0.0044191135093569756\n",
      "Adjusting learning rate of group 0 to 1.9855e-03.\n",
      "Adjusting learning rate of group 0 to 1.9855e-03.\n",
      "Epoch : 1335,\ttrain_loss : 0.006000194698572159\tvalid_loss :0.004305469803512096\n",
      "Adjusting learning rate of group 0 to 1.9801e-03.\n",
      "Adjusting learning rate of group 0 to 1.9801e-03.\n",
      "Epoch : 1336,\ttrain_loss : 0.005950337741523981\tvalid_loss :0.004164412617683411\n",
      "Adjusting learning rate of group 0 to 1.9747e-03.\n",
      "Adjusting learning rate of group 0 to 1.9747e-03.\n",
      "Epoch : 1337,\ttrain_loss : 0.005969375837594271\tvalid_loss :0.004366276785731316\n",
      "Adjusting learning rate of group 0 to 1.9692e-03.\n",
      "Adjusting learning rate of group 0 to 1.9692e-03.\n",
      "Epoch : 1338,\ttrain_loss : 0.0059409961104393005\tvalid_loss :0.004206744488328695\n",
      "Adjusting learning rate of group 0 to 1.9638e-03.\n",
      "Adjusting learning rate of group 0 to 1.9638e-03.\n",
      "Epoch : 1339,\ttrain_loss : 0.0059571717865765095\tvalid_loss :0.00432103406637907\n",
      "Adjusting learning rate of group 0 to 1.9584e-03.\n",
      "Adjusting learning rate of group 0 to 1.9584e-03.\n",
      "Epoch : 1340,\ttrain_loss : 0.005945821292698383\tvalid_loss :0.004308589734137058\n",
      "Adjusting learning rate of group 0 to 1.9530e-03.\n",
      "Adjusting learning rate of group 0 to 1.9530e-03.\n",
      "Epoch : 1341,\ttrain_loss : 0.005940862465649843\tvalid_loss :0.004326796159148216\n",
      "Adjusting learning rate of group 0 to 1.9476e-03.\n",
      "Adjusting learning rate of group 0 to 1.9476e-03.\n",
      "Epoch : 1342,\ttrain_loss : 0.005890932399779558\tvalid_loss :0.0043286411091685295\n",
      "Adjusting learning rate of group 0 to 1.9422e-03.\n",
      "Adjusting learning rate of group 0 to 1.9422e-03.\n",
      "Epoch : 1343,\ttrain_loss : 0.005913507658988237\tvalid_loss :0.004325231537222862\n",
      "Adjusting learning rate of group 0 to 1.9369e-03.\n",
      "Adjusting learning rate of group 0 to 1.9369e-03.\n",
      "Epoch : 1344,\ttrain_loss : 0.005902829114347696\tvalid_loss :0.004344042390584946\n",
      "Adjusting learning rate of group 0 to 1.9315e-03.\n",
      "Adjusting learning rate of group 0 to 1.9315e-03.\n",
      "Epoch : 1345,\ttrain_loss : 0.005898445378988981\tvalid_loss :0.004300151020288467\n",
      "Adjusting learning rate of group 0 to 1.9261e-03.\n",
      "Adjusting learning rate of group 0 to 1.9261e-03.\n",
      "Epoch : 1346,\ttrain_loss : 0.0058705625124275684\tvalid_loss :0.004282579757273197\n",
      "Adjusting learning rate of group 0 to 1.9207e-03.\n",
      "Adjusting learning rate of group 0 to 1.9207e-03.\n",
      "Epoch : 1347,\ttrain_loss : 0.005886682774871588\tvalid_loss :0.004276650957763195\n",
      "Adjusting learning rate of group 0 to 1.9154e-03.\n",
      "Adjusting learning rate of group 0 to 1.9154e-03.\n",
      "Epoch : 1348,\ttrain_loss : 0.005874398164451122\tvalid_loss :0.004239732399582863\n",
      "Adjusting learning rate of group 0 to 1.9100e-03.\n",
      "Adjusting learning rate of group 0 to 1.9100e-03.\n",
      "Epoch : 1349,\ttrain_loss : 0.005884494166821241\tvalid_loss :0.004214454907923937\n",
      "Adjusting learning rate of group 0 to 1.9047e-03.\n",
      "Adjusting learning rate of group 0 to 1.9047e-03.\n",
      "Epoch : 1350,\ttrain_loss : 0.005911608692258596\tvalid_loss :0.004189794883131981\n",
      "Adjusting learning rate of group 0 to 1.8993e-03.\n",
      "Adjusting learning rate of group 0 to 1.8993e-03.\n",
      "Epoch : 1351,\ttrain_loss : 0.005905555095523596\tvalid_loss :0.0042008510790765285\n",
      "Adjusting learning rate of group 0 to 1.8940e-03.\n",
      "Adjusting learning rate of group 0 to 1.8940e-03.\n",
      "Epoch : 1352,\ttrain_loss : 0.005925473291426897\tvalid_loss :0.004222032614052296\n",
      "Adjusting learning rate of group 0 to 1.8886e-03.\n",
      "Adjusting learning rate of group 0 to 1.8886e-03.\n",
      "Epoch : 1353,\ttrain_loss : 0.0059041595086455345\tvalid_loss :0.004240053240209818\n",
      "Adjusting learning rate of group 0 to 1.8833e-03.\n",
      "Adjusting learning rate of group 0 to 1.8833e-03.\n",
      "Epoch : 1354,\ttrain_loss : 0.00594624038785696\tvalid_loss :0.004261315800249577\n",
      "Adjusting learning rate of group 0 to 1.8780e-03.\n",
      "Adjusting learning rate of group 0 to 1.8780e-03.\n",
      "Epoch : 1355,\ttrain_loss : 0.005902978125959635\tvalid_loss :0.004358801990747452\n",
      "Adjusting learning rate of group 0 to 1.8726e-03.\n",
      "Adjusting learning rate of group 0 to 1.8726e-03.\n",
      "Epoch : 1356,\ttrain_loss : 0.005895105190575123\tvalid_loss :0.0043099792674183846\n",
      "Adjusting learning rate of group 0 to 1.8673e-03.\n",
      "Adjusting learning rate of group 0 to 1.8673e-03.\n",
      "Epoch : 1357,\ttrain_loss : 0.005894002504646778\tvalid_loss :0.00427302997559309\n",
      "Adjusting learning rate of group 0 to 1.8620e-03.\n",
      "Adjusting learning rate of group 0 to 1.8620e-03.\n",
      "Epoch : 1358,\ttrain_loss : 0.0058985305950045586\tvalid_loss :0.004307604860514402\n",
      "Adjusting learning rate of group 0 to 1.8567e-03.\n",
      "Adjusting learning rate of group 0 to 1.8567e-03.\n",
      "Epoch : 1359,\ttrain_loss : 0.0058846925385296345\tvalid_loss :0.004205059725791216\n",
      "Adjusting learning rate of group 0 to 1.8514e-03.\n",
      "Adjusting learning rate of group 0 to 1.8514e-03.\n",
      "Epoch : 1360,\ttrain_loss : 0.005897623486816883\tvalid_loss :0.00422858539968729\n",
      "Adjusting learning rate of group 0 to 1.8461e-03.\n",
      "Adjusting learning rate of group 0 to 1.8461e-03.\n",
      "Epoch : 1361,\ttrain_loss : 0.0059072114527225494\tvalid_loss :0.004255885258316994\n",
      "Adjusting learning rate of group 0 to 1.8408e-03.\n",
      "Adjusting learning rate of group 0 to 1.8408e-03.\n",
      "Epoch : 1362,\ttrain_loss : 0.005889129359275103\tvalid_loss :0.004276127554476261\n",
      "Adjusting learning rate of group 0 to 1.8355e-03.\n",
      "Adjusting learning rate of group 0 to 1.8355e-03.\n",
      "Epoch : 1363,\ttrain_loss : 0.005900223273783922\tvalid_loss :0.004365133587270975\n",
      "Adjusting learning rate of group 0 to 1.8302e-03.\n",
      "Adjusting learning rate of group 0 to 1.8302e-03.\n",
      "Epoch : 1364,\ttrain_loss : 0.005862549878656864\tvalid_loss :0.0042563327588140965\n",
      "Adjusting learning rate of group 0 to 1.8250e-03.\n",
      "Adjusting learning rate of group 0 to 1.8250e-03.\n",
      "Epoch : 1365,\ttrain_loss : 0.005860034376382828\tvalid_loss :0.004347603302448988\n",
      "Adjusting learning rate of group 0 to 1.8197e-03.\n",
      "Adjusting learning rate of group 0 to 1.8197e-03.\n",
      "Epoch : 1366,\ttrain_loss : 0.005818724632263184\tvalid_loss :0.004286140203475952\n",
      "Adjusting learning rate of group 0 to 1.8144e-03.\n",
      "Adjusting learning rate of group 0 to 1.8144e-03.\n",
      "Epoch : 1367,\ttrain_loss : 0.00585181312635541\tvalid_loss :0.004229440353810787\n",
      "Adjusting learning rate of group 0 to 1.8092e-03.\n",
      "Adjusting learning rate of group 0 to 1.8092e-03.\n",
      "Epoch : 1368,\ttrain_loss : 0.0058601172640919685\tvalid_loss :0.004292241297662258\n",
      "Adjusting learning rate of group 0 to 1.8039e-03.\n",
      "Adjusting learning rate of group 0 to 1.8039e-03.\n",
      "Epoch : 1369,\ttrain_loss : 0.005864676088094711\tvalid_loss :0.004342961125075817\n",
      "Adjusting learning rate of group 0 to 1.7987e-03.\n",
      "Adjusting learning rate of group 0 to 1.7987e-03.\n",
      "Epoch : 1370,\ttrain_loss : 0.005815198179334402\tvalid_loss :0.004258098546415567\n",
      "Adjusting learning rate of group 0 to 1.7934e-03.\n",
      "Adjusting learning rate of group 0 to 1.7934e-03.\n",
      "Epoch : 1371,\ttrain_loss : 0.005834122188389301\tvalid_loss :0.004363516811281443\n",
      "Adjusting learning rate of group 0 to 1.7882e-03.\n",
      "Adjusting learning rate of group 0 to 1.7882e-03.\n",
      "Epoch : 1372,\ttrain_loss : 0.005817405879497528\tvalid_loss :0.004277991130948067\n",
      "Adjusting learning rate of group 0 to 1.7829e-03.\n",
      "Adjusting learning rate of group 0 to 1.7829e-03.\n",
      "Epoch : 1373,\ttrain_loss : 0.005825351923704147\tvalid_loss :0.004217340610921383\n",
      "Adjusting learning rate of group 0 to 1.7777e-03.\n",
      "Adjusting learning rate of group 0 to 1.7777e-03.\n",
      "Epoch : 1374,\ttrain_loss : 0.005861879326403141\tvalid_loss :0.004354700446128845\n",
      "Adjusting learning rate of group 0 to 1.7725e-03.\n",
      "Adjusting learning rate of group 0 to 1.7725e-03.\n",
      "Epoch : 1375,\ttrain_loss : 0.005826854612678289\tvalid_loss :0.004294650163501501\n",
      "Adjusting learning rate of group 0 to 1.7673e-03.\n",
      "Adjusting learning rate of group 0 to 1.7673e-03.\n",
      "Epoch : 1376,\ttrain_loss : 0.00584805104881525\tvalid_loss :0.004290126264095306\n",
      "Adjusting learning rate of group 0 to 1.7621e-03.\n",
      "Adjusting learning rate of group 0 to 1.7621e-03.\n",
      "Epoch : 1377,\ttrain_loss : 0.005831505171954632\tvalid_loss :0.004198966547846794\n",
      "Adjusting learning rate of group 0 to 1.7569e-03.\n",
      "Adjusting learning rate of group 0 to 1.7569e-03.\n",
      "Epoch : 1378,\ttrain_loss : 0.005853747017681599\tvalid_loss :0.004209932871162891\n",
      "Adjusting learning rate of group 0 to 1.7517e-03.\n",
      "Adjusting learning rate of group 0 to 1.7517e-03.\n",
      "Epoch : 1379,\ttrain_loss : 0.005843127612024546\tvalid_loss :0.004222447052598\n",
      "Adjusting learning rate of group 0 to 1.7465e-03.\n",
      "Adjusting learning rate of group 0 to 1.7465e-03.\n",
      "Epoch : 1380,\ttrain_loss : 0.005848486442118883\tvalid_loss :0.00419962964951992\n",
      "Adjusting learning rate of group 0 to 1.7413e-03.\n",
      "Adjusting learning rate of group 0 to 1.7413e-03.\n",
      "Epoch : 1381,\ttrain_loss : 0.005849018692970276\tvalid_loss :0.004222069401293993\n",
      "Adjusting learning rate of group 0 to 1.7361e-03.\n",
      "Adjusting learning rate of group 0 to 1.7361e-03.\n",
      "Epoch : 1382,\ttrain_loss : 0.005852977745234966\tvalid_loss :0.004309228155761957\n",
      "Adjusting learning rate of group 0 to 1.7309e-03.\n",
      "Adjusting learning rate of group 0 to 1.7309e-03.\n",
      "Epoch : 1383,\ttrain_loss : 0.00585579639300704\tvalid_loss :0.004382586106657982\n",
      "Adjusting learning rate of group 0 to 1.7258e-03.\n",
      "Adjusting learning rate of group 0 to 1.7258e-03.\n",
      "Epoch : 1384,\ttrain_loss : 0.005832326132804155\tvalid_loss :0.0043161953799426556\n",
      "Adjusting learning rate of group 0 to 1.7206e-03.\n",
      "Adjusting learning rate of group 0 to 1.7206e-03.\n",
      "Epoch : 1385,\ttrain_loss : 0.0058288867585361\tvalid_loss :0.0043704332783818245\n",
      "Adjusting learning rate of group 0 to 1.7154e-03.\n",
      "Adjusting learning rate of group 0 to 1.7154e-03.\n",
      "Epoch : 1386,\ttrain_loss : 0.005793808493763208\tvalid_loss :0.004351613577455282\n",
      "Adjusting learning rate of group 0 to 1.7103e-03.\n",
      "Adjusting learning rate of group 0 to 1.7103e-03.\n",
      "Epoch : 1387,\ttrain_loss : 0.005782133899629116\tvalid_loss :0.004256389103829861\n",
      "Adjusting learning rate of group 0 to 1.7051e-03.\n",
      "Adjusting learning rate of group 0 to 1.7051e-03.\n",
      "Epoch : 1388,\ttrain_loss : 0.005819684825837612\tvalid_loss :0.004316012375056744\n",
      "Adjusting learning rate of group 0 to 1.7000e-03.\n",
      "Adjusting learning rate of group 0 to 1.7000e-03.\n",
      "Epoch : 1389,\ttrain_loss : 0.0058042919263243675\tvalid_loss :0.004295839928090572\n",
      "Adjusting learning rate of group 0 to 1.6948e-03.\n",
      "Adjusting learning rate of group 0 to 1.6948e-03.\n",
      "Epoch : 1390,\ttrain_loss : 0.005771039519459009\tvalid_loss :0.00421080132946372\n",
      "Adjusting learning rate of group 0 to 1.6897e-03.\n",
      "Adjusting learning rate of group 0 to 1.6897e-03.\n",
      "Epoch : 1391,\ttrain_loss : 0.005838054232299328\tvalid_loss :0.0042729973793029785\n",
      "Adjusting learning rate of group 0 to 1.6846e-03.\n",
      "Adjusting learning rate of group 0 to 1.6846e-03.\n",
      "Epoch : 1392,\ttrain_loss : 0.005815976765006781\tvalid_loss :0.004262576811015606\n",
      "Adjusting learning rate of group 0 to 1.6795e-03.\n",
      "Adjusting learning rate of group 0 to 1.6795e-03.\n",
      "Epoch : 1393,\ttrain_loss : 0.005836650729179382\tvalid_loss :0.004286802373826504\n",
      "Adjusting learning rate of group 0 to 1.6743e-03.\n",
      "Adjusting learning rate of group 0 to 1.6743e-03.\n",
      "Epoch : 1394,\ttrain_loss : 0.005831704940646887\tvalid_loss :0.004218205809593201\n",
      "Adjusting learning rate of group 0 to 1.6692e-03.\n",
      "Adjusting learning rate of group 0 to 1.6692e-03.\n",
      "Epoch : 1395,\ttrain_loss : 0.005844683386385441\tvalid_loss :0.0041953097097575665\n",
      "Adjusting learning rate of group 0 to 1.6641e-03.\n",
      "Adjusting learning rate of group 0 to 1.6641e-03.\n",
      "Epoch : 1396,\ttrain_loss : 0.005883385427296162\tvalid_loss :0.00444102892652154\n",
      "Adjusting learning rate of group 0 to 1.6590e-03.\n",
      "Adjusting learning rate of group 0 to 1.6590e-03.\n",
      "Epoch : 1397,\ttrain_loss : 0.005811539478600025\tvalid_loss :0.0043160212226212025\n",
      "Adjusting learning rate of group 0 to 1.6539e-03.\n",
      "Adjusting learning rate of group 0 to 1.6539e-03.\n",
      "Epoch : 1398,\ttrain_loss : 0.005803870968520641\tvalid_loss :0.004391929134726524\n",
      "Adjusting learning rate of group 0 to 1.6489e-03.\n",
      "Adjusting learning rate of group 0 to 1.6489e-03.\n",
      "Epoch : 1399,\ttrain_loss : 0.0058091976679861546\tvalid_loss :0.004282885696738958\n",
      "Adjusting learning rate of group 0 to 1.6438e-03.\n",
      "Adjusting learning rate of group 0 to 1.6438e-03.\n",
      "Epoch : 1400,\ttrain_loss : 0.005790729075670242\tvalid_loss :0.004322708584368229\n",
      "Adjusting learning rate of group 0 to 1.6387e-03.\n",
      "Adjusting learning rate of group 0 to 1.6387e-03.\n",
      "Epoch : 1401,\ttrain_loss : 0.005790114868432283\tvalid_loss :0.004309501964598894\n",
      "Adjusting learning rate of group 0 to 1.6336e-03.\n",
      "Adjusting learning rate of group 0 to 1.6336e-03.\n",
      "Epoch : 1402,\ttrain_loss : 0.005780245643109083\tvalid_loss :0.004295621998608112\n",
      "Adjusting learning rate of group 0 to 1.6286e-03.\n",
      "Adjusting learning rate of group 0 to 1.6286e-03.\n",
      "Epoch : 1403,\ttrain_loss : 0.00580220902338624\tvalid_loss :0.004182000644505024\n",
      "Adjusting learning rate of group 0 to 1.6235e-03.\n",
      "Adjusting learning rate of group 0 to 1.6235e-03.\n",
      "Epoch : 1404,\ttrain_loss : 0.005858290009200573\tvalid_loss :0.0044014365412294865\n",
      "Adjusting learning rate of group 0 to 1.6185e-03.\n",
      "Adjusting learning rate of group 0 to 1.6185e-03.\n",
      "Epoch : 1405,\ttrain_loss : 0.005784913897514343\tvalid_loss :0.004304118920117617\n",
      "Adjusting learning rate of group 0 to 1.6134e-03.\n",
      "Adjusting learning rate of group 0 to 1.6134e-03.\n",
      "Epoch : 1406,\ttrain_loss : 0.005821608938276768\tvalid_loss :0.00436962116509676\n",
      "Adjusting learning rate of group 0 to 1.6084e-03.\n",
      "Adjusting learning rate of group 0 to 1.6084e-03.\n",
      "Epoch : 1407,\ttrain_loss : 0.00581339793279767\tvalid_loss :0.004328303039073944\n",
      "Adjusting learning rate of group 0 to 1.6033e-03.\n",
      "Adjusting learning rate of group 0 to 1.6033e-03.\n",
      "Epoch : 1408,\ttrain_loss : 0.005760764237493277\tvalid_loss :0.00436395825818181\n",
      "Adjusting learning rate of group 0 to 1.5983e-03.\n",
      "Adjusting learning rate of group 0 to 1.5983e-03.\n",
      "Epoch : 1409,\ttrain_loss : 0.005780308973044157\tvalid_loss :0.0044050924479961395\n",
      "Adjusting learning rate of group 0 to 1.5933e-03.\n",
      "Adjusting learning rate of group 0 to 1.5933e-03.\n",
      "Epoch : 1410,\ttrain_loss : 0.005791458301246166\tvalid_loss :0.004394027404487133\n",
      "Adjusting learning rate of group 0 to 1.5883e-03.\n",
      "Adjusting learning rate of group 0 to 1.5883e-03.\n",
      "Epoch : 1411,\ttrain_loss : 0.0057946257293224335\tvalid_loss :0.004400897771120071\n",
      "Adjusting learning rate of group 0 to 1.5833e-03.\n",
      "Adjusting learning rate of group 0 to 1.5833e-03.\n",
      "Epoch : 1412,\ttrain_loss : 0.005789107643067837\tvalid_loss :0.004381079226732254\n",
      "Adjusting learning rate of group 0 to 1.5783e-03.\n",
      "Adjusting learning rate of group 0 to 1.5783e-03.\n",
      "Epoch : 1413,\ttrain_loss : 0.005767422262579203\tvalid_loss :0.004275127779692411\n",
      "Adjusting learning rate of group 0 to 1.5733e-03.\n",
      "Adjusting learning rate of group 0 to 1.5733e-03.\n",
      "Epoch : 1414,\ttrain_loss : 0.005757342558354139\tvalid_loss :0.004257480148226023\n",
      "Adjusting learning rate of group 0 to 1.5683e-03.\n",
      "Adjusting learning rate of group 0 to 1.5683e-03.\n",
      "Epoch : 1415,\ttrain_loss : 0.005776300560683012\tvalid_loss :0.004239384084939957\n",
      "Adjusting learning rate of group 0 to 1.5633e-03.\n",
      "Adjusting learning rate of group 0 to 1.5633e-03.\n",
      "Epoch : 1416,\ttrain_loss : 0.005791401024907827\tvalid_loss :0.004289980512112379\n",
      "Adjusting learning rate of group 0 to 1.5583e-03.\n",
      "Adjusting learning rate of group 0 to 1.5583e-03.\n",
      "Epoch : 1417,\ttrain_loss : 0.005742975976318121\tvalid_loss :0.004255287349224091\n",
      "Adjusting learning rate of group 0 to 1.5533e-03.\n",
      "Adjusting learning rate of group 0 to 1.5533e-03.\n",
      "Epoch : 1418,\ttrain_loss : 0.0057821981608867645\tvalid_loss :0.004235738422721624\n",
      "Adjusting learning rate of group 0 to 1.5484e-03.\n",
      "Adjusting learning rate of group 0 to 1.5484e-03.\n",
      "Epoch : 1419,\ttrain_loss : 0.00580787006765604\tvalid_loss :0.004469782114028931\n",
      "Adjusting learning rate of group 0 to 1.5434e-03.\n",
      "Adjusting learning rate of group 0 to 1.5434e-03.\n",
      "Epoch : 1420,\ttrain_loss : 0.005764068569988012\tvalid_loss :0.004215233493596315\n",
      "Adjusting learning rate of group 0 to 1.5385e-03.\n",
      "Adjusting learning rate of group 0 to 1.5385e-03.\n",
      "Epoch : 1421,\ttrain_loss : 0.005771201103925705\tvalid_loss :0.004229296930134296\n",
      "Adjusting learning rate of group 0 to 1.5335e-03.\n",
      "Adjusting learning rate of group 0 to 1.5335e-03.\n",
      "Epoch : 1422,\ttrain_loss : 0.005804053973406553\tvalid_loss :0.004405743908137083\n",
      "Adjusting learning rate of group 0 to 1.5286e-03.\n",
      "Adjusting learning rate of group 0 to 1.5286e-03.\n",
      "Epoch : 1423,\ttrain_loss : 0.005749721545726061\tvalid_loss :0.004215171094983816\n",
      "Adjusting learning rate of group 0 to 1.5236e-03.\n",
      "Adjusting learning rate of group 0 to 1.5236e-03.\n",
      "Epoch : 1424,\ttrain_loss : 0.005756924860179424\tvalid_loss :0.004265846684575081\n",
      "Adjusting learning rate of group 0 to 1.5187e-03.\n",
      "Adjusting learning rate of group 0 to 1.5187e-03.\n",
      "Epoch : 1425,\ttrain_loss : 0.005772255826741457\tvalid_loss :0.004273637663573027\n",
      "Adjusting learning rate of group 0 to 1.5138e-03.\n",
      "Adjusting learning rate of group 0 to 1.5138e-03.\n",
      "Epoch : 1426,\ttrain_loss : 0.005767409689724445\tvalid_loss :0.004261948633939028\n",
      "Adjusting learning rate of group 0 to 1.5088e-03.\n",
      "Adjusting learning rate of group 0 to 1.5088e-03.\n",
      "Epoch : 1427,\ttrain_loss : 0.0057363067753612995\tvalid_loss :0.004223964177072048\n",
      "Adjusting learning rate of group 0 to 1.5039e-03.\n",
      "Adjusting learning rate of group 0 to 1.5039e-03.\n",
      "Epoch : 1428,\ttrain_loss : 0.0057578193955123425\tvalid_loss :0.004179813899099827\n",
      "Adjusting learning rate of group 0 to 1.4990e-03.\n",
      "Adjusting learning rate of group 0 to 1.4990e-03.\n",
      "Epoch : 1429,\ttrain_loss : 0.005791754927486181\tvalid_loss :0.004293875768780708\n",
      "Adjusting learning rate of group 0 to 1.4941e-03.\n",
      "Adjusting learning rate of group 0 to 1.4941e-03.\n",
      "Epoch : 1430,\ttrain_loss : 0.005745687056332827\tvalid_loss :0.004207262769341469\n",
      "Adjusting learning rate of group 0 to 1.4892e-03.\n",
      "Adjusting learning rate of group 0 to 1.4892e-03.\n",
      "Epoch : 1431,\ttrain_loss : 0.005756706930696964\tvalid_loss :0.0042519718408584595\n",
      "Adjusting learning rate of group 0 to 1.4843e-03.\n",
      "Adjusting learning rate of group 0 to 1.4843e-03.\n",
      "Epoch : 1432,\ttrain_loss : 0.0057737380266189575\tvalid_loss :0.004253591876477003\n",
      "Adjusting learning rate of group 0 to 1.4795e-03.\n",
      "Adjusting learning rate of group 0 to 1.4795e-03.\n",
      "Epoch : 1433,\ttrain_loss : 0.005817919038236141\tvalid_loss :0.0042861877009272575\n",
      "Adjusting learning rate of group 0 to 1.4746e-03.\n",
      "Adjusting learning rate of group 0 to 1.4746e-03.\n",
      "Epoch : 1434,\ttrain_loss : 0.005759366322308779\tvalid_loss :0.004261044319719076\n",
      "Adjusting learning rate of group 0 to 1.4697e-03.\n",
      "Adjusting learning rate of group 0 to 1.4697e-03.\n",
      "Epoch : 1435,\ttrain_loss : 0.005792160984128714\tvalid_loss :0.004360563587397337\n",
      "Adjusting learning rate of group 0 to 1.4649e-03.\n",
      "Adjusting learning rate of group 0 to 1.4649e-03.\n",
      "Epoch : 1436,\ttrain_loss : 0.005789265502244234\tvalid_loss :0.004354857373982668\n",
      "Adjusting learning rate of group 0 to 1.4600e-03.\n",
      "Adjusting learning rate of group 0 to 1.4600e-03.\n",
      "Epoch : 1437,\ttrain_loss : 0.005784597713500261\tvalid_loss :0.004322224296629429\n",
      "Adjusting learning rate of group 0 to 1.4551e-03.\n",
      "Adjusting learning rate of group 0 to 1.4551e-03.\n",
      "Epoch : 1438,\ttrain_loss : 0.0057499888353049755\tvalid_loss :0.004299274645745754\n",
      "Adjusting learning rate of group 0 to 1.4503e-03.\n",
      "Adjusting learning rate of group 0 to 1.4503e-03.\n",
      "Epoch : 1439,\ttrain_loss : 0.005768103059381247\tvalid_loss :0.004402404185384512\n",
      "Adjusting learning rate of group 0 to 1.4455e-03.\n",
      "Adjusting learning rate of group 0 to 1.4455e-03.\n",
      "Epoch : 1440,\ttrain_loss : 0.005774572025984526\tvalid_loss :0.004443270619958639\n",
      "Adjusting learning rate of group 0 to 1.4406e-03.\n",
      "Adjusting learning rate of group 0 to 1.4406e-03.\n",
      "Epoch : 1441,\ttrain_loss : 0.005745636764913797\tvalid_loss :0.004263727925717831\n",
      "Adjusting learning rate of group 0 to 1.4358e-03.\n",
      "Adjusting learning rate of group 0 to 1.4358e-03.\n",
      "Epoch : 1442,\ttrain_loss : 0.00574162881821394\tvalid_loss :0.004316024482250214\n",
      "Adjusting learning rate of group 0 to 1.4310e-03.\n",
      "Adjusting learning rate of group 0 to 1.4310e-03.\n",
      "Epoch : 1443,\ttrain_loss : 0.005788435693830252\tvalid_loss :0.004361072089523077\n",
      "Adjusting learning rate of group 0 to 1.4262e-03.\n",
      "Adjusting learning rate of group 0 to 1.4262e-03.\n",
      "Epoch : 1444,\ttrain_loss : 0.005722277797758579\tvalid_loss :0.004296611063182354\n",
      "Adjusting learning rate of group 0 to 1.4214e-03.\n",
      "Adjusting learning rate of group 0 to 1.4214e-03.\n",
      "Epoch : 1445,\ttrain_loss : 0.0057805804535746574\tvalid_loss :0.004317931830883026\n",
      "Adjusting learning rate of group 0 to 1.4166e-03.\n",
      "Adjusting learning rate of group 0 to 1.4166e-03.\n",
      "Epoch : 1446,\ttrain_loss : 0.0057320138439536095\tvalid_loss :0.0043046558275818825\n",
      "Adjusting learning rate of group 0 to 1.4118e-03.\n",
      "Adjusting learning rate of group 0 to 1.4118e-03.\n",
      "Epoch : 1447,\ttrain_loss : 0.005795788019895554\tvalid_loss :0.004365402273833752\n",
      "Adjusting learning rate of group 0 to 1.4070e-03.\n",
      "Adjusting learning rate of group 0 to 1.4070e-03.\n",
      "Epoch : 1448,\ttrain_loss : 0.0057033454068005085\tvalid_loss :0.004237486515194178\n",
      "Adjusting learning rate of group 0 to 1.4022e-03.\n",
      "Adjusting learning rate of group 0 to 1.4022e-03.\n",
      "Epoch : 1449,\ttrain_loss : 0.005781862419098616\tvalid_loss :0.004322610329836607\n",
      "Adjusting learning rate of group 0 to 1.3974e-03.\n",
      "Adjusting learning rate of group 0 to 1.3974e-03.\n",
      "Epoch : 1450,\ttrain_loss : 0.005731649696826935\tvalid_loss :0.004253963939845562\n",
      "Adjusting learning rate of group 0 to 1.3927e-03.\n",
      "Adjusting learning rate of group 0 to 1.3927e-03.\n",
      "Epoch : 1451,\ttrain_loss : 0.005744718015193939\tvalid_loss :0.0042688581161201\n",
      "Adjusting learning rate of group 0 to 1.3879e-03.\n",
      "Adjusting learning rate of group 0 to 1.3879e-03.\n",
      "Epoch : 1452,\ttrain_loss : 0.00576822180300951\tvalid_loss :0.00432280357927084\n",
      "Adjusting learning rate of group 0 to 1.3831e-03.\n",
      "Adjusting learning rate of group 0 to 1.3831e-03.\n",
      "Epoch : 1453,\ttrain_loss : 0.005738400854170322\tvalid_loss :0.004194449167698622\n",
      "Adjusting learning rate of group 0 to 1.3784e-03.\n",
      "Adjusting learning rate of group 0 to 1.3784e-03.\n",
      "Epoch : 1454,\ttrain_loss : 0.005728612653911114\tvalid_loss :0.004208732396364212\n",
      "Adjusting learning rate of group 0 to 1.3737e-03.\n",
      "Adjusting learning rate of group 0 to 1.3737e-03.\n",
      "Epoch : 1455,\ttrain_loss : 0.005761546548455954\tvalid_loss :0.0042638457380235195\n",
      "Adjusting learning rate of group 0 to 1.3689e-03.\n",
      "Adjusting learning rate of group 0 to 1.3689e-03.\n",
      "Epoch : 1456,\ttrain_loss : 0.005728552583605051\tvalid_loss :0.004287251736968756\n",
      "Adjusting learning rate of group 0 to 1.3642e-03.\n",
      "Adjusting learning rate of group 0 to 1.3642e-03.\n",
      "Epoch : 1457,\ttrain_loss : 0.005766070447862148\tvalid_loss :0.004295927006751299\n",
      "Adjusting learning rate of group 0 to 1.3595e-03.\n",
      "Adjusting learning rate of group 0 to 1.3595e-03.\n",
      "Epoch : 1458,\ttrain_loss : 0.0057421112433075905\tvalid_loss :0.004214024171233177\n",
      "Adjusting learning rate of group 0 to 1.3548e-03.\n",
      "Adjusting learning rate of group 0 to 1.3548e-03.\n",
      "Epoch : 1459,\ttrain_loss : 0.0057057784870266914\tvalid_loss :0.004141144920140505\n",
      "Adjusting learning rate of group 0 to 1.3500e-03.\n",
      "Adjusting learning rate of group 0 to 1.3500e-03.\n",
      "Epoch : 1460,\ttrain_loss : 0.005734700243920088\tvalid_loss :0.004275325685739517\n",
      "Adjusting learning rate of group 0 to 1.3453e-03.\n",
      "Adjusting learning rate of group 0 to 1.3453e-03.\n",
      "Epoch : 1461,\ttrain_loss : 0.005746050272136927\tvalid_loss :0.004277291242033243\n",
      "Adjusting learning rate of group 0 to 1.3406e-03.\n",
      "Adjusting learning rate of group 0 to 1.3406e-03.\n",
      "Epoch : 1462,\ttrain_loss : 0.005749801639467478\tvalid_loss :0.004294131882488728\n",
      "Adjusting learning rate of group 0 to 1.3360e-03.\n",
      "Adjusting learning rate of group 0 to 1.3360e-03.\n",
      "Epoch : 1463,\ttrain_loss : 0.005709566175937653\tvalid_loss :0.004196581896394491\n",
      "Adjusting learning rate of group 0 to 1.3313e-03.\n",
      "Adjusting learning rate of group 0 to 1.3313e-03.\n",
      "Epoch : 1464,\ttrain_loss : 0.005740279797464609\tvalid_loss :0.004262877162545919\n",
      "Adjusting learning rate of group 0 to 1.3266e-03.\n",
      "Adjusting learning rate of group 0 to 1.3266e-03.\n",
      "Epoch : 1465,\ttrain_loss : 0.005727565381675959\tvalid_loss :0.004214404616504908\n",
      "Adjusting learning rate of group 0 to 1.3219e-03.\n",
      "Adjusting learning rate of group 0 to 1.3219e-03.\n",
      "Epoch : 1466,\ttrain_loss : 0.005736280232667923\tvalid_loss :0.004191387444734573\n",
      "Adjusting learning rate of group 0 to 1.3173e-03.\n",
      "Adjusting learning rate of group 0 to 1.3173e-03.\n",
      "Epoch : 1467,\ttrain_loss : 0.005742591805756092\tvalid_loss :0.004185443744063377\n",
      "Adjusting learning rate of group 0 to 1.3126e-03.\n",
      "Adjusting learning rate of group 0 to 1.3126e-03.\n",
      "Epoch : 1468,\ttrain_loss : 0.005722138099372387\tvalid_loss :0.004199234303086996\n",
      "Adjusting learning rate of group 0 to 1.3079e-03.\n",
      "Adjusting learning rate of group 0 to 1.3079e-03.\n",
      "Epoch : 1469,\ttrain_loss : 0.005721413530409336\tvalid_loss :0.004184960853308439\n",
      "Adjusting learning rate of group 0 to 1.3033e-03.\n",
      "Adjusting learning rate of group 0 to 1.3033e-03.\n",
      "Epoch : 1470,\ttrain_loss : 0.005737846251577139\tvalid_loss :0.004246240481734276\n",
      "Adjusting learning rate of group 0 to 1.2987e-03.\n",
      "Adjusting learning rate of group 0 to 1.2987e-03.\n",
      "Epoch : 1471,\ttrain_loss : 0.00572962686419487\tvalid_loss :0.004286651499569416\n",
      "Adjusting learning rate of group 0 to 1.2940e-03.\n",
      "Adjusting learning rate of group 0 to 1.2940e-03.\n",
      "Epoch : 1472,\ttrain_loss : 0.005723122041672468\tvalid_loss :0.00422501377761364\n",
      "Adjusting learning rate of group 0 to 1.2894e-03.\n",
      "Adjusting learning rate of group 0 to 1.2894e-03.\n",
      "Epoch : 1473,\ttrain_loss : 0.005746946670114994\tvalid_loss :0.0041960543021559715\n",
      "Adjusting learning rate of group 0 to 1.2848e-03.\n",
      "Adjusting learning rate of group 0 to 1.2848e-03.\n",
      "Epoch : 1474,\ttrain_loss : 0.0057115317322313786\tvalid_loss :0.004217258654534817\n",
      "Adjusting learning rate of group 0 to 1.2802e-03.\n",
      "Adjusting learning rate of group 0 to 1.2802e-03.\n",
      "Epoch : 1475,\ttrain_loss : 0.005743692163378\tvalid_loss :0.004217974841594696\n",
      "Adjusting learning rate of group 0 to 1.2756e-03.\n",
      "Adjusting learning rate of group 0 to 1.2756e-03.\n",
      "Epoch : 1476,\ttrain_loss : 0.00573227321729064\tvalid_loss :0.004159674048423767\n",
      "Adjusting learning rate of group 0 to 1.2710e-03.\n",
      "Adjusting learning rate of group 0 to 1.2710e-03.\n",
      "Epoch : 1477,\ttrain_loss : 0.005736198741942644\tvalid_loss :0.004262108821421862\n",
      "Adjusting learning rate of group 0 to 1.2664e-03.\n",
      "Adjusting learning rate of group 0 to 1.2664e-03.\n",
      "Epoch : 1478,\ttrain_loss : 0.005729575641453266\tvalid_loss :0.004134120419621468\n",
      "Adjusting learning rate of group 0 to 1.2618e-03.\n",
      "Adjusting learning rate of group 0 to 1.2618e-03.\n",
      "Epoch : 1479,\ttrain_loss : 0.005734648555517197\tvalid_loss :0.004229818005114794\n",
      "Adjusting learning rate of group 0 to 1.2572e-03.\n",
      "Adjusting learning rate of group 0 to 1.2572e-03.\n",
      "Epoch : 1480,\ttrain_loss : 0.005727552808821201\tvalid_loss :0.00416523078456521\n",
      "Adjusting learning rate of group 0 to 1.2527e-03.\n",
      "Adjusting learning rate of group 0 to 1.2527e-03.\n",
      "Epoch : 1481,\ttrain_loss : 0.005735649727284908\tvalid_loss :0.00416716281324625\n",
      "Adjusting learning rate of group 0 to 1.2481e-03.\n",
      "Adjusting learning rate of group 0 to 1.2481e-03.\n",
      "Epoch : 1482,\ttrain_loss : 0.005719316191971302\tvalid_loss :0.004091041162610054\n",
      "**********Valid loss decreased (0.004095 ==> 0.004091)**********\n",
      "Adjusting learning rate of group 0 to 1.2435e-03.\n",
      "Adjusting learning rate of group 0 to 1.2435e-03.\n",
      "Epoch : 1483,\ttrain_loss : 0.0057717375457286835\tvalid_loss :0.004183635581284761\n",
      "Adjusting learning rate of group 0 to 1.2390e-03.\n",
      "Adjusting learning rate of group 0 to 1.2390e-03.\n",
      "Epoch : 1484,\ttrain_loss : 0.0057765948586165905\tvalid_loss :0.004128548316657543\n",
      "Adjusting learning rate of group 0 to 1.2345e-03.\n",
      "Adjusting learning rate of group 0 to 1.2345e-03.\n",
      "Epoch : 1485,\ttrain_loss : 0.005770381074398756\tvalid_loss :0.004175205249339342\n",
      "Adjusting learning rate of group 0 to 1.2299e-03.\n",
      "Adjusting learning rate of group 0 to 1.2299e-03.\n",
      "Epoch : 1486,\ttrain_loss : 0.005796052515506744\tvalid_loss :0.004183441866189241\n",
      "Adjusting learning rate of group 0 to 1.2254e-03.\n",
      "Adjusting learning rate of group 0 to 1.2254e-03.\n",
      "Epoch : 1487,\ttrain_loss : 0.005806555040180683\tvalid_loss :0.004114223178476095\n",
      "Adjusting learning rate of group 0 to 1.2209e-03.\n",
      "Adjusting learning rate of group 0 to 1.2209e-03.\n",
      "Epoch : 1488,\ttrain_loss : 0.005806684028357267\tvalid_loss :0.004103768616914749\n",
      "Adjusting learning rate of group 0 to 1.2163e-03.\n",
      "Adjusting learning rate of group 0 to 1.2163e-03.\n",
      "Epoch : 1489,\ttrain_loss : 0.005862048827111721\tvalid_loss :0.004081845283508301\n",
      "**********Valid loss decreased (0.004091 ==> 0.004082)**********\n",
      "Adjusting learning rate of group 0 to 1.2118e-03.\n",
      "Adjusting learning rate of group 0 to 1.2118e-03.\n",
      "Epoch : 1490,\ttrain_loss : 0.005882799159735441\tvalid_loss :0.0041105374693870544\n",
      "Adjusting learning rate of group 0 to 1.2073e-03.\n",
      "Adjusting learning rate of group 0 to 1.2073e-03.\n",
      "Epoch : 1491,\ttrain_loss : 0.005894848145544529\tvalid_loss :0.004114815033972263\n",
      "Adjusting learning rate of group 0 to 1.2028e-03.\n",
      "Adjusting learning rate of group 0 to 1.2028e-03.\n",
      "Epoch : 1492,\ttrain_loss : 0.005958652589470148\tvalid_loss :0.004154502879828215\n",
      "Adjusting learning rate of group 0 to 1.1984e-03.\n",
      "Adjusting learning rate of group 0 to 1.1984e-03.\n",
      "Epoch : 1493,\ttrain_loss : 0.0060128578916192055\tvalid_loss :0.0042223441414535046\n",
      "Adjusting learning rate of group 0 to 1.1939e-03.\n",
      "Adjusting learning rate of group 0 to 1.1939e-03.\n",
      "Epoch : 1494,\ttrain_loss : 0.006020997650921345\tvalid_loss :0.004241283982992172\n",
      "Adjusting learning rate of group 0 to 1.1894e-03.\n",
      "Adjusting learning rate of group 0 to 1.1894e-03.\n",
      "Epoch : 1495,\ttrain_loss : 0.005990568548440933\tvalid_loss :0.004402433522045612\n",
      "Adjusting learning rate of group 0 to 1.1849e-03.\n",
      "Adjusting learning rate of group 0 to 1.1849e-03.\n",
      "Epoch : 1496,\ttrain_loss : 0.005998554639518261\tvalid_loss :0.004345174413174391\n",
      "Adjusting learning rate of group 0 to 1.1805e-03.\n",
      "Adjusting learning rate of group 0 to 1.1805e-03.\n",
      "Epoch : 1497,\ttrain_loss : 0.005973867140710354\tvalid_loss :0.0043830228969454765\n",
      "Adjusting learning rate of group 0 to 1.1760e-03.\n",
      "Adjusting learning rate of group 0 to 1.1760e-03.\n",
      "Epoch : 1498,\ttrain_loss : 0.005933030974119902\tvalid_loss :0.004352090414613485\n",
      "Adjusting learning rate of group 0 to 1.1716e-03.\n",
      "Adjusting learning rate of group 0 to 1.1716e-03.\n",
      "Epoch : 1499,\ttrain_loss : 0.0059171998873353004\tvalid_loss :0.004361691884696484\n",
      "Adjusting learning rate of group 0 to 1.1671e-03.\n",
      "Adjusting learning rate of group 0 to 1.1671e-03.\n",
      "Epoch : 1500,\ttrain_loss : 0.005899150390177965\tvalid_loss :0.004339911043643951\n",
      "Adjusting learning rate of group 0 to 1.1627e-03.\n",
      "Adjusting learning rate of group 0 to 1.1627e-03.\n",
      "Epoch : 1501,\ttrain_loss : 0.005911352578550577\tvalid_loss :0.004403220489621162\n",
      "Adjusting learning rate of group 0 to 1.1583e-03.\n",
      "Adjusting learning rate of group 0 to 1.1583e-03.\n",
      "Epoch : 1502,\ttrain_loss : 0.005925772245973349\tvalid_loss :0.004463288001716137\n",
      "Adjusting learning rate of group 0 to 1.1539e-03.\n",
      "Adjusting learning rate of group 0 to 1.1539e-03.\n",
      "Epoch : 1503,\ttrain_loss : 0.005907787475734949\tvalid_loss :0.004396508447825909\n",
      "Adjusting learning rate of group 0 to 1.1494e-03.\n",
      "Adjusting learning rate of group 0 to 1.1494e-03.\n",
      "Epoch : 1504,\ttrain_loss : 0.005869812797755003\tvalid_loss :0.004380852449685335\n",
      "Adjusting learning rate of group 0 to 1.1450e-03.\n",
      "Adjusting learning rate of group 0 to 1.1450e-03.\n",
      "Epoch : 1505,\ttrain_loss : 0.005878168623894453\tvalid_loss :0.004373734351247549\n",
      "Adjusting learning rate of group 0 to 1.1406e-03.\n",
      "Adjusting learning rate of group 0 to 1.1406e-03.\n",
      "Epoch : 1506,\ttrain_loss : 0.005854188930243254\tvalid_loss :0.004349635913968086\n",
      "Adjusting learning rate of group 0 to 1.1363e-03.\n",
      "Adjusting learning rate of group 0 to 1.1363e-03.\n",
      "Epoch : 1507,\ttrain_loss : 0.005826364737004042\tvalid_loss :0.004289077594876289\n",
      "Adjusting learning rate of group 0 to 1.1319e-03.\n",
      "Adjusting learning rate of group 0 to 1.1319e-03.\n",
      "Epoch : 1508,\ttrain_loss : 0.005828683264553547\tvalid_loss :0.004376779310405254\n",
      "Adjusting learning rate of group 0 to 1.1275e-03.\n",
      "Adjusting learning rate of group 0 to 1.1275e-03.\n",
      "Epoch : 1509,\ttrain_loss : 0.005834122188389301\tvalid_loss :0.0044367266818881035\n",
      "Adjusting learning rate of group 0 to 1.1231e-03.\n",
      "Adjusting learning rate of group 0 to 1.1231e-03.\n",
      "Epoch : 1510,\ttrain_loss : 0.005837738048285246\tvalid_loss :0.004288357682526112\n",
      "Adjusting learning rate of group 0 to 1.1188e-03.\n",
      "Adjusting learning rate of group 0 to 1.1188e-03.\n",
      "Epoch : 1511,\ttrain_loss : 0.005788623820990324\tvalid_loss :0.00433184253051877\n",
      "Adjusting learning rate of group 0 to 1.1144e-03.\n",
      "Adjusting learning rate of group 0 to 1.1144e-03.\n",
      "Epoch : 1512,\ttrain_loss : 0.0058184354566037655\tvalid_loss :0.004311845172196627\n",
      "Adjusting learning rate of group 0 to 1.1101e-03.\n",
      "Adjusting learning rate of group 0 to 1.1101e-03.\n",
      "Epoch : 1513,\ttrain_loss : 0.00579656520858407\tvalid_loss :0.004288508091121912\n",
      "Adjusting learning rate of group 0 to 1.1057e-03.\n",
      "Adjusting learning rate of group 0 to 1.1057e-03.\n",
      "Epoch : 1514,\ttrain_loss : 0.005804351530969143\tvalid_loss :0.004358022008091211\n",
      "Adjusting learning rate of group 0 to 1.1014e-03.\n",
      "Adjusting learning rate of group 0 to 1.1014e-03.\n",
      "Epoch : 1515,\ttrain_loss : 0.005821096245199442\tvalid_loss :0.004400074947625399\n",
      "Adjusting learning rate of group 0 to 1.0971e-03.\n",
      "Adjusting learning rate of group 0 to 1.0971e-03.\n",
      "Epoch : 1516,\ttrain_loss : 0.0058110738173127174\tvalid_loss :0.0043525611981749535\n",
      "Adjusting learning rate of group 0 to 1.0927e-03.\n",
      "Adjusting learning rate of group 0 to 1.0927e-03.\n",
      "Epoch : 1517,\ttrain_loss : 0.005799671169370413\tvalid_loss :0.004319988191127777\n",
      "Adjusting learning rate of group 0 to 1.0884e-03.\n",
      "Adjusting learning rate of group 0 to 1.0884e-03.\n",
      "Epoch : 1518,\ttrain_loss : 0.005794833414256573\tvalid_loss :0.004240256734192371\n",
      "Adjusting learning rate of group 0 to 1.0841e-03.\n",
      "Adjusting learning rate of group 0 to 1.0841e-03.\n",
      "Epoch : 1519,\ttrain_loss : 0.0057547991164028645\tvalid_loss :0.004302316345274448\n",
      "Adjusting learning rate of group 0 to 1.0798e-03.\n",
      "Adjusting learning rate of group 0 to 1.0798e-03.\n",
      "Epoch : 1520,\ttrain_loss : 0.005780543200671673\tvalid_loss :0.004249743185937405\n",
      "Adjusting learning rate of group 0 to 1.0755e-03.\n",
      "Adjusting learning rate of group 0 to 1.0755e-03.\n",
      "Epoch : 1521,\ttrain_loss : 0.005759055260568857\tvalid_loss :0.0042992462404072285\n",
      "Adjusting learning rate of group 0 to 1.0713e-03.\n",
      "Adjusting learning rate of group 0 to 1.0713e-03.\n",
      "Epoch : 1522,\ttrain_loss : 0.005783037282526493\tvalid_loss :0.004312505479902029\n",
      "Adjusting learning rate of group 0 to 1.0670e-03.\n",
      "Adjusting learning rate of group 0 to 1.0670e-03.\n",
      "Epoch : 1523,\ttrain_loss : 0.005777218844741583\tvalid_loss :0.00435133371502161\n",
      "Adjusting learning rate of group 0 to 1.0627e-03.\n",
      "Adjusting learning rate of group 0 to 1.0627e-03.\n",
      "Epoch : 1524,\ttrain_loss : 0.005777341313660145\tvalid_loss :0.004318524617701769\n",
      "Adjusting learning rate of group 0 to 1.0584e-03.\n",
      "Adjusting learning rate of group 0 to 1.0584e-03.\n",
      "Epoch : 1525,\ttrain_loss : 0.0057743629440665245\tvalid_loss :0.004293401725590229\n",
      "Adjusting learning rate of group 0 to 1.0542e-03.\n",
      "Adjusting learning rate of group 0 to 1.0542e-03.\n",
      "Epoch : 1526,\ttrain_loss : 0.005758908577263355\tvalid_loss :0.004299495369195938\n",
      "Adjusting learning rate of group 0 to 1.0499e-03.\n",
      "Adjusting learning rate of group 0 to 1.0499e-03.\n",
      "Epoch : 1527,\ttrain_loss : 0.005774569232016802\tvalid_loss :0.004343217238783836\n",
      "Adjusting learning rate of group 0 to 1.0457e-03.\n",
      "Adjusting learning rate of group 0 to 1.0457e-03.\n",
      "Epoch : 1528,\ttrain_loss : 0.0057760789059102535\tvalid_loss :0.004249980673193932\n",
      "Adjusting learning rate of group 0 to 1.0415e-03.\n",
      "Adjusting learning rate of group 0 to 1.0415e-03.\n",
      "Epoch : 1529,\ttrain_loss : 0.0057459729723632336\tvalid_loss :0.004255657084286213\n",
      "Adjusting learning rate of group 0 to 1.0373e-03.\n",
      "Adjusting learning rate of group 0 to 1.0373e-03.\n",
      "Epoch : 1530,\ttrain_loss : 0.005750429350882769\tvalid_loss :0.0042851585894823074\n",
      "Adjusting learning rate of group 0 to 1.0330e-03.\n",
      "Adjusting learning rate of group 0 to 1.0330e-03.\n",
      "Epoch : 1531,\ttrain_loss : 0.005752500146627426\tvalid_loss :0.004208340309560299\n",
      "Adjusting learning rate of group 0 to 1.0288e-03.\n",
      "Adjusting learning rate of group 0 to 1.0288e-03.\n",
      "Epoch : 1532,\ttrain_loss : 0.005730141419917345\tvalid_loss :0.004248309414833784\n",
      "Adjusting learning rate of group 0 to 1.0246e-03.\n",
      "Adjusting learning rate of group 0 to 1.0246e-03.\n",
      "Epoch : 1533,\ttrain_loss : 0.005740820430219173\tvalid_loss :0.00428525498136878\n",
      "Adjusting learning rate of group 0 to 1.0204e-03.\n",
      "Adjusting learning rate of group 0 to 1.0204e-03.\n",
      "Epoch : 1534,\ttrain_loss : 0.005754946731030941\tvalid_loss :0.004278430715203285\n",
      "Adjusting learning rate of group 0 to 1.0162e-03.\n",
      "Adjusting learning rate of group 0 to 1.0162e-03.\n",
      "Epoch : 1535,\ttrain_loss : 0.0057562002912163734\tvalid_loss :0.0042792800813913345\n",
      "Adjusting learning rate of group 0 to 1.0121e-03.\n",
      "Adjusting learning rate of group 0 to 1.0121e-03.\n",
      "Epoch : 1536,\ttrain_loss : 0.005737487226724625\tvalid_loss :0.0042528980411589146\n",
      "Adjusting learning rate of group 0 to 1.0079e-03.\n",
      "Adjusting learning rate of group 0 to 1.0079e-03.\n",
      "Epoch : 1537,\ttrain_loss : 0.005740858148783445\tvalid_loss :0.004223257768899202\n",
      "Adjusting learning rate of group 0 to 1.0037e-03.\n",
      "Adjusting learning rate of group 0 to 1.0037e-03.\n",
      "Epoch : 1538,\ttrain_loss : 0.00572923244908452\tvalid_loss :0.00426140334457159\n",
      "Adjusting learning rate of group 0 to 9.9956e-04.\n",
      "Adjusting learning rate of group 0 to 9.9956e-04.\n",
      "Epoch : 1539,\ttrain_loss : 0.0057390728034079075\tvalid_loss :0.004291835241019726\n",
      "Adjusting learning rate of group 0 to 9.9540e-04.\n",
      "Adjusting learning rate of group 0 to 9.9540e-04.\n",
      "Epoch : 1540,\ttrain_loss : 0.005759052466601133\tvalid_loss :0.004280261695384979\n",
      "Adjusting learning rate of group 0 to 9.9126e-04.\n",
      "Adjusting learning rate of group 0 to 9.9126e-04.\n",
      "Epoch : 1541,\ttrain_loss : 0.005747006740421057\tvalid_loss :0.004220929462462664\n",
      "Adjusting learning rate of group 0 to 9.8712e-04.\n",
      "Adjusting learning rate of group 0 to 9.8712e-04.\n",
      "Epoch : 1542,\ttrain_loss : 0.005725504364818335\tvalid_loss :0.004275099840015173\n",
      "Adjusting learning rate of group 0 to 9.8299e-04.\n",
      "Adjusting learning rate of group 0 to 9.8299e-04.\n",
      "Epoch : 1543,\ttrain_loss : 0.005721758119761944\tvalid_loss :0.004183843731880188\n",
      "Adjusting learning rate of group 0 to 9.7887e-04.\n",
      "Adjusting learning rate of group 0 to 9.7887e-04.\n",
      "Epoch : 1544,\ttrain_loss : 0.00570237310603261\tvalid_loss :0.004169813357293606\n",
      "Adjusting learning rate of group 0 to 9.7476e-04.\n",
      "Adjusting learning rate of group 0 to 9.7476e-04.\n",
      "Epoch : 1545,\ttrain_loss : 0.005702331196516752\tvalid_loss :0.004178996197879314\n",
      "Adjusting learning rate of group 0 to 9.7065e-04.\n",
      "Adjusting learning rate of group 0 to 9.7065e-04.\n",
      "Epoch : 1546,\ttrain_loss : 0.005705969873815775\tvalid_loss :0.004187032580375671\n",
      "Adjusting learning rate of group 0 to 9.6655e-04.\n",
      "Adjusting learning rate of group 0 to 9.6655e-04.\n",
      "Epoch : 1547,\ttrain_loss : 0.005716014187783003\tvalid_loss :0.004206958692520857\n",
      "Adjusting learning rate of group 0 to 9.6246e-04.\n",
      "Adjusting learning rate of group 0 to 9.6246e-04.\n",
      "Epoch : 1548,\ttrain_loss : 0.005717633757740259\tvalid_loss :0.004186918959021568\n",
      "Adjusting learning rate of group 0 to 9.5838e-04.\n",
      "Adjusting learning rate of group 0 to 9.5838e-04.\n",
      "Epoch : 1549,\ttrain_loss : 0.005715464241802692\tvalid_loss :0.004173257388174534\n",
      "Adjusting learning rate of group 0 to 9.5430e-04.\n",
      "Adjusting learning rate of group 0 to 9.5430e-04.\n",
      "Epoch : 1550,\ttrain_loss : 0.00570445042103529\tvalid_loss :0.0041648270562291145\n",
      "Adjusting learning rate of group 0 to 9.5023e-04.\n",
      "Adjusting learning rate of group 0 to 9.5023e-04.\n",
      "Epoch : 1551,\ttrain_loss : 0.005707558244466782\tvalid_loss :0.004158928524702787\n",
      "Adjusting learning rate of group 0 to 9.4617e-04.\n",
      "Adjusting learning rate of group 0 to 9.4617e-04.\n",
      "Epoch : 1552,\ttrain_loss : 0.005705368705093861\tvalid_loss :0.004155642352998257\n",
      "Adjusting learning rate of group 0 to 9.4211e-04.\n",
      "Adjusting learning rate of group 0 to 9.4211e-04.\n",
      "Epoch : 1553,\ttrain_loss : 0.005698334891349077\tvalid_loss :0.0041593932546675205\n",
      "Adjusting learning rate of group 0 to 9.3807e-04.\n",
      "Adjusting learning rate of group 0 to 9.3807e-04.\n",
      "Epoch : 1554,\ttrain_loss : 0.005707664880901575\tvalid_loss :0.004143060650676489\n",
      "Adjusting learning rate of group 0 to 9.3403e-04.\n",
      "Adjusting learning rate of group 0 to 9.3403e-04.\n",
      "Epoch : 1555,\ttrain_loss : 0.005703295581042767\tvalid_loss :0.004155749920755625\n",
      "Adjusting learning rate of group 0 to 9.3000e-04.\n",
      "Adjusting learning rate of group 0 to 9.3000e-04.\n",
      "Epoch : 1556,\ttrain_loss : 0.005702727008610964\tvalid_loss :0.004183913581073284\n",
      "Adjusting learning rate of group 0 to 9.2597e-04.\n",
      "Adjusting learning rate of group 0 to 9.2597e-04.\n",
      "Epoch : 1557,\ttrain_loss : 0.005740251392126083\tvalid_loss :0.004200242459774017\n",
      "Adjusting learning rate of group 0 to 9.2196e-04.\n",
      "Adjusting learning rate of group 0 to 9.2196e-04.\n",
      "Epoch : 1558,\ttrain_loss : 0.00574423698708415\tvalid_loss :0.0042306650429964066\n",
      "Adjusting learning rate of group 0 to 9.1795e-04.\n",
      "Adjusting learning rate of group 0 to 9.1795e-04.\n",
      "Epoch : 1559,\ttrain_loss : 0.005754046142101288\tvalid_loss :0.004158944357186556\n",
      "Adjusting learning rate of group 0 to 9.1395e-04.\n",
      "Adjusting learning rate of group 0 to 9.1395e-04.\n",
      "Epoch : 1560,\ttrain_loss : 0.005728166084736586\tvalid_loss :0.004189993254840374\n",
      "Adjusting learning rate of group 0 to 9.0995e-04.\n",
      "Adjusting learning rate of group 0 to 9.0995e-04.\n",
      "Epoch : 1561,\ttrain_loss : 0.005746383219957352\tvalid_loss :0.004195625428110361\n",
      "Adjusting learning rate of group 0 to 9.0597e-04.\n",
      "Adjusting learning rate of group 0 to 9.0597e-04.\n",
      "Epoch : 1562,\ttrain_loss : 0.005746176932007074\tvalid_loss :0.0041918507777154446\n",
      "Adjusting learning rate of group 0 to 9.0199e-04.\n",
      "Adjusting learning rate of group 0 to 9.0199e-04.\n",
      "Epoch : 1563,\ttrain_loss : 0.0057396357879042625\tvalid_loss :0.004160664975643158\n",
      "Adjusting learning rate of group 0 to 8.9802e-04.\n",
      "Adjusting learning rate of group 0 to 8.9802e-04.\n",
      "Epoch : 1564,\ttrain_loss : 0.005708680022507906\tvalid_loss :0.004155552946031094\n",
      "Adjusting learning rate of group 0 to 8.9405e-04.\n",
      "Adjusting learning rate of group 0 to 8.9405e-04.\n",
      "Epoch : 1565,\ttrain_loss : 0.005715850740671158\tvalid_loss :0.004157451447099447\n",
      "Adjusting learning rate of group 0 to 8.9010e-04.\n",
      "Adjusting learning rate of group 0 to 8.9010e-04.\n",
      "Epoch : 1566,\ttrain_loss : 0.005718086380511522\tvalid_loss :0.004164878744632006\n",
      "Adjusting learning rate of group 0 to 8.8615e-04.\n",
      "Adjusting learning rate of group 0 to 8.8615e-04.\n",
      "Epoch : 1567,\ttrain_loss : 0.005719567183405161\tvalid_loss :0.00417822040617466\n",
      "Adjusting learning rate of group 0 to 8.8221e-04.\n",
      "Adjusting learning rate of group 0 to 8.8221e-04.\n",
      "Epoch : 1568,\ttrain_loss : 0.005725058261305094\tvalid_loss :0.004178980831056833\n",
      "Adjusting learning rate of group 0 to 8.7828e-04.\n",
      "Adjusting learning rate of group 0 to 8.7828e-04.\n",
      "Epoch : 1569,\ttrain_loss : 0.0057173119857907295\tvalid_loss :0.0041626207530498505\n",
      "Adjusting learning rate of group 0 to 8.7435e-04.\n",
      "Adjusting learning rate of group 0 to 8.7435e-04.\n",
      "Epoch : 1570,\ttrain_loss : 0.005699783097952604\tvalid_loss :0.004159135743975639\n",
      "Adjusting learning rate of group 0 to 8.7044e-04.\n",
      "Adjusting learning rate of group 0 to 8.7044e-04.\n",
      "Epoch : 1571,\ttrain_loss : 0.005723145790398121\tvalid_loss :0.004183440003544092\n",
      "Adjusting learning rate of group 0 to 8.6653e-04.\n",
      "Adjusting learning rate of group 0 to 8.6653e-04.\n",
      "Epoch : 1572,\ttrain_loss : 0.005724871996790171\tvalid_loss :0.004130668472498655\n",
      "Adjusting learning rate of group 0 to 8.6263e-04.\n",
      "Adjusting learning rate of group 0 to 8.6263e-04.\n",
      "Epoch : 1573,\ttrain_loss : 0.005694751627743244\tvalid_loss :0.004133366979658604\n",
      "Adjusting learning rate of group 0 to 8.5873e-04.\n",
      "Adjusting learning rate of group 0 to 8.5873e-04.\n",
      "Epoch : 1574,\ttrain_loss : 0.005694741848856211\tvalid_loss :0.004138070624321699\n",
      "Adjusting learning rate of group 0 to 8.5485e-04.\n",
      "Adjusting learning rate of group 0 to 8.5485e-04.\n",
      "Epoch : 1575,\ttrain_loss : 0.0056958612985908985\tvalid_loss :0.004134218208491802\n",
      "Adjusting learning rate of group 0 to 8.5097e-04.\n",
      "Adjusting learning rate of group 0 to 8.5097e-04.\n",
      "Epoch : 1576,\ttrain_loss : 0.005668864119797945\tvalid_loss :0.004125991836190224\n",
      "Adjusting learning rate of group 0 to 8.4710e-04.\n",
      "Adjusting learning rate of group 0 to 8.4710e-04.\n",
      "Epoch : 1577,\ttrain_loss : 0.005669808946549892\tvalid_loss :0.00411976408213377\n",
      "Adjusting learning rate of group 0 to 8.4323e-04.\n",
      "Adjusting learning rate of group 0 to 8.4323e-04.\n",
      "Epoch : 1578,\ttrain_loss : 0.005671332590281963\tvalid_loss :0.004113183356821537\n",
      "Adjusting learning rate of group 0 to 8.3938e-04.\n",
      "Adjusting learning rate of group 0 to 8.3938e-04.\n",
      "Epoch : 1579,\ttrain_loss : 0.005677810404449701\tvalid_loss :0.004134454298764467\n",
      "Adjusting learning rate of group 0 to 8.3553e-04.\n",
      "Adjusting learning rate of group 0 to 8.3553e-04.\n",
      "Epoch : 1580,\ttrain_loss : 0.005695882719010115\tvalid_loss :0.004141225945204496\n",
      "Adjusting learning rate of group 0 to 8.3169e-04.\n",
      "Adjusting learning rate of group 0 to 8.3169e-04.\n",
      "Epoch : 1581,\ttrain_loss : 0.0056980992667376995\tvalid_loss :0.004126422107219696\n",
      "Adjusting learning rate of group 0 to 8.2786e-04.\n",
      "Adjusting learning rate of group 0 to 8.2786e-04.\n",
      "Epoch : 1582,\ttrain_loss : 0.005692292936146259\tvalid_loss :0.004119470715522766\n",
      "Adjusting learning rate of group 0 to 8.2404e-04.\n",
      "Adjusting learning rate of group 0 to 8.2404e-04.\n",
      "Epoch : 1583,\ttrain_loss : 0.005689395125955343\tvalid_loss :0.0041204472072422504\n",
      "Adjusting learning rate of group 0 to 8.2022e-04.\n",
      "Adjusting learning rate of group 0 to 8.2022e-04.\n",
      "Epoch : 1584,\ttrain_loss : 0.0056886519305408\tvalid_loss :0.00413089245557785\n",
      "Adjusting learning rate of group 0 to 8.1641e-04.\n",
      "Adjusting learning rate of group 0 to 8.1641e-04.\n",
      "Epoch : 1585,\ttrain_loss : 0.00569172902032733\tvalid_loss :0.0041374992579221725\n",
      "Adjusting learning rate of group 0 to 8.1261e-04.\n",
      "Adjusting learning rate of group 0 to 8.1261e-04.\n",
      "Epoch : 1586,\ttrain_loss : 0.00568137364462018\tvalid_loss :0.004175736103206873\n",
      "Adjusting learning rate of group 0 to 8.0882e-04.\n",
      "Adjusting learning rate of group 0 to 8.0882e-04.\n",
      "Epoch : 1587,\ttrain_loss : 0.005676005966961384\tvalid_loss :0.004134926479309797\n",
      "Adjusting learning rate of group 0 to 8.0504e-04.\n",
      "Adjusting learning rate of group 0 to 8.0504e-04.\n",
      "Epoch : 1588,\ttrain_loss : 0.005667786113917828\tvalid_loss :0.0041268919594585896\n",
      "Adjusting learning rate of group 0 to 8.0126e-04.\n",
      "Adjusting learning rate of group 0 to 8.0126e-04.\n",
      "Epoch : 1589,\ttrain_loss : 0.0056786746717989445\tvalid_loss :0.004124335944652557\n",
      "Adjusting learning rate of group 0 to 7.9749e-04.\n",
      "Adjusting learning rate of group 0 to 7.9749e-04.\n",
      "Epoch : 1590,\ttrain_loss : 0.005689886398613453\tvalid_loss :0.00411249278113246\n",
      "Adjusting learning rate of group 0 to 7.9373e-04.\n",
      "Adjusting learning rate of group 0 to 7.9373e-04.\n",
      "Epoch : 1591,\ttrain_loss : 0.00568777322769165\tvalid_loss :0.004128739703446627\n",
      "Adjusting learning rate of group 0 to 7.8998e-04.\n",
      "Adjusting learning rate of group 0 to 7.8998e-04.\n",
      "Epoch : 1592,\ttrain_loss : 0.005680226255208254\tvalid_loss :0.004134481307119131\n",
      "Adjusting learning rate of group 0 to 7.8623e-04.\n",
      "Adjusting learning rate of group 0 to 7.8623e-04.\n",
      "Epoch : 1593,\ttrain_loss : 0.00568727869540453\tvalid_loss :0.0041433642618358135\n",
      "Adjusting learning rate of group 0 to 7.8250e-04.\n",
      "Adjusting learning rate of group 0 to 7.8250e-04.\n",
      "Epoch : 1594,\ttrain_loss : 0.0056903669610619545\tvalid_loss :0.004146689083427191\n",
      "Adjusting learning rate of group 0 to 7.7877e-04.\n",
      "Adjusting learning rate of group 0 to 7.7877e-04.\n",
      "Epoch : 1595,\ttrain_loss : 0.0056825741194188595\tvalid_loss :0.004148482345044613\n",
      "Adjusting learning rate of group 0 to 7.7505e-04.\n",
      "Adjusting learning rate of group 0 to 7.7505e-04.\n",
      "Epoch : 1596,\ttrain_loss : 0.005682873073965311\tvalid_loss :0.004146857187151909\n",
      "Adjusting learning rate of group 0 to 7.7133e-04.\n",
      "Adjusting learning rate of group 0 to 7.7133e-04.\n",
      "Epoch : 1597,\ttrain_loss : 0.00569172902032733\tvalid_loss :0.004136546049267054\n",
      "Adjusting learning rate of group 0 to 7.6763e-04.\n",
      "Adjusting learning rate of group 0 to 7.6763e-04.\n",
      "Epoch : 1598,\ttrain_loss : 0.005670604296028614\tvalid_loss :0.004127797205001116\n",
      "Adjusting learning rate of group 0 to 7.6393e-04.\n",
      "Adjusting learning rate of group 0 to 7.6393e-04.\n",
      "Epoch : 1599,\ttrain_loss : 0.005673428066074848\tvalid_loss :0.0041640689596533775\n",
      "Adjusting learning rate of group 0 to 7.6024e-04.\n",
      "Adjusting learning rate of group 0 to 7.6024e-04.\n",
      "Epoch : 1600,\ttrain_loss : 0.005697245709598064\tvalid_loss :0.004117872100323439\n",
      "Adjusting learning rate of group 0 to 7.5656e-04.\n",
      "Adjusting learning rate of group 0 to 7.5656e-04.\n",
      "Epoch : 1601,\ttrain_loss : 0.005666365381330252\tvalid_loss :0.004126421175897121\n",
      "Adjusting learning rate of group 0 to 7.5289e-04.\n",
      "Adjusting learning rate of group 0 to 7.5289e-04.\n",
      "Epoch : 1602,\ttrain_loss : 0.005679658614099026\tvalid_loss :0.004141413141041994\n",
      "Adjusting learning rate of group 0 to 7.4922e-04.\n",
      "Adjusting learning rate of group 0 to 7.4922e-04.\n",
      "Epoch : 1603,\ttrain_loss : 0.005679312627762556\tvalid_loss :0.004126758314669132\n",
      "Adjusting learning rate of group 0 to 7.4557e-04.\n",
      "Adjusting learning rate of group 0 to 7.4557e-04.\n",
      "Epoch : 1604,\ttrain_loss : 0.005672555882483721\tvalid_loss :0.004147320985794067\n",
      "Adjusting learning rate of group 0 to 7.4192e-04.\n",
      "Adjusting learning rate of group 0 to 7.4192e-04.\n",
      "Epoch : 1605,\ttrain_loss : 0.005674178712069988\tvalid_loss :0.004132605157792568\n",
      "Adjusting learning rate of group 0 to 7.3828e-04.\n",
      "Adjusting learning rate of group 0 to 7.3828e-04.\n",
      "Epoch : 1606,\ttrain_loss : 0.005675647873431444\tvalid_loss :0.004126240033656359\n",
      "Adjusting learning rate of group 0 to 7.3464e-04.\n",
      "Adjusting learning rate of group 0 to 7.3464e-04.\n",
      "Epoch : 1607,\ttrain_loss : 0.0056736352853477\tvalid_loss :0.004139985889196396\n",
      "Adjusting learning rate of group 0 to 7.3102e-04.\n",
      "Adjusting learning rate of group 0 to 7.3102e-04.\n",
      "Epoch : 1608,\ttrain_loss : 0.005675750318914652\tvalid_loss :0.004124714992940426\n",
      "Adjusting learning rate of group 0 to 7.2740e-04.\n",
      "Adjusting learning rate of group 0 to 7.2740e-04.\n",
      "Epoch : 1609,\ttrain_loss : 0.005674310959875584\tvalid_loss :0.004117181524634361\n",
      "Adjusting learning rate of group 0 to 7.2379e-04.\n",
      "Adjusting learning rate of group 0 to 7.2379e-04.\n",
      "Epoch : 1610,\ttrain_loss : 0.005654245149344206\tvalid_loss :0.0041146534495055676\n",
      "Adjusting learning rate of group 0 to 7.2019e-04.\n",
      "Adjusting learning rate of group 0 to 7.2019e-04.\n",
      "Epoch : 1611,\ttrain_loss : 0.005659300833940506\tvalid_loss :0.00412472989410162\n",
      "Adjusting learning rate of group 0 to 7.1660e-04.\n",
      "Adjusting learning rate of group 0 to 7.1660e-04.\n",
      "Epoch : 1612,\ttrain_loss : 0.005668566096574068\tvalid_loss :0.004119609482586384\n",
      "Adjusting learning rate of group 0 to 7.1301e-04.\n",
      "Adjusting learning rate of group 0 to 7.1301e-04.\n",
      "Epoch : 1613,\ttrain_loss : 0.005667562130838633\tvalid_loss :0.004109957721084356\n",
      "Adjusting learning rate of group 0 to 7.0944e-04.\n",
      "Adjusting learning rate of group 0 to 7.0944e-04.\n",
      "Epoch : 1614,\ttrain_loss : 0.005657743196934462\tvalid_loss :0.004115420859307051\n",
      "Adjusting learning rate of group 0 to 7.0587e-04.\n",
      "Adjusting learning rate of group 0 to 7.0587e-04.\n",
      "Epoch : 1615,\ttrain_loss : 0.005656910594552755\tvalid_loss :0.00411584135144949\n",
      "Adjusting learning rate of group 0 to 7.0231e-04.\n",
      "Adjusting learning rate of group 0 to 7.0231e-04.\n",
      "Epoch : 1616,\ttrain_loss : 0.005655449349433184\tvalid_loss :0.004107361193746328\n",
      "Adjusting learning rate of group 0 to 6.9876e-04.\n",
      "Adjusting learning rate of group 0 to 6.9876e-04.\n",
      "Epoch : 1617,\ttrain_loss : 0.005649537779390812\tvalid_loss :0.0041057150810956955\n",
      "Adjusting learning rate of group 0 to 6.9521e-04.\n",
      "Adjusting learning rate of group 0 to 6.9521e-04.\n",
      "Epoch : 1618,\ttrain_loss : 0.005648608319461346\tvalid_loss :0.004107069224119186\n",
      "Adjusting learning rate of group 0 to 6.9168e-04.\n",
      "Adjusting learning rate of group 0 to 6.9168e-04.\n",
      "Epoch : 1619,\ttrain_loss : 0.005653644911944866\tvalid_loss :0.0041100578382611275\n",
      "Adjusting learning rate of group 0 to 6.8815e-04.\n",
      "Adjusting learning rate of group 0 to 6.8815e-04.\n",
      "Epoch : 1620,\ttrain_loss : 0.0056535229086875916\tvalid_loss :0.004108186345547438\n",
      "Adjusting learning rate of group 0 to 6.8463e-04.\n",
      "Adjusting learning rate of group 0 to 6.8463e-04.\n",
      "Epoch : 1621,\ttrain_loss : 0.005652582738548517\tvalid_loss :0.004103291314095259\n",
      "Adjusting learning rate of group 0 to 6.8112e-04.\n",
      "Adjusting learning rate of group 0 to 6.8112e-04.\n",
      "Epoch : 1622,\ttrain_loss : 0.005644154269248247\tvalid_loss :0.00411548838019371\n",
      "Adjusting learning rate of group 0 to 6.7762e-04.\n",
      "Adjusting learning rate of group 0 to 6.7762e-04.\n",
      "Epoch : 1623,\ttrain_loss : 0.005659156478941441\tvalid_loss :0.004110623151063919\n",
      "Adjusting learning rate of group 0 to 6.7412e-04.\n",
      "Adjusting learning rate of group 0 to 6.7412e-04.\n",
      "Epoch : 1624,\ttrain_loss : 0.005662198178470135\tvalid_loss :0.004107649903744459\n",
      "Adjusting learning rate of group 0 to 6.7063e-04.\n",
      "Adjusting learning rate of group 0 to 6.7063e-04.\n",
      "Epoch : 1625,\ttrain_loss : 0.005658168811351061\tvalid_loss :0.00410307664424181\n",
      "Adjusting learning rate of group 0 to 6.6716e-04.\n",
      "Adjusting learning rate of group 0 to 6.6716e-04.\n",
      "Epoch : 1626,\ttrain_loss : 0.005641280673444271\tvalid_loss :0.004105713218450546\n",
      "Adjusting learning rate of group 0 to 6.6369e-04.\n",
      "Adjusting learning rate of group 0 to 6.6369e-04.\n",
      "Epoch : 1627,\ttrain_loss : 0.005653605330735445\tvalid_loss :0.004097792319953442\n",
      "Adjusting learning rate of group 0 to 6.6022e-04.\n",
      "Adjusting learning rate of group 0 to 6.6022e-04.\n",
      "Epoch : 1628,\ttrain_loss : 0.005641295574605465\tvalid_loss :0.004109961446374655\n",
      "Adjusting learning rate of group 0 to 6.5677e-04.\n",
      "Adjusting learning rate of group 0 to 6.5677e-04.\n",
      "Epoch : 1629,\ttrain_loss : 0.005646210629492998\tvalid_loss :0.004114575684070587\n",
      "Adjusting learning rate of group 0 to 6.5333e-04.\n",
      "Adjusting learning rate of group 0 to 6.5333e-04.\n",
      "Epoch : 1630,\ttrain_loss : 0.005649331025779247\tvalid_loss :0.0041112639009952545\n",
      "Adjusting learning rate of group 0 to 6.4989e-04.\n",
      "Adjusting learning rate of group 0 to 6.4989e-04.\n",
      "Epoch : 1631,\ttrain_loss : 0.005645151250064373\tvalid_loss :0.00410554651170969\n",
      "Adjusting learning rate of group 0 to 6.4646e-04.\n",
      "Adjusting learning rate of group 0 to 6.4646e-04.\n",
      "Epoch : 1632,\ttrain_loss : 0.005642423871904612\tvalid_loss :0.004108926746994257\n",
      "Adjusting learning rate of group 0 to 6.4304e-04.\n",
      "Adjusting learning rate of group 0 to 6.4304e-04.\n",
      "Epoch : 1633,\ttrain_loss : 0.005650897044688463\tvalid_loss :0.0041066724807024\n",
      "Adjusting learning rate of group 0 to 6.3963e-04.\n",
      "Adjusting learning rate of group 0 to 6.3963e-04.\n",
      "Epoch : 1634,\ttrain_loss : 0.005651931278407574\tvalid_loss :0.004108842927962542\n",
      "Adjusting learning rate of group 0 to 6.3622e-04.\n",
      "Adjusting learning rate of group 0 to 6.3622e-04.\n",
      "Epoch : 1635,\ttrain_loss : 0.005655602551996708\tvalid_loss :0.004098350647836924\n",
      "Adjusting learning rate of group 0 to 6.3283e-04.\n",
      "Adjusting learning rate of group 0 to 6.3283e-04.\n",
      "Epoch : 1636,\ttrain_loss : 0.00563978822901845\tvalid_loss :0.004097239580005407\n",
      "Adjusting learning rate of group 0 to 6.2944e-04.\n",
      "Adjusting learning rate of group 0 to 6.2944e-04.\n",
      "Epoch : 1637,\ttrain_loss : 0.005638130474835634\tvalid_loss :0.00410070177167654\n",
      "Adjusting learning rate of group 0 to 6.2606e-04.\n",
      "Adjusting learning rate of group 0 to 6.2606e-04.\n",
      "Epoch : 1638,\ttrain_loss : 0.005642755422741175\tvalid_loss :0.004101446829736233\n",
      "Adjusting learning rate of group 0 to 6.2269e-04.\n",
      "Adjusting learning rate of group 0 to 6.2269e-04.\n",
      "Epoch : 1639,\ttrain_loss : 0.005639190785586834\tvalid_loss :0.004099912475794554\n",
      "Adjusting learning rate of group 0 to 6.1933e-04.\n",
      "Adjusting learning rate of group 0 to 6.1933e-04.\n",
      "Epoch : 1640,\ttrain_loss : 0.005636679939925671\tvalid_loss :0.004102515988051891\n",
      "Adjusting learning rate of group 0 to 6.1597e-04.\n",
      "Adjusting learning rate of group 0 to 6.1597e-04.\n",
      "Epoch : 1641,\ttrain_loss : 0.005645277909934521\tvalid_loss :0.004114524926990271\n",
      "Adjusting learning rate of group 0 to 6.1263e-04.\n",
      "Adjusting learning rate of group 0 to 6.1263e-04.\n",
      "Epoch : 1642,\ttrain_loss : 0.005650916136801243\tvalid_loss :0.004098782315850258\n",
      "Adjusting learning rate of group 0 to 6.0929e-04.\n",
      "Adjusting learning rate of group 0 to 6.0929e-04.\n",
      "Epoch : 1643,\ttrain_loss : 0.005632895510643721\tvalid_loss :0.004095328506082296\n",
      "Adjusting learning rate of group 0 to 6.0596e-04.\n",
      "Adjusting learning rate of group 0 to 6.0596e-04.\n",
      "Epoch : 1644,\ttrain_loss : 0.005634107626974583\tvalid_loss :0.004107693675905466\n",
      "Adjusting learning rate of group 0 to 6.0264e-04.\n",
      "Adjusting learning rate of group 0 to 6.0264e-04.\n",
      "Epoch : 1645,\ttrain_loss : 0.0056432378478348255\tvalid_loss :0.004113023169338703\n",
      "Adjusting learning rate of group 0 to 5.9933e-04.\n",
      "Adjusting learning rate of group 0 to 5.9933e-04.\n",
      "Epoch : 1646,\ttrain_loss : 0.005634476896375418\tvalid_loss :0.004101076163351536\n",
      "Adjusting learning rate of group 0 to 5.9602e-04.\n",
      "Adjusting learning rate of group 0 to 5.9602e-04.\n",
      "Epoch : 1647,\ttrain_loss : 0.005640134215354919\tvalid_loss :0.004111026413738728\n",
      "Adjusting learning rate of group 0 to 5.9273e-04.\n",
      "Adjusting learning rate of group 0 to 5.9273e-04.\n",
      "Epoch : 1648,\ttrain_loss : 0.005649833474308252\tvalid_loss :0.004107659216970205\n",
      "Adjusting learning rate of group 0 to 5.8944e-04.\n",
      "Adjusting learning rate of group 0 to 5.8944e-04.\n",
      "Epoch : 1649,\ttrain_loss : 0.005636286921799183\tvalid_loss :0.004107325803488493\n",
      "Adjusting learning rate of group 0 to 5.8616e-04.\n",
      "Adjusting learning rate of group 0 to 5.8616e-04.\n",
      "Epoch : 1650,\ttrain_loss : 0.005643143318593502\tvalid_loss :0.004102356731891632\n",
      "Adjusting learning rate of group 0 to 5.8289e-04.\n",
      "Adjusting learning rate of group 0 to 5.8289e-04.\n",
      "Epoch : 1651,\ttrain_loss : 0.005636754911392927\tvalid_loss :0.0041094208136200905\n",
      "Adjusting learning rate of group 0 to 5.7963e-04.\n",
      "Adjusting learning rate of group 0 to 5.7963e-04.\n",
      "Epoch : 1652,\ttrain_loss : 0.00563852908089757\tvalid_loss :0.004094342235475779\n",
      "Adjusting learning rate of group 0 to 5.7637e-04.\n",
      "Adjusting learning rate of group 0 to 5.7637e-04.\n",
      "Epoch : 1653,\ttrain_loss : 0.005629606544971466\tvalid_loss :0.004093628376722336\n",
      "Adjusting learning rate of group 0 to 5.7313e-04.\n",
      "Adjusting learning rate of group 0 to 5.7313e-04.\n",
      "Epoch : 1654,\ttrain_loss : 0.00563498679548502\tvalid_loss :0.0040969690307974815\n",
      "Adjusting learning rate of group 0 to 5.6989e-04.\n",
      "Adjusting learning rate of group 0 to 5.6989e-04.\n",
      "Epoch : 1655,\ttrain_loss : 0.005628649611026049\tvalid_loss :0.004097519442439079\n",
      "Adjusting learning rate of group 0 to 5.6667e-04.\n",
      "Adjusting learning rate of group 0 to 5.6667e-04.\n",
      "Epoch : 1656,\ttrain_loss : 0.005628096405416727\tvalid_loss :0.004096101503819227\n",
      "Adjusting learning rate of group 0 to 5.6345e-04.\n",
      "Adjusting learning rate of group 0 to 5.6345e-04.\n",
      "Epoch : 1657,\ttrain_loss : 0.005632881540805101\tvalid_loss :0.004101281054317951\n",
      "Adjusting learning rate of group 0 to 5.6023e-04.\n",
      "Adjusting learning rate of group 0 to 5.6023e-04.\n",
      "Epoch : 1658,\ttrain_loss : 0.005635949783027172\tvalid_loss :0.004096158314496279\n",
      "Adjusting learning rate of group 0 to 5.5703e-04.\n",
      "Adjusting learning rate of group 0 to 5.5703e-04.\n",
      "Epoch : 1659,\ttrain_loss : 0.005629499442875385\tvalid_loss :0.004088062793016434\n",
      "Adjusting learning rate of group 0 to 5.5384e-04.\n",
      "Adjusting learning rate of group 0 to 5.5384e-04.\n",
      "Epoch : 1660,\ttrain_loss : 0.005632938351482153\tvalid_loss :0.004096096381545067\n",
      "Adjusting learning rate of group 0 to 5.5065e-04.\n",
      "Adjusting learning rate of group 0 to 5.5065e-04.\n",
      "Epoch : 1661,\ttrain_loss : 0.005634315777570009\tvalid_loss :0.004088535439223051\n",
      "Adjusting learning rate of group 0 to 5.4747e-04.\n",
      "Adjusting learning rate of group 0 to 5.4747e-04.\n",
      "Epoch : 1662,\ttrain_loss : 0.005630081053823233\tvalid_loss :0.004101322032511234\n",
      "Adjusting learning rate of group 0 to 5.4431e-04.\n",
      "Adjusting learning rate of group 0 to 5.4431e-04.\n",
      "Epoch : 1663,\ttrain_loss : 0.00563429668545723\tvalid_loss :0.004092567600309849\n",
      "Adjusting learning rate of group 0 to 5.4115e-04.\n",
      "Adjusting learning rate of group 0 to 5.4115e-04.\n",
      "Epoch : 1664,\ttrain_loss : 0.0056306845508515835\tvalid_loss :0.004082133993506432\n",
      "Adjusting learning rate of group 0 to 5.3799e-04.\n",
      "Adjusting learning rate of group 0 to 5.3799e-04.\n",
      "Epoch : 1665,\ttrain_loss : 0.005631972104310989\tvalid_loss :0.004093733616173267\n",
      "Adjusting learning rate of group 0 to 5.3485e-04.\n",
      "Adjusting learning rate of group 0 to 5.3485e-04.\n",
      "Epoch : 1666,\ttrain_loss : 0.005629645194858313\tvalid_loss :0.004090266767889261\n",
      "Adjusting learning rate of group 0 to 5.3172e-04.\n",
      "Adjusting learning rate of group 0 to 5.3172e-04.\n",
      "Epoch : 1667,\ttrain_loss : 0.005627635400742292\tvalid_loss :0.004102184437215328\n",
      "Adjusting learning rate of group 0 to 5.2859e-04.\n",
      "Adjusting learning rate of group 0 to 5.2859e-04.\n",
      "Epoch : 1668,\ttrain_loss : 0.005629436112940311\tvalid_loss :0.0040874420665204525\n",
      "Adjusting learning rate of group 0 to 5.2547e-04.\n",
      "Adjusting learning rate of group 0 to 5.2547e-04.\n",
      "Epoch : 1669,\ttrain_loss : 0.005629855673760176\tvalid_loss :0.004086707718670368\n",
      "Adjusting learning rate of group 0 to 5.2237e-04.\n",
      "Adjusting learning rate of group 0 to 5.2237e-04.\n",
      "Epoch : 1670,\ttrain_loss : 0.00562632130458951\tvalid_loss :0.004145834129303694\n",
      "Adjusting learning rate of group 0 to 5.1926e-04.\n",
      "Adjusting learning rate of group 0 to 5.1926e-04.\n",
      "Epoch : 1671,\ttrain_loss : 0.005655684974044561\tvalid_loss :0.004109424538910389\n",
      "Adjusting learning rate of group 0 to 5.1617e-04.\n",
      "Adjusting learning rate of group 0 to 5.1617e-04.\n",
      "Epoch : 1672,\ttrain_loss : 0.0056430017575621605\tvalid_loss :0.004152676090598106\n",
      "Adjusting learning rate of group 0 to 5.1309e-04.\n",
      "Adjusting learning rate of group 0 to 5.1309e-04.\n",
      "Epoch : 1673,\ttrain_loss : 0.005666661541908979\tvalid_loss :0.00412331148982048\n",
      "Adjusting learning rate of group 0 to 5.1002e-04.\n",
      "Adjusting learning rate of group 0 to 5.1002e-04.\n",
      "Epoch : 1674,\ttrain_loss : 0.005676838103681803\tvalid_loss :0.004087177570909262\n",
      "Adjusting learning rate of group 0 to 5.0695e-04.\n",
      "Adjusting learning rate of group 0 to 5.0695e-04.\n",
      "Epoch : 1675,\ttrain_loss : 0.005678048823028803\tvalid_loss :0.004069859627634287\n",
      "**********Valid loss decreased (0.004082 ==> 0.004070)**********\n",
      "Adjusting learning rate of group 0 to 5.0389e-04.\n",
      "Adjusting learning rate of group 0 to 5.0389e-04.\n",
      "Epoch : 1676,\ttrain_loss : 0.005665266420692205\tvalid_loss :0.004071544855833054\n",
      "Adjusting learning rate of group 0 to 5.0084e-04.\n",
      "Adjusting learning rate of group 0 to 5.0084e-04.\n",
      "Epoch : 1677,\ttrain_loss : 0.005655789282172918\tvalid_loss :0.0040670461021363735\n",
      "**********Valid loss decreased (0.004070 ==> 0.004067)**********\n",
      "Adjusting learning rate of group 0 to 4.9780e-04.\n",
      "Adjusting learning rate of group 0 to 4.9780e-04.\n",
      "Epoch : 1678,\ttrain_loss : 0.005648870021104813\tvalid_loss :0.004071554634720087\n",
      "Adjusting learning rate of group 0 to 4.9477e-04.\n",
      "Adjusting learning rate of group 0 to 4.9477e-04.\n",
      "Epoch : 1679,\ttrain_loss : 0.005634435918182135\tvalid_loss :0.004071233794093132\n",
      "Adjusting learning rate of group 0 to 4.9175e-04.\n",
      "Adjusting learning rate of group 0 to 4.9175e-04.\n",
      "Epoch : 1680,\ttrain_loss : 0.005638567265123129\tvalid_loss :0.004083416424691677\n",
      "Adjusting learning rate of group 0 to 4.8874e-04.\n",
      "Adjusting learning rate of group 0 to 4.8874e-04.\n",
      "Epoch : 1681,\ttrain_loss : 0.005637588910758495\tvalid_loss :0.0040857186540961266\n",
      "Adjusting learning rate of group 0 to 4.8573e-04.\n",
      "Adjusting learning rate of group 0 to 4.8573e-04.\n",
      "Epoch : 1682,\ttrain_loss : 0.005625979509204626\tvalid_loss :0.004082425497472286\n",
      "Adjusting learning rate of group 0 to 4.8273e-04.\n",
      "Adjusting learning rate of group 0 to 4.8273e-04.\n",
      "Epoch : 1683,\ttrain_loss : 0.005618984345346689\tvalid_loss :0.004072981886565685\n",
      "Adjusting learning rate of group 0 to 4.7975e-04.\n",
      "Adjusting learning rate of group 0 to 4.7975e-04.\n",
      "Epoch : 1684,\ttrain_loss : 0.0056259711273014545\tvalid_loss :0.004070304334163666\n",
      "Adjusting learning rate of group 0 to 4.7677e-04.\n",
      "Adjusting learning rate of group 0 to 4.7677e-04.\n",
      "Epoch : 1685,\ttrain_loss : 0.005630236119031906\tvalid_loss :0.004073765594512224\n",
      "Adjusting learning rate of group 0 to 4.7380e-04.\n",
      "Adjusting learning rate of group 0 to 4.7380e-04.\n",
      "Epoch : 1686,\ttrain_loss : 0.0056246439926326275\tvalid_loss :0.004080041311681271\n",
      "Adjusting learning rate of group 0 to 4.7084e-04.\n",
      "Adjusting learning rate of group 0 to 4.7084e-04.\n",
      "Epoch : 1687,\ttrain_loss : 0.005621631164103746\tvalid_loss :0.004071410279721022\n",
      "Adjusting learning rate of group 0 to 4.6788e-04.\n",
      "Adjusting learning rate of group 0 to 4.6788e-04.\n",
      "Epoch : 1688,\ttrain_loss : 0.005615499336272478\tvalid_loss :0.0040795099921524525\n",
      "Adjusting learning rate of group 0 to 4.6494e-04.\n",
      "Adjusting learning rate of group 0 to 4.6494e-04.\n",
      "Epoch : 1689,\ttrain_loss : 0.005610865540802479\tvalid_loss :0.004070905968546867\n",
      "Adjusting learning rate of group 0 to 4.6200e-04.\n",
      "Adjusting learning rate of group 0 to 4.6200e-04.\n",
      "Epoch : 1690,\ttrain_loss : 0.005614434368908405\tvalid_loss :0.00408901646733284\n",
      "Adjusting learning rate of group 0 to 4.5907e-04.\n",
      "Adjusting learning rate of group 0 to 4.5907e-04.\n",
      "Epoch : 1691,\ttrain_loss : 0.00560558820143342\tvalid_loss :0.004066357389092445\n",
      "**********Valid loss decreased (0.004067 ==> 0.004066)**********\n",
      "Adjusting learning rate of group 0 to 4.5616e-04.\n",
      "Adjusting learning rate of group 0 to 4.5616e-04.\n",
      "Epoch : 1692,\ttrain_loss : 0.0056165894493460655\tvalid_loss :0.004073758609592915\n",
      "Adjusting learning rate of group 0 to 4.5325e-04.\n",
      "Adjusting learning rate of group 0 to 4.5325e-04.\n",
      "Epoch : 1693,\ttrain_loss : 0.005618908442556858\tvalid_loss :0.004070261027663946\n",
      "Adjusting learning rate of group 0 to 4.5035e-04.\n",
      "Adjusting learning rate of group 0 to 4.5035e-04.\n",
      "Epoch : 1694,\ttrain_loss : 0.005608029663562775\tvalid_loss :0.004085235763341188\n",
      "Adjusting learning rate of group 0 to 4.4745e-04.\n",
      "Adjusting learning rate of group 0 to 4.4745e-04.\n",
      "Epoch : 1695,\ttrain_loss : 0.005595226772129536\tvalid_loss :0.0040753805078566074\n",
      "Adjusting learning rate of group 0 to 4.4457e-04.\n",
      "Adjusting learning rate of group 0 to 4.4457e-04.\n",
      "Epoch : 1696,\ttrain_loss : 0.005596137139946222\tvalid_loss :0.004065692890435457\n",
      "**********Valid loss decreased (0.004066 ==> 0.004066)**********\n",
      "Adjusting learning rate of group 0 to 4.4170e-04.\n",
      "Adjusting learning rate of group 0 to 4.4170e-04.\n",
      "Epoch : 1697,\ttrain_loss : 0.005608479492366314\tvalid_loss :0.004064895212650299\n",
      "**********Valid loss decreased (0.004066 ==> 0.004065)**********\n",
      "Adjusting learning rate of group 0 to 4.3883e-04.\n",
      "Adjusting learning rate of group 0 to 4.3883e-04.\n",
      "Epoch : 1698,\ttrain_loss : 0.005609971471130848\tvalid_loss :0.004066666588187218\n",
      "Adjusting learning rate of group 0 to 4.3597e-04.\n",
      "Adjusting learning rate of group 0 to 4.3597e-04.\n",
      "Epoch : 1699,\ttrain_loss : 0.00561485905200243\tvalid_loss :0.0040796073153615\n",
      "Adjusting learning rate of group 0 to 4.3313e-04.\n",
      "Adjusting learning rate of group 0 to 4.3313e-04.\n",
      "Epoch : 1700,\ttrain_loss : 0.005601546261459589\tvalid_loss :0.004082507453858852\n",
      "Adjusting learning rate of group 0 to 4.3029e-04.\n",
      "Adjusting learning rate of group 0 to 4.3029e-04.\n",
      "Epoch : 1701,\ttrain_loss : 0.005599387921392918\tvalid_loss :0.004081777762621641\n",
      "Adjusting learning rate of group 0 to 4.2746e-04.\n",
      "Adjusting learning rate of group 0 to 4.2746e-04.\n",
      "Epoch : 1702,\ttrain_loss : 0.005600083619356155\tvalid_loss :0.004076855257153511\n",
      "Adjusting learning rate of group 0 to 4.2463e-04.\n",
      "Adjusting learning rate of group 0 to 4.2463e-04.\n",
      "Epoch : 1703,\ttrain_loss : 0.005591862834990025\tvalid_loss :0.004083514213562012\n",
      "Adjusting learning rate of group 0 to 4.2182e-04.\n",
      "Adjusting learning rate of group 0 to 4.2182e-04.\n",
      "Epoch : 1704,\ttrain_loss : 0.005599073134362698\tvalid_loss :0.004090786445885897\n",
      "Adjusting learning rate of group 0 to 4.1902e-04.\n",
      "Adjusting learning rate of group 0 to 4.1902e-04.\n",
      "Epoch : 1705,\ttrain_loss : 0.005592550151050091\tvalid_loss :0.004070494323968887\n",
      "Adjusting learning rate of group 0 to 4.1622e-04.\n",
      "Adjusting learning rate of group 0 to 4.1622e-04.\n",
      "Epoch : 1706,\ttrain_loss : 0.005597070790827274\tvalid_loss :0.004068284295499325\n",
      "Adjusting learning rate of group 0 to 4.1344e-04.\n",
      "Adjusting learning rate of group 0 to 4.1344e-04.\n",
      "Epoch : 1707,\ttrain_loss : 0.005591477733105421\tvalid_loss :0.004084804095327854\n",
      "Adjusting learning rate of group 0 to 4.1066e-04.\n",
      "Adjusting learning rate of group 0 to 4.1066e-04.\n",
      "Epoch : 1708,\ttrain_loss : 0.005588828586041927\tvalid_loss :0.004086285829544067\n",
      "Adjusting learning rate of group 0 to 4.0789e-04.\n",
      "Adjusting learning rate of group 0 to 4.0789e-04.\n",
      "Epoch : 1709,\ttrain_loss : 0.005588063038885593\tvalid_loss :0.0040756408125162125\n",
      "Adjusting learning rate of group 0 to 4.0513e-04.\n",
      "Adjusting learning rate of group 0 to 4.0513e-04.\n",
      "Epoch : 1710,\ttrain_loss : 0.005592172034084797\tvalid_loss :0.004085774999111891\n",
      "Adjusting learning rate of group 0 to 4.0238e-04.\n",
      "Adjusting learning rate of group 0 to 4.0238e-04.\n",
      "Epoch : 1711,\ttrain_loss : 0.0055858981795609\tvalid_loss :0.004079571925103664\n",
      "Adjusting learning rate of group 0 to 3.9964e-04.\n",
      "Adjusting learning rate of group 0 to 3.9964e-04.\n",
      "Epoch : 1712,\ttrain_loss : 0.0055906870402395725\tvalid_loss :0.0040866294875741005\n",
      "Adjusting learning rate of group 0 to 3.9690e-04.\n",
      "Adjusting learning rate of group 0 to 3.9690e-04.\n",
      "Epoch : 1713,\ttrain_loss : 0.005585833452641964\tvalid_loss :0.004074228927493095\n",
      "Adjusting learning rate of group 0 to 3.9418e-04.\n",
      "Adjusting learning rate of group 0 to 3.9418e-04.\n",
      "Epoch : 1714,\ttrain_loss : 0.0055906628258526325\tvalid_loss :0.004095272161066532\n",
      "Adjusting learning rate of group 0 to 3.9146e-04.\n",
      "Adjusting learning rate of group 0 to 3.9146e-04.\n",
      "Epoch : 1715,\ttrain_loss : 0.005585529841482639\tvalid_loss :0.004069948568940163\n",
      "Adjusting learning rate of group 0 to 3.8876e-04.\n",
      "Adjusting learning rate of group 0 to 3.8876e-04.\n",
      "Epoch : 1716,\ttrain_loss : 0.005591267254203558\tvalid_loss :0.004078329540789127\n",
      "Adjusting learning rate of group 0 to 3.8606e-04.\n",
      "Adjusting learning rate of group 0 to 3.8606e-04.\n",
      "Epoch : 1717,\ttrain_loss : 0.005586806684732437\tvalid_loss :0.004080211278051138\n",
      "Adjusting learning rate of group 0 to 3.8337e-04.\n",
      "Adjusting learning rate of group 0 to 3.8337e-04.\n",
      "Epoch : 1718,\ttrain_loss : 0.005585029721260071\tvalid_loss :0.004091561771929264\n",
      "Adjusting learning rate of group 0 to 3.8069e-04.\n",
      "Adjusting learning rate of group 0 to 3.8069e-04.\n",
      "Epoch : 1719,\ttrain_loss : 0.005582313984632492\tvalid_loss :0.004089269787073135\n",
      "Adjusting learning rate of group 0 to 3.7802e-04.\n",
      "Adjusting learning rate of group 0 to 3.7802e-04.\n",
      "Epoch : 1720,\ttrain_loss : 0.005588593427091837\tvalid_loss :0.004080069251358509\n",
      "Adjusting learning rate of group 0 to 3.7536e-04.\n",
      "Adjusting learning rate of group 0 to 3.7536e-04.\n",
      "Epoch : 1721,\ttrain_loss : 0.005589558742940426\tvalid_loss :0.004077143967151642\n",
      "Adjusting learning rate of group 0 to 3.7271e-04.\n",
      "Adjusting learning rate of group 0 to 3.7271e-04.\n",
      "Epoch : 1722,\ttrain_loss : 0.005586036015301943\tvalid_loss :0.004081288352608681\n",
      "Adjusting learning rate of group 0 to 3.7006e-04.\n",
      "Adjusting learning rate of group 0 to 3.7006e-04.\n",
      "Epoch : 1723,\ttrain_loss : 0.005587353836745024\tvalid_loss :0.004076754674315453\n",
      "Adjusting learning rate of group 0 to 3.6743e-04.\n",
      "Adjusting learning rate of group 0 to 3.6743e-04.\n",
      "Epoch : 1724,\ttrain_loss : 0.005585382226854563\tvalid_loss :0.004104455467313528\n",
      "Adjusting learning rate of group 0 to 3.6480e-04.\n",
      "Adjusting learning rate of group 0 to 3.6480e-04.\n",
      "Epoch : 1725,\ttrain_loss : 0.005579893942922354\tvalid_loss :0.0040940698236227036\n",
      "Adjusting learning rate of group 0 to 3.6218e-04.\n",
      "Adjusting learning rate of group 0 to 3.6218e-04.\n",
      "Epoch : 1726,\ttrain_loss : 0.005590599030256271\tvalid_loss :0.004100492224097252\n",
      "Adjusting learning rate of group 0 to 3.5958e-04.\n",
      "Adjusting learning rate of group 0 to 3.5958e-04.\n",
      "Epoch : 1727,\ttrain_loss : 0.005579185672104359\tvalid_loss :0.004086112137883902\n",
      "Adjusting learning rate of group 0 to 3.5698e-04.\n",
      "Adjusting learning rate of group 0 to 3.5698e-04.\n",
      "Epoch : 1728,\ttrain_loss : 0.005588109605014324\tvalid_loss :0.0041055381298065186\n",
      "Adjusting learning rate of group 0 to 3.5439e-04.\n",
      "Adjusting learning rate of group 0 to 3.5439e-04.\n",
      "Epoch : 1729,\ttrain_loss : 0.0055843135342001915\tvalid_loss :0.004082678351551294\n",
      "Adjusting learning rate of group 0 to 3.5181e-04.\n",
      "Adjusting learning rate of group 0 to 3.5181e-04.\n",
      "Epoch : 1730,\ttrain_loss : 0.0055818757973611355\tvalid_loss :0.00408918596804142\n",
      "Adjusting learning rate of group 0 to 3.4923e-04.\n",
      "Adjusting learning rate of group 0 to 3.4923e-04.\n",
      "Epoch : 1731,\ttrain_loss : 0.005584788974374533\tvalid_loss :0.004089316353201866\n",
      "Adjusting learning rate of group 0 to 3.4667e-04.\n",
      "Adjusting learning rate of group 0 to 3.4667e-04.\n",
      "Epoch : 1732,\ttrain_loss : 0.005585263017565012\tvalid_loss :0.004085857421159744\n",
      "Adjusting learning rate of group 0 to 3.4412e-04.\n",
      "Adjusting learning rate of group 0 to 3.4412e-04.\n",
      "Epoch : 1733,\ttrain_loss : 0.005581206642091274\tvalid_loss :0.0041109067387878895\n",
      "Adjusting learning rate of group 0 to 3.4157e-04.\n",
      "Adjusting learning rate of group 0 to 3.4157e-04.\n",
      "Epoch : 1734,\ttrain_loss : 0.0055846585892140865\tvalid_loss :0.004096774850040674\n",
      "Adjusting learning rate of group 0 to 3.3904e-04.\n",
      "Adjusting learning rate of group 0 to 3.3904e-04.\n",
      "Epoch : 1735,\ttrain_loss : 0.005577546544373035\tvalid_loss :0.0041075353510677814\n",
      "Adjusting learning rate of group 0 to 3.3651e-04.\n",
      "Adjusting learning rate of group 0 to 3.3651e-04.\n",
      "Epoch : 1736,\ttrain_loss : 0.005584455095231533\tvalid_loss :0.0041010938584804535\n",
      "Adjusting learning rate of group 0 to 3.3399e-04.\n",
      "Adjusting learning rate of group 0 to 3.3399e-04.\n",
      "Epoch : 1737,\ttrain_loss : 0.005578099749982357\tvalid_loss :0.004104221239686012\n",
      "Adjusting learning rate of group 0 to 3.3148e-04.\n",
      "Adjusting learning rate of group 0 to 3.3148e-04.\n",
      "Epoch : 1738,\ttrain_loss : 0.0055833375081419945\tvalid_loss :0.004095901269465685\n",
      "Adjusting learning rate of group 0 to 3.2898e-04.\n",
      "Adjusting learning rate of group 0 to 3.2898e-04.\n",
      "Epoch : 1739,\ttrain_loss : 0.005579513963311911\tvalid_loss :0.004103574901819229\n",
      "Adjusting learning rate of group 0 to 3.2649e-04.\n",
      "Adjusting learning rate of group 0 to 3.2649e-04.\n",
      "Epoch : 1740,\ttrain_loss : 0.005585395731031895\tvalid_loss :0.004109922796487808\n",
      "Adjusting learning rate of group 0 to 3.2401e-04.\n",
      "Adjusting learning rate of group 0 to 3.2401e-04.\n",
      "Epoch : 1741,\ttrain_loss : 0.005580199416726828\tvalid_loss :0.004099162295460701\n",
      "Adjusting learning rate of group 0 to 3.2154e-04.\n",
      "Adjusting learning rate of group 0 to 3.2154e-04.\n",
      "Epoch : 1742,\ttrain_loss : 0.005582526791840792\tvalid_loss :0.004091473761945963\n",
      "Adjusting learning rate of group 0 to 3.1907e-04.\n",
      "Adjusting learning rate of group 0 to 3.1907e-04.\n",
      "Epoch : 1743,\ttrain_loss : 0.0055794790387153625\tvalid_loss :0.004111493472009897\n",
      "Adjusting learning rate of group 0 to 3.1662e-04.\n",
      "Adjusting learning rate of group 0 to 3.1662e-04.\n",
      "Epoch : 1744,\ttrain_loss : 0.005587209947407246\tvalid_loss :0.004121927544474602\n",
      "Adjusting learning rate of group 0 to 3.1417e-04.\n",
      "Adjusting learning rate of group 0 to 3.1417e-04.\n",
      "Epoch : 1745,\ttrain_loss : 0.005598554853349924\tvalid_loss :0.004115475341677666\n",
      "Adjusting learning rate of group 0 to 3.1174e-04.\n",
      "Adjusting learning rate of group 0 to 3.1174e-04.\n",
      "Epoch : 1746,\ttrain_loss : 0.005589275620877743\tvalid_loss :0.0041173845529556274\n",
      "Adjusting learning rate of group 0 to 3.0931e-04.\n",
      "Adjusting learning rate of group 0 to 3.0931e-04.\n",
      "Epoch : 1747,\ttrain_loss : 0.00559279415756464\tvalid_loss :0.004119499586522579\n",
      "Adjusting learning rate of group 0 to 3.0689e-04.\n",
      "Adjusting learning rate of group 0 to 3.0689e-04.\n",
      "Epoch : 1748,\ttrain_loss : 0.005591969005763531\tvalid_loss :0.0041258735582232475\n",
      "Adjusting learning rate of group 0 to 3.0448e-04.\n",
      "Adjusting learning rate of group 0 to 3.0448e-04.\n",
      "Epoch : 1749,\ttrain_loss : 0.005598112009465694\tvalid_loss :0.004120129160583019\n",
      "Adjusting learning rate of group 0 to 3.0208e-04.\n",
      "Adjusting learning rate of group 0 to 3.0208e-04.\n",
      "Epoch : 1750,\ttrain_loss : 0.005596518982201815\tvalid_loss :0.0041117435321211815\n",
      "Adjusting learning rate of group 0 to 2.9969e-04.\n",
      "Adjusting learning rate of group 0 to 2.9969e-04.\n",
      "Epoch : 1751,\ttrain_loss : 0.005588249769061804\tvalid_loss :0.004122632555663586\n",
      "Adjusting learning rate of group 0 to 2.9731e-04.\n",
      "Adjusting learning rate of group 0 to 2.9731e-04.\n",
      "Epoch : 1752,\ttrain_loss : 0.0055923606269061565\tvalid_loss :0.004113214090466499\n",
      "Adjusting learning rate of group 0 to 2.9494e-04.\n",
      "Adjusting learning rate of group 0 to 2.9494e-04.\n",
      "Epoch : 1753,\ttrain_loss : 0.005596226546913385\tvalid_loss :0.004101335071027279\n",
      "Adjusting learning rate of group 0 to 2.9257e-04.\n",
      "Adjusting learning rate of group 0 to 2.9257e-04.\n",
      "Epoch : 1754,\ttrain_loss : 0.005599523428827524\tvalid_loss :0.004107546526938677\n",
      "Adjusting learning rate of group 0 to 2.9022e-04.\n",
      "Adjusting learning rate of group 0 to 2.9022e-04.\n",
      "Epoch : 1755,\ttrain_loss : 0.005602440796792507\tvalid_loss :0.004116337280720472\n",
      "Adjusting learning rate of group 0 to 2.8787e-04.\n",
      "Adjusting learning rate of group 0 to 2.8787e-04.\n",
      "Epoch : 1756,\ttrain_loss : 0.005605257581919432\tvalid_loss :0.004112097434699535\n",
      "Adjusting learning rate of group 0 to 2.8554e-04.\n",
      "Adjusting learning rate of group 0 to 2.8554e-04.\n",
      "Epoch : 1757,\ttrain_loss : 0.005606210790574551\tvalid_loss :0.004085984546691179\n",
      "Adjusting learning rate of group 0 to 2.8321e-04.\n",
      "Adjusting learning rate of group 0 to 2.8321e-04.\n",
      "Epoch : 1758,\ttrain_loss : 0.00560244033113122\tvalid_loss :0.004099629819393158\n",
      "Adjusting learning rate of group 0 to 2.8089e-04.\n",
      "Adjusting learning rate of group 0 to 2.8089e-04.\n",
      "Epoch : 1759,\ttrain_loss : 0.005608837585896254\tvalid_loss :0.004078646656125784\n",
      "Adjusting learning rate of group 0 to 2.7859e-04.\n",
      "Adjusting learning rate of group 0 to 2.7859e-04.\n",
      "Epoch : 1760,\ttrain_loss : 0.005604761652648449\tvalid_loss :0.004074611701071262\n",
      "Adjusting learning rate of group 0 to 2.7629e-04.\n",
      "Adjusting learning rate of group 0 to 2.7629e-04.\n",
      "Epoch : 1761,\ttrain_loss : 0.005607412196695805\tvalid_loss :0.004065157845616341\n",
      "Adjusting learning rate of group 0 to 2.7400e-04.\n",
      "Adjusting learning rate of group 0 to 2.7400e-04.\n",
      "Epoch : 1762,\ttrain_loss : 0.0056007117964327335\tvalid_loss :0.004072145558893681\n",
      "Adjusting learning rate of group 0 to 2.7172e-04.\n",
      "Adjusting learning rate of group 0 to 2.7172e-04.\n",
      "Epoch : 1763,\ttrain_loss : 0.005596423055976629\tvalid_loss :0.0040576644241809845\n",
      "**********Valid loss decreased (0.004065 ==> 0.004058)**********\n",
      "Adjusting learning rate of group 0 to 2.6944e-04.\n",
      "Adjusting learning rate of group 0 to 2.6944e-04.\n",
      "Epoch : 1764,\ttrain_loss : 0.005591640714555979\tvalid_loss :0.0040551600977778435\n",
      "**********Valid loss decreased (0.004058 ==> 0.004055)**********\n",
      "Adjusting learning rate of group 0 to 2.6718e-04.\n",
      "Adjusting learning rate of group 0 to 2.6718e-04.\n",
      "Epoch : 1765,\ttrain_loss : 0.005584996193647385\tvalid_loss :0.004053007811307907\n",
      "**********Valid loss decreased (0.004055 ==> 0.004053)**********\n",
      "Adjusting learning rate of group 0 to 2.6493e-04.\n",
      "Adjusting learning rate of group 0 to 2.6493e-04.\n",
      "Epoch : 1766,\ttrain_loss : 0.005579093005508184\tvalid_loss :0.004052626434713602\n",
      "**********Valid loss decreased (0.004053 ==> 0.004053)**********\n",
      "Adjusting learning rate of group 0 to 2.6268e-04.\n",
      "Adjusting learning rate of group 0 to 2.6268e-04.\n",
      "Epoch : 1767,\ttrain_loss : 0.005576394964009523\tvalid_loss :0.004052316769957542\n",
      "**********Valid loss decreased (0.004053 ==> 0.004052)**********\n",
      "Adjusting learning rate of group 0 to 2.6045e-04.\n",
      "Adjusting learning rate of group 0 to 2.6045e-04.\n",
      "Epoch : 1768,\ttrain_loss : 0.00557688158005476\tvalid_loss :0.004052393604069948\n",
      "Adjusting learning rate of group 0 to 2.5822e-04.\n",
      "Adjusting learning rate of group 0 to 2.5822e-04.\n",
      "Epoch : 1769,\ttrain_loss : 0.005570380948483944\tvalid_loss :0.004052662756294012\n",
      "Adjusting learning rate of group 0 to 2.5601e-04.\n",
      "Adjusting learning rate of group 0 to 2.5601e-04.\n",
      "Epoch : 1770,\ttrain_loss : 0.005570605397224426\tvalid_loss :0.0040555982850492\n",
      "Adjusting learning rate of group 0 to 2.5380e-04.\n",
      "Adjusting learning rate of group 0 to 2.5380e-04.\n",
      "Epoch : 1771,\ttrain_loss : 0.005573193076997995\tvalid_loss :0.004053096752613783\n",
      "Adjusting learning rate of group 0 to 2.5160e-04.\n",
      "Adjusting learning rate of group 0 to 2.5160e-04.\n",
      "Epoch : 1772,\ttrain_loss : 0.005572274327278137\tvalid_loss :0.004052397795021534\n",
      "Adjusting learning rate of group 0 to 2.4941e-04.\n",
      "Adjusting learning rate of group 0 to 2.4941e-04.\n",
      "Epoch : 1773,\ttrain_loss : 0.005568228662014008\tvalid_loss :0.004050967283546925\n",
      "**********Valid loss decreased (0.004052 ==> 0.004051)**********\n",
      "Adjusting learning rate of group 0 to 2.4723e-04.\n",
      "Adjusting learning rate of group 0 to 2.4723e-04.\n",
      "Epoch : 1774,\ttrain_loss : 0.005568534601479769\tvalid_loss :0.0040518357418477535\n",
      "Adjusting learning rate of group 0 to 2.4506e-04.\n",
      "Adjusting learning rate of group 0 to 2.4506e-04.\n",
      "Epoch : 1775,\ttrain_loss : 0.005568015854805708\tvalid_loss :0.0040520718321204185\n",
      "Adjusting learning rate of group 0 to 2.4290e-04.\n",
      "Adjusting learning rate of group 0 to 2.4290e-04.\n",
      "Epoch : 1776,\ttrain_loss : 0.005566374398767948\tvalid_loss :0.004055684898048639\n",
      "Adjusting learning rate of group 0 to 2.4075e-04.\n",
      "Adjusting learning rate of group 0 to 2.4075e-04.\n",
      "Epoch : 1777,\ttrain_loss : 0.0055645303800702095\tvalid_loss :0.004055097699165344\n",
      "Adjusting learning rate of group 0 to 2.3861e-04.\n",
      "Adjusting learning rate of group 0 to 2.3861e-04.\n",
      "Epoch : 1778,\ttrain_loss : 0.005564311984926462\tvalid_loss :0.004052573814988136\n",
      "Adjusting learning rate of group 0 to 2.3648e-04.\n",
      "Adjusting learning rate of group 0 to 2.3648e-04.\n",
      "Epoch : 1779,\ttrain_loss : 0.005563236307352781\tvalid_loss :0.004052105825394392\n",
      "Adjusting learning rate of group 0 to 2.3435e-04.\n",
      "Adjusting learning rate of group 0 to 2.3435e-04.\n",
      "Epoch : 1780,\ttrain_loss : 0.005565829109400511\tvalid_loss :0.004053507931530476\n",
      "Adjusting learning rate of group 0 to 2.3224e-04.\n",
      "Adjusting learning rate of group 0 to 2.3224e-04.\n",
      "Epoch : 1781,\ttrain_loss : 0.005564674269407988\tvalid_loss :0.0040543172508478165\n",
      "Adjusting learning rate of group 0 to 2.3013e-04.\n",
      "Adjusting learning rate of group 0 to 2.3013e-04.\n",
      "Epoch : 1782,\ttrain_loss : 0.005563323851674795\tvalid_loss :0.004054867662489414\n",
      "Adjusting learning rate of group 0 to 2.2804e-04.\n",
      "Adjusting learning rate of group 0 to 2.2804e-04.\n",
      "Epoch : 1783,\ttrain_loss : 0.005563610699027777\tvalid_loss :0.004055171273648739\n",
      "Adjusting learning rate of group 0 to 2.2595e-04.\n",
      "Adjusting learning rate of group 0 to 2.2595e-04.\n",
      "Epoch : 1784,\ttrain_loss : 0.005561781115829945\tvalid_loss :0.004050177522003651\n",
      "**********Valid loss decreased (0.004051 ==> 0.004050)**********\n",
      "Adjusting learning rate of group 0 to 2.2387e-04.\n",
      "Adjusting learning rate of group 0 to 2.2387e-04.\n",
      "Epoch : 1785,\ttrain_loss : 0.005561031401157379\tvalid_loss :0.004051492549479008\n",
      "Adjusting learning rate of group 0 to 2.2181e-04.\n",
      "Adjusting learning rate of group 0 to 2.2181e-04.\n",
      "Epoch : 1786,\ttrain_loss : 0.005563174840062857\tvalid_loss :0.004049955867230892\n",
      "**********Valid loss decreased (0.004050 ==> 0.004050)**********\n",
      "Adjusting learning rate of group 0 to 2.1975e-04.\n",
      "Adjusting learning rate of group 0 to 2.1975e-04.\n",
      "Epoch : 1787,\ttrain_loss : 0.005559440702199936\tvalid_loss :0.004052171483635902\n",
      "Adjusting learning rate of group 0 to 2.1770e-04.\n",
      "Adjusting learning rate of group 0 to 2.1770e-04.\n",
      "Epoch : 1788,\ttrain_loss : 0.005561384372413158\tvalid_loss :0.00405100267380476\n",
      "Adjusting learning rate of group 0 to 2.1566e-04.\n",
      "Adjusting learning rate of group 0 to 2.1566e-04.\n",
      "Epoch : 1789,\ttrain_loss : 0.005559101700782776\tvalid_loss :0.004056841600686312\n",
      "Adjusting learning rate of group 0 to 2.1363e-04.\n",
      "Adjusting learning rate of group 0 to 2.1363e-04.\n",
      "Epoch : 1790,\ttrain_loss : 0.005561326630413532\tvalid_loss :0.004049839451909065\n",
      "**********Valid loss decreased (0.004050 ==> 0.004050)**********\n",
      "Adjusting learning rate of group 0 to 2.1161e-04.\n",
      "Adjusting learning rate of group 0 to 2.1161e-04.\n",
      "Epoch : 1791,\ttrain_loss : 0.0055580055341124535\tvalid_loss :0.004055912606418133\n",
      "Adjusting learning rate of group 0 to 2.0959e-04.\n",
      "Adjusting learning rate of group 0 to 2.0959e-04.\n",
      "Epoch : 1792,\ttrain_loss : 0.005561180412769318\tvalid_loss :0.0040488410741090775\n",
      "**********Valid loss decreased (0.004050 ==> 0.004049)**********\n",
      "Adjusting learning rate of group 0 to 2.0759e-04.\n",
      "Adjusting learning rate of group 0 to 2.0759e-04.\n",
      "Epoch : 1793,\ttrain_loss : 0.005560316611081362\tvalid_loss :0.004053471144288778\n",
      "Adjusting learning rate of group 0 to 2.0560e-04.\n",
      "Adjusting learning rate of group 0 to 2.0560e-04.\n",
      "Epoch : 1794,\ttrain_loss : 0.0055604721419513226\tvalid_loss :0.004057084210216999\n",
      "Adjusting learning rate of group 0 to 2.0362e-04.\n",
      "Adjusting learning rate of group 0 to 2.0362e-04.\n",
      "Epoch : 1795,\ttrain_loss : 0.005560967605561018\tvalid_loss :0.0040502711199223995\n",
      "Adjusting learning rate of group 0 to 2.0164e-04.\n",
      "Adjusting learning rate of group 0 to 2.0164e-04.\n",
      "Epoch : 1796,\ttrain_loss : 0.005556211341172457\tvalid_loss :0.0040532853454351425\n",
      "Adjusting learning rate of group 0 to 1.9968e-04.\n",
      "Adjusting learning rate of group 0 to 1.9968e-04.\n",
      "Epoch : 1797,\ttrain_loss : 0.005555774085223675\tvalid_loss :0.0040476382710039616\n",
      "**********Valid loss decreased (0.004049 ==> 0.004048)**********\n",
      "Adjusting learning rate of group 0 to 1.9772e-04.\n",
      "Adjusting learning rate of group 0 to 1.9772e-04.\n",
      "Epoch : 1798,\ttrain_loss : 0.005553542170673609\tvalid_loss :0.00405261293053627\n",
      "Adjusting learning rate of group 0 to 1.9577e-04.\n",
      "Adjusting learning rate of group 0 to 1.9577e-04.\n",
      "Epoch : 1799,\ttrain_loss : 0.005554787814617157\tvalid_loss :0.004051955416798592\n",
      "Adjusting learning rate of group 0 to 1.9384e-04.\n",
      "Adjusting learning rate of group 0 to 1.9384e-04.\n",
      "Epoch : 1800,\ttrain_loss : 0.005555406212806702\tvalid_loss :0.004052658099681139\n",
      "Adjusting learning rate of group 0 to 1.9191e-04.\n",
      "Adjusting learning rate of group 0 to 1.9191e-04.\n",
      "Epoch : 1801,\ttrain_loss : 0.0055542909540236\tvalid_loss :0.004053959622979164\n",
      "Adjusting learning rate of group 0 to 1.8999e-04.\n",
      "Adjusting learning rate of group 0 to 1.8999e-04.\n",
      "Epoch : 1802,\ttrain_loss : 0.005553445778787136\tvalid_loss :0.004057486075907946\n",
      "Adjusting learning rate of group 0 to 1.8808e-04.\n",
      "Adjusting learning rate of group 0 to 1.8808e-04.\n",
      "Epoch : 1803,\ttrain_loss : 0.005555085372179747\tvalid_loss :0.004053901880979538\n",
      "Adjusting learning rate of group 0 to 1.8618e-04.\n",
      "Adjusting learning rate of group 0 to 1.8618e-04.\n",
      "Epoch : 1804,\ttrain_loss : 0.005553681403398514\tvalid_loss :0.004055504687130451\n",
      "Adjusting learning rate of group 0 to 1.8429e-04.\n",
      "Adjusting learning rate of group 0 to 1.8429e-04.\n",
      "Epoch : 1805,\ttrain_loss : 0.005555243697017431\tvalid_loss :0.004053988493978977\n",
      "Adjusting learning rate of group 0 to 1.8241e-04.\n",
      "Adjusting learning rate of group 0 to 1.8241e-04.\n",
      "Epoch : 1806,\ttrain_loss : 0.005553041119128466\tvalid_loss :0.004050472751259804\n",
      "Adjusting learning rate of group 0 to 1.8054e-04.\n",
      "Adjusting learning rate of group 0 to 1.8054e-04.\n",
      "Epoch : 1807,\ttrain_loss : 0.005552778486162424\tvalid_loss :0.004045700654387474\n",
      "**********Valid loss decreased (0.004048 ==> 0.004046)**********\n",
      "Adjusting learning rate of group 0 to 1.7868e-04.\n",
      "Adjusting learning rate of group 0 to 1.7868e-04.\n",
      "Epoch : 1808,\ttrain_loss : 0.005549015011638403\tvalid_loss :0.004054233897477388\n",
      "Adjusting learning rate of group 0 to 1.7683e-04.\n",
      "Adjusting learning rate of group 0 to 1.7683e-04.\n",
      "Epoch : 1809,\ttrain_loss : 0.005553270224481821\tvalid_loss :0.004058412741869688\n",
      "Adjusting learning rate of group 0 to 1.7499e-04.\n",
      "Adjusting learning rate of group 0 to 1.7499e-04.\n",
      "Epoch : 1810,\ttrain_loss : 0.005552269518375397\tvalid_loss :0.0040513621643185616\n",
      "Adjusting learning rate of group 0 to 1.7315e-04.\n",
      "Adjusting learning rate of group 0 to 1.7315e-04.\n",
      "Epoch : 1811,\ttrain_loss : 0.005549631081521511\tvalid_loss :0.004056788049638271\n",
      "Adjusting learning rate of group 0 to 1.7133e-04.\n",
      "Adjusting learning rate of group 0 to 1.7133e-04.\n",
      "Epoch : 1812,\ttrain_loss : 0.005554452072829008\tvalid_loss :0.0040597571060061455\n",
      "Adjusting learning rate of group 0 to 1.6951e-04.\n",
      "Adjusting learning rate of group 0 to 1.6951e-04.\n",
      "Epoch : 1813,\ttrain_loss : 0.0055574700236320496\tvalid_loss :0.004058280494064093\n",
      "Adjusting learning rate of group 0 to 1.6771e-04.\n",
      "Adjusting learning rate of group 0 to 1.6771e-04.\n",
      "Epoch : 1814,\ttrain_loss : 0.005559678189456463\tvalid_loss :0.00405550841242075\n",
      "Adjusting learning rate of group 0 to 1.6591e-04.\n",
      "Adjusting learning rate of group 0 to 1.6591e-04.\n",
      "Epoch : 1815,\ttrain_loss : 0.005558285862207413\tvalid_loss :0.004059562925249338\n",
      "Adjusting learning rate of group 0 to 1.6413e-04.\n",
      "Adjusting learning rate of group 0 to 1.6413e-04.\n",
      "Epoch : 1816,\ttrain_loss : 0.005559317767620087\tvalid_loss :0.00405358150601387\n",
      "Adjusting learning rate of group 0 to 1.6235e-04.\n",
      "Adjusting learning rate of group 0 to 1.6235e-04.\n",
      "Epoch : 1817,\ttrain_loss : 0.005557772237807512\tvalid_loss :0.004052981734275818\n",
      "Adjusting learning rate of group 0 to 1.6058e-04.\n",
      "Adjusting learning rate of group 0 to 1.6058e-04.\n",
      "Epoch : 1818,\ttrain_loss : 0.005555347539484501\tvalid_loss :0.004053396638482809\n",
      "Adjusting learning rate of group 0 to 1.5883e-04.\n",
      "Adjusting learning rate of group 0 to 1.5883e-04.\n",
      "Epoch : 1819,\ttrain_loss : 0.005562270991504192\tvalid_loss :0.00404954282566905\n",
      "Adjusting learning rate of group 0 to 1.5708e-04.\n",
      "Adjusting learning rate of group 0 to 1.5708e-04.\n",
      "Epoch : 1820,\ttrain_loss : 0.005556281190365553\tvalid_loss :0.004049806855618954\n",
      "Adjusting learning rate of group 0 to 1.5534e-04.\n",
      "Adjusting learning rate of group 0 to 1.5534e-04.\n",
      "Epoch : 1821,\ttrain_loss : 0.005562656093388796\tvalid_loss :0.004048203118145466\n",
      "Adjusting learning rate of group 0 to 1.5361e-04.\n",
      "Adjusting learning rate of group 0 to 1.5361e-04.\n",
      "Epoch : 1822,\ttrain_loss : 0.005564049817621708\tvalid_loss :0.004047537222504616\n",
      "Adjusting learning rate of group 0 to 1.5189e-04.\n",
      "Adjusting learning rate of group 0 to 1.5189e-04.\n",
      "Epoch : 1823,\ttrain_loss : 0.005562934558838606\tvalid_loss :0.0040423134341835976\n",
      "**********Valid loss decreased (0.004046 ==> 0.004042)**********\n",
      "Adjusting learning rate of group 0 to 1.5018e-04.\n",
      "Adjusting learning rate of group 0 to 1.5018e-04.\n",
      "Epoch : 1824,\ttrain_loss : 0.005556715186685324\tvalid_loss :0.004046949557960033\n",
      "Adjusting learning rate of group 0 to 1.4848e-04.\n",
      "Adjusting learning rate of group 0 to 1.4848e-04.\n",
      "Epoch : 1825,\ttrain_loss : 0.005554679781198502\tvalid_loss :0.004048383329063654\n",
      "Adjusting learning rate of group 0 to 1.4679e-04.\n",
      "Adjusting learning rate of group 0 to 1.4679e-04.\n",
      "Epoch : 1826,\ttrain_loss : 0.0055588711984455585\tvalid_loss :0.0040474385023117065\n",
      "Adjusting learning rate of group 0 to 1.4511e-04.\n",
      "Adjusting learning rate of group 0 to 1.4511e-04.\n",
      "Epoch : 1827,\ttrain_loss : 0.005554534960538149\tvalid_loss :0.004047762602567673\n",
      "Adjusting learning rate of group 0 to 1.4343e-04.\n",
      "Adjusting learning rate of group 0 to 1.4343e-04.\n",
      "Epoch : 1828,\ttrain_loss : 0.005553437862545252\tvalid_loss :0.004045587033033371\n",
      "Adjusting learning rate of group 0 to 1.4177e-04.\n",
      "Adjusting learning rate of group 0 to 1.4177e-04.\n",
      "Epoch : 1829,\ttrain_loss : 0.0055503542535007\tvalid_loss :0.004048000555485487\n",
      "Adjusting learning rate of group 0 to 1.4012e-04.\n",
      "Adjusting learning rate of group 0 to 1.4012e-04.\n",
      "Epoch : 1830,\ttrain_loss : 0.005551662761718035\tvalid_loss :0.004047367721796036\n",
      "Adjusting learning rate of group 0 to 1.3847e-04.\n",
      "Adjusting learning rate of group 0 to 1.3847e-04.\n",
      "Epoch : 1831,\ttrain_loss : 0.00555127440020442\tvalid_loss :0.004048482980579138\n",
      "Adjusting learning rate of group 0 to 1.3684e-04.\n",
      "Adjusting learning rate of group 0 to 1.3684e-04.\n",
      "Epoch : 1832,\ttrain_loss : 0.0055477130226790905\tvalid_loss :0.004049891605973244\n",
      "Adjusting learning rate of group 0 to 1.3521e-04.\n",
      "Adjusting learning rate of group 0 to 1.3521e-04.\n",
      "Epoch : 1833,\ttrain_loss : 0.005545855034142733\tvalid_loss :0.004048873670399189\n",
      "Adjusting learning rate of group 0 to 1.3360e-04.\n",
      "Adjusting learning rate of group 0 to 1.3360e-04.\n",
      "Epoch : 1834,\ttrain_loss : 0.005546319764107466\tvalid_loss :0.004047255963087082\n",
      "Adjusting learning rate of group 0 to 1.3199e-04.\n",
      "Adjusting learning rate of group 0 to 1.3199e-04.\n",
      "Epoch : 1835,\ttrain_loss : 0.0055442252196371555\tvalid_loss :0.0040498171001672745\n",
      "Adjusting learning rate of group 0 to 1.3040e-04.\n",
      "Adjusting learning rate of group 0 to 1.3040e-04.\n",
      "Epoch : 1836,\ttrain_loss : 0.00554466899484396\tvalid_loss :0.004049734212458134\n",
      "Adjusting learning rate of group 0 to 1.2881e-04.\n",
      "Adjusting learning rate of group 0 to 1.2881e-04.\n",
      "Epoch : 1837,\ttrain_loss : 0.005546957720071077\tvalid_loss :0.004045020788908005\n",
      "Adjusting learning rate of group 0 to 1.2723e-04.\n",
      "Adjusting learning rate of group 0 to 1.2723e-04.\n",
      "Epoch : 1838,\ttrain_loss : 0.005542069673538208\tvalid_loss :0.0040503512136638165\n",
      "Adjusting learning rate of group 0 to 1.2567e-04.\n",
      "Adjusting learning rate of group 0 to 1.2567e-04.\n",
      "Epoch : 1839,\ttrain_loss : 0.005543798208236694\tvalid_loss :0.004049522802233696\n",
      "Adjusting learning rate of group 0 to 1.2411e-04.\n",
      "Adjusting learning rate of group 0 to 1.2411e-04.\n",
      "Epoch : 1840,\ttrain_loss : 0.005536634474992752\tvalid_loss :0.004045617766678333\n",
      "Adjusting learning rate of group 0 to 1.2256e-04.\n",
      "Adjusting learning rate of group 0 to 1.2256e-04.\n",
      "Epoch : 1841,\ttrain_loss : 0.005537237040698528\tvalid_loss :0.004047381691634655\n",
      "Adjusting learning rate of group 0 to 1.2102e-04.\n",
      "Adjusting learning rate of group 0 to 1.2102e-04.\n",
      "Epoch : 1842,\ttrain_loss : 0.00553898373618722\tvalid_loss :0.00404189620167017\n",
      "**********Valid loss decreased (0.004042 ==> 0.004042)**********\n",
      "Adjusting learning rate of group 0 to 1.1949e-04.\n",
      "Adjusting learning rate of group 0 to 1.1949e-04.\n",
      "Epoch : 1843,\ttrain_loss : 0.005536478012800217\tvalid_loss :0.004045551642775536\n",
      "Adjusting learning rate of group 0 to 1.1797e-04.\n",
      "Adjusting learning rate of group 0 to 1.1797e-04.\n",
      "Epoch : 1844,\ttrain_loss : 0.005540467333048582\tvalid_loss :0.004044439177960157\n",
      "Adjusting learning rate of group 0 to 1.1646e-04.\n",
      "Adjusting learning rate of group 0 to 1.1646e-04.\n",
      "Epoch : 1845,\ttrain_loss : 0.005538162775337696\tvalid_loss :0.004047049209475517\n",
      "Adjusting learning rate of group 0 to 1.1496e-04.\n",
      "Adjusting learning rate of group 0 to 1.1496e-04.\n",
      "Epoch : 1846,\ttrain_loss : 0.005538799799978733\tvalid_loss :0.0040463171899318695\n",
      "Adjusting learning rate of group 0 to 1.1347e-04.\n",
      "Adjusting learning rate of group 0 to 1.1347e-04.\n",
      "Epoch : 1847,\ttrain_loss : 0.005535086151212454\tvalid_loss :0.004043541382998228\n",
      "Adjusting learning rate of group 0 to 1.1199e-04.\n",
      "Adjusting learning rate of group 0 to 1.1199e-04.\n",
      "Epoch : 1848,\ttrain_loss : 0.005534992087632418\tvalid_loss :0.004046108108013868\n",
      "Adjusting learning rate of group 0 to 1.1052e-04.\n",
      "Adjusting learning rate of group 0 to 1.1052e-04.\n",
      "Epoch : 1849,\ttrain_loss : 0.005538648460060358\tvalid_loss :0.0040407078340649605\n",
      "**********Valid loss decreased (0.004042 ==> 0.004041)**********\n",
      "Adjusting learning rate of group 0 to 1.0906e-04.\n",
      "Adjusting learning rate of group 0 to 1.0906e-04.\n",
      "Epoch : 1850,\ttrain_loss : 0.005533765535801649\tvalid_loss :0.004045757465064526\n",
      "Adjusting learning rate of group 0 to 1.0761e-04.\n",
      "Adjusting learning rate of group 0 to 1.0761e-04.\n",
      "Epoch : 1851,\ttrain_loss : 0.005538654047995806\tvalid_loss :0.004045798443257809\n",
      "Adjusting learning rate of group 0 to 1.0616e-04.\n",
      "Adjusting learning rate of group 0 to 1.0616e-04.\n",
      "Epoch : 1852,\ttrain_loss : 0.00553527195006609\tvalid_loss :0.004041723441332579\n",
      "Adjusting learning rate of group 0 to 1.0473e-04.\n",
      "Adjusting learning rate of group 0 to 1.0473e-04.\n",
      "Epoch : 1853,\ttrain_loss : 0.005532949697226286\tvalid_loss :0.004044383764266968\n",
      "Adjusting learning rate of group 0 to 1.0331e-04.\n",
      "Adjusting learning rate of group 0 to 1.0331e-04.\n",
      "Epoch : 1854,\ttrain_loss : 0.005534977186471224\tvalid_loss :0.004042596090584993\n",
      "Adjusting learning rate of group 0 to 1.0189e-04.\n",
      "Adjusting learning rate of group 0 to 1.0189e-04.\n",
      "Epoch : 1855,\ttrain_loss : 0.005532510112971067\tvalid_loss :0.004042591899633408\n",
      "Adjusting learning rate of group 0 to 1.0049e-04.\n",
      "Adjusting learning rate of group 0 to 1.0049e-04.\n",
      "Epoch : 1856,\ttrain_loss : 0.005533807445317507\tvalid_loss :0.004041673615574837\n",
      "Adjusting learning rate of group 0 to 9.9093e-05.\n",
      "Adjusting learning rate of group 0 to 9.9093e-05.\n",
      "Epoch : 1857,\ttrain_loss : 0.005533736199140549\tvalid_loss :0.004043149761855602\n",
      "Adjusting learning rate of group 0 to 9.7708e-05.\n",
      "Adjusting learning rate of group 0 to 9.7708e-05.\n",
      "Epoch : 1858,\ttrain_loss : 0.005533069372177124\tvalid_loss :0.004042675253003836\n",
      "Adjusting learning rate of group 0 to 9.6333e-05.\n",
      "Adjusting learning rate of group 0 to 9.6333e-05.\n",
      "Epoch : 1859,\ttrain_loss : 0.005532932933419943\tvalid_loss :0.004042881540954113\n",
      "Adjusting learning rate of group 0 to 9.4967e-05.\n",
      "Adjusting learning rate of group 0 to 9.4967e-05.\n",
      "Epoch : 1860,\ttrain_loss : 0.005532543174922466\tvalid_loss :0.004042727407068014\n",
      "Adjusting learning rate of group 0 to 9.3611e-05.\n",
      "Adjusting learning rate of group 0 to 9.3611e-05.\n",
      "Epoch : 1861,\ttrain_loss : 0.005532592535018921\tvalid_loss :0.004042281769216061\n",
      "Adjusting learning rate of group 0 to 9.2264e-05.\n",
      "Adjusting learning rate of group 0 to 9.2264e-05.\n",
      "Epoch : 1862,\ttrain_loss : 0.005531627219170332\tvalid_loss :0.0040412298403680325\n",
      "Adjusting learning rate of group 0 to 9.0928e-05.\n",
      "Adjusting learning rate of group 0 to 9.0928e-05.\n",
      "Epoch : 1863,\ttrain_loss : 0.005532733164727688\tvalid_loss :0.004041610285639763\n",
      "Adjusting learning rate of group 0 to 8.9600e-05.\n",
      "Adjusting learning rate of group 0 to 8.9600e-05.\n",
      "Epoch : 1864,\ttrain_loss : 0.005532688461244106\tvalid_loss :0.004040575586259365\n",
      "**********Valid loss decreased (0.004041 ==> 0.004041)**********\n",
      "Adjusting learning rate of group 0 to 8.8283e-05.\n",
      "Adjusting learning rate of group 0 to 8.8283e-05.\n",
      "Epoch : 1865,\ttrain_loss : 0.005531236995011568\tvalid_loss :0.0040418896824121475\n",
      "Adjusting learning rate of group 0 to 8.6975e-05.\n",
      "Adjusting learning rate of group 0 to 8.6975e-05.\n",
      "Epoch : 1866,\ttrain_loss : 0.0055322712287306786\tvalid_loss :0.004042875487357378\n",
      "Adjusting learning rate of group 0 to 8.5676e-05.\n",
      "Adjusting learning rate of group 0 to 8.5676e-05.\n",
      "Epoch : 1867,\ttrain_loss : 0.005533808376640081\tvalid_loss :0.004038764163851738\n",
      "**********Valid loss decreased (0.004041 ==> 0.004039)**********\n",
      "Adjusting learning rate of group 0 to 8.4388e-05.\n",
      "Adjusting learning rate of group 0 to 8.4388e-05.\n",
      "Epoch : 1868,\ttrain_loss : 0.0055304341949522495\tvalid_loss :0.004043790977448225\n",
      "Adjusting learning rate of group 0 to 8.3109e-05.\n",
      "Adjusting learning rate of group 0 to 8.3109e-05.\n",
      "Epoch : 1869,\ttrain_loss : 0.005530457012355328\tvalid_loss :0.004039536230266094\n",
      "Adjusting learning rate of group 0 to 8.1839e-05.\n",
      "Adjusting learning rate of group 0 to 8.1839e-05.\n",
      "Epoch : 1870,\ttrain_loss : 0.005530177149921656\tvalid_loss :0.0040413327515125275\n",
      "Adjusting learning rate of group 0 to 8.0580e-05.\n",
      "Adjusting learning rate of group 0 to 8.0580e-05.\n",
      "Epoch : 1871,\ttrain_loss : 0.005530750844627619\tvalid_loss :0.004040781874209642\n",
      "Adjusting learning rate of group 0 to 7.9330e-05.\n",
      "Adjusting learning rate of group 0 to 7.9330e-05.\n",
      "Epoch : 1872,\ttrain_loss : 0.0055306656286120415\tvalid_loss :0.004040450789034367\n",
      "Adjusting learning rate of group 0 to 7.8090e-05.\n",
      "Adjusting learning rate of group 0 to 7.8090e-05.\n",
      "Epoch : 1873,\ttrain_loss : 0.005529444664716721\tvalid_loss :0.004041274543851614\n",
      "Adjusting learning rate of group 0 to 7.6859e-05.\n",
      "Adjusting learning rate of group 0 to 7.6859e-05.\n",
      "Epoch : 1874,\ttrain_loss : 0.005530561786144972\tvalid_loss :0.004041033796966076\n",
      "Adjusting learning rate of group 0 to 7.5638e-05.\n",
      "Adjusting learning rate of group 0 to 7.5638e-05.\n",
      "Epoch : 1875,\ttrain_loss : 0.005528892390429974\tvalid_loss :0.004040438681840897\n",
      "Adjusting learning rate of group 0 to 7.4427e-05.\n",
      "Adjusting learning rate of group 0 to 7.4427e-05.\n",
      "Epoch : 1876,\ttrain_loss : 0.00552966445684433\tvalid_loss :0.0040402403101325035\n",
      "Adjusting learning rate of group 0 to 7.3225e-05.\n",
      "Adjusting learning rate of group 0 to 7.3225e-05.\n",
      "Epoch : 1877,\ttrain_loss : 0.0055288164876401424\tvalid_loss :0.004040906205773354\n",
      "Adjusting learning rate of group 0 to 7.2033e-05.\n",
      "Adjusting learning rate of group 0 to 7.2033e-05.\n",
      "Epoch : 1878,\ttrain_loss : 0.005529258865863085\tvalid_loss :0.004040109924972057\n",
      "Adjusting learning rate of group 0 to 7.0851e-05.\n",
      "Adjusting learning rate of group 0 to 7.0851e-05.\n",
      "Epoch : 1879,\ttrain_loss : 0.005528164561837912\tvalid_loss :0.004041023086756468\n",
      "Adjusting learning rate of group 0 to 6.9678e-05.\n",
      "Adjusting learning rate of group 0 to 6.9678e-05.\n",
      "Epoch : 1880,\ttrain_loss : 0.005529412534087896\tvalid_loss :0.004040176514536142\n",
      "Adjusting learning rate of group 0 to 6.8516e-05.\n",
      "Adjusting learning rate of group 0 to 6.8516e-05.\n",
      "Epoch : 1881,\ttrain_loss : 0.005528359208256006\tvalid_loss :0.004040915053337812\n",
      "Adjusting learning rate of group 0 to 6.7363e-05.\n",
      "Adjusting learning rate of group 0 to 6.7363e-05.\n",
      "Epoch : 1882,\ttrain_loss : 0.005528641398996115\tvalid_loss :0.004040078725665808\n",
      "Adjusting learning rate of group 0 to 6.6219e-05.\n",
      "Adjusting learning rate of group 0 to 6.6219e-05.\n",
      "Epoch : 1883,\ttrain_loss : 0.005528333131223917\tvalid_loss :0.004040171392261982\n",
      "Adjusting learning rate of group 0 to 6.5085e-05.\n",
      "Adjusting learning rate of group 0 to 6.5085e-05.\n",
      "Epoch : 1884,\ttrain_loss : 0.005528596229851246\tvalid_loss :0.004041546024382114\n",
      "Adjusting learning rate of group 0 to 6.3961e-05.\n",
      "Adjusting learning rate of group 0 to 6.3961e-05.\n",
      "Epoch : 1885,\ttrain_loss : 0.005528710782527924\tvalid_loss :0.004039352294057608\n",
      "Adjusting learning rate of group 0 to 6.2847e-05.\n",
      "Adjusting learning rate of group 0 to 6.2847e-05.\n",
      "Epoch : 1886,\ttrain_loss : 0.005527574568986893\tvalid_loss :0.00404169037938118\n",
      "Adjusting learning rate of group 0 to 6.1743e-05.\n",
      "Adjusting learning rate of group 0 to 6.1743e-05.\n",
      "Epoch : 1887,\ttrain_loss : 0.005529108457267284\tvalid_loss :0.004039159044623375\n",
      "Adjusting learning rate of group 0 to 6.0648e-05.\n",
      "Adjusting learning rate of group 0 to 6.0648e-05.\n",
      "Epoch : 1888,\ttrain_loss : 0.0055284323170781136\tvalid_loss :0.00404182355850935\n",
      "Adjusting learning rate of group 0 to 5.9563e-05.\n",
      "Adjusting learning rate of group 0 to 5.9563e-05.\n",
      "Epoch : 1889,\ttrain_loss : 0.005528576206415892\tvalid_loss :0.004039025399833918\n",
      "Adjusting learning rate of group 0 to 5.8487e-05.\n",
      "Adjusting learning rate of group 0 to 5.8487e-05.\n",
      "Epoch : 1890,\ttrain_loss : 0.005528255365788937\tvalid_loss :0.004042033106088638\n",
      "Adjusting learning rate of group 0 to 5.7422e-05.\n",
      "Adjusting learning rate of group 0 to 5.7422e-05.\n",
      "Epoch : 1891,\ttrain_loss : 0.005529765505343676\tvalid_loss :0.004039349965751171\n",
      "Adjusting learning rate of group 0 to 5.6366e-05.\n",
      "Adjusting learning rate of group 0 to 5.6366e-05.\n",
      "Epoch : 1892,\ttrain_loss : 0.005527735222131014\tvalid_loss :0.004041201435029507\n",
      "Adjusting learning rate of group 0 to 5.5319e-05.\n",
      "Adjusting learning rate of group 0 to 5.5319e-05.\n",
      "Epoch : 1893,\ttrain_loss : 0.005527288652956486\tvalid_loss :0.004039408639073372\n",
      "Adjusting learning rate of group 0 to 5.4283e-05.\n",
      "Adjusting learning rate of group 0 to 5.4283e-05.\n",
      "Epoch : 1894,\ttrain_loss : 0.005528436508029699\tvalid_loss :0.004042789805680513\n",
      "Adjusting learning rate of group 0 to 5.3256e-05.\n",
      "Adjusting learning rate of group 0 to 5.3256e-05.\n",
      "Epoch : 1895,\ttrain_loss : 0.0055279857479035854\tvalid_loss :0.004039681516587734\n",
      "Adjusting learning rate of group 0 to 5.2239e-05.\n",
      "Adjusting learning rate of group 0 to 5.2239e-05.\n",
      "Epoch : 1896,\ttrain_loss : 0.005527955945581198\tvalid_loss :0.004042013082653284\n",
      "Adjusting learning rate of group 0 to 5.1232e-05.\n",
      "Adjusting learning rate of group 0 to 5.1232e-05.\n",
      "Epoch : 1897,\ttrain_loss : 0.005528472363948822\tvalid_loss :0.004039597697556019\n",
      "Adjusting learning rate of group 0 to 5.0234e-05.\n",
      "Adjusting learning rate of group 0 to 5.0234e-05.\n",
      "Epoch : 1898,\ttrain_loss : 0.005527749191969633\tvalid_loss :0.004041890613734722\n",
      "Adjusting learning rate of group 0 to 4.9247e-05.\n",
      "Adjusting learning rate of group 0 to 4.9247e-05.\n",
      "Epoch : 1899,\ttrain_loss : 0.005528733134269714\tvalid_loss :0.00404037069529295\n",
      "Adjusting learning rate of group 0 to 4.8269e-05.\n",
      "Adjusting learning rate of group 0 to 4.8269e-05.\n",
      "Epoch : 1900,\ttrain_loss : 0.005528632085770369\tvalid_loss :0.0040391297079622746\n",
      "Adjusting learning rate of group 0 to 4.7300e-05.\n",
      "Adjusting learning rate of group 0 to 4.7300e-05.\n",
      "Epoch : 1901,\ttrain_loss : 0.005530002061277628\tvalid_loss :0.004039199091494083\n",
      "Adjusting learning rate of group 0 to 4.6342e-05.\n",
      "Adjusting learning rate of group 0 to 4.6342e-05.\n",
      "Epoch : 1902,\ttrain_loss : 0.005530052352696657\tvalid_loss :0.004038709681481123\n",
      "**********Valid loss decreased (0.004039 ==> 0.004039)**********\n",
      "Adjusting learning rate of group 0 to 4.5393e-05.\n",
      "Adjusting learning rate of group 0 to 4.5393e-05.\n",
      "Epoch : 1903,\ttrain_loss : 0.005530728492885828\tvalid_loss :0.004038533195853233\n",
      "**********Valid loss decreased (0.004039 ==> 0.004039)**********\n",
      "Adjusting learning rate of group 0 to 4.4454e-05.\n",
      "Adjusting learning rate of group 0 to 4.4454e-05.\n",
      "Epoch : 1904,\ttrain_loss : 0.00552974920719862\tvalid_loss :0.004037249367684126\n",
      "**********Valid loss decreased (0.004039 ==> 0.004037)**********\n",
      "Adjusting learning rate of group 0 to 4.3525e-05.\n",
      "Adjusting learning rate of group 0 to 4.3525e-05.\n",
      "Epoch : 1905,\ttrain_loss : 0.005529642570763826\tvalid_loss :0.0040365708991885185\n",
      "**********Valid loss decreased (0.004037 ==> 0.004037)**********\n",
      "Adjusting learning rate of group 0 to 4.2605e-05.\n",
      "Adjusting learning rate of group 0 to 4.2605e-05.\n",
      "Epoch : 1906,\ttrain_loss : 0.005528580397367477\tvalid_loss :0.004037007223814726\n",
      "Adjusting learning rate of group 0 to 4.1696e-05.\n",
      "Adjusting learning rate of group 0 to 4.1696e-05.\n",
      "Epoch : 1907,\ttrain_loss : 0.005528403911739588\tvalid_loss :0.004038350656628609\n",
      "Adjusting learning rate of group 0 to 4.0796e-05.\n",
      "Adjusting learning rate of group 0 to 4.0796e-05.\n",
      "Epoch : 1908,\ttrain_loss : 0.005528982728719711\tvalid_loss :0.004038568120449781\n",
      "Adjusting learning rate of group 0 to 3.9905e-05.\n",
      "Adjusting learning rate of group 0 to 3.9905e-05.\n",
      "Epoch : 1909,\ttrain_loss : 0.005529553163796663\tvalid_loss :0.004037606995552778\n",
      "Adjusting learning rate of group 0 to 3.9025e-05.\n",
      "Adjusting learning rate of group 0 to 3.9025e-05.\n",
      "Epoch : 1910,\ttrain_loss : 0.005528741050511599\tvalid_loss :0.004037654493004084\n",
      "Adjusting learning rate of group 0 to 3.8154e-05.\n",
      "Adjusting learning rate of group 0 to 3.8154e-05.\n",
      "Epoch : 1911,\ttrain_loss : 0.00552911264821887\tvalid_loss :0.004038309212774038\n",
      "Adjusting learning rate of group 0 to 3.7293e-05.\n",
      "Adjusting learning rate of group 0 to 3.7293e-05.\n",
      "Epoch : 1912,\ttrain_loss : 0.0055290465243160725\tvalid_loss :0.004039200022816658\n",
      "Adjusting learning rate of group 0 to 3.6442e-05.\n",
      "Adjusting learning rate of group 0 to 3.6442e-05.\n",
      "Epoch : 1913,\ttrain_loss : 0.005530055146664381\tvalid_loss :0.004039186984300613\n",
      "Adjusting learning rate of group 0 to 3.5601e-05.\n",
      "Adjusting learning rate of group 0 to 3.5601e-05.\n",
      "Epoch : 1914,\ttrain_loss : 0.0055306716822087765\tvalid_loss :0.004038468934595585\n",
      "Adjusting learning rate of group 0 to 3.4769e-05.\n",
      "Adjusting learning rate of group 0 to 3.4769e-05.\n",
      "Epoch : 1915,\ttrain_loss : 0.005530624650418758\tvalid_loss :0.004037944599986076\n",
      "Adjusting learning rate of group 0 to 3.3948e-05.\n",
      "Adjusting learning rate of group 0 to 3.3948e-05.\n",
      "Epoch : 1916,\ttrain_loss : 0.005530387628823519\tvalid_loss :0.004037428647279739\n",
      "Adjusting learning rate of group 0 to 3.3136e-05.\n",
      "Adjusting learning rate of group 0 to 3.3136e-05.\n",
      "Epoch : 1917,\ttrain_loss : 0.005530247930437326\tvalid_loss :0.004037214443087578\n",
      "Adjusting learning rate of group 0 to 3.2334e-05.\n",
      "Adjusting learning rate of group 0 to 3.2334e-05.\n",
      "Epoch : 1918,\ttrain_loss : 0.0055299666710197926\tvalid_loss :0.004038194660097361\n",
      "Adjusting learning rate of group 0 to 3.1541e-05.\n",
      "Adjusting learning rate of group 0 to 3.1541e-05.\n",
      "Epoch : 1919,\ttrain_loss : 0.00553006399422884\tvalid_loss :0.004037098027765751\n",
      "Adjusting learning rate of group 0 to 3.0759e-05.\n",
      "Adjusting learning rate of group 0 to 3.0759e-05.\n",
      "Epoch : 1920,\ttrain_loss : 0.005529546178877354\tvalid_loss :0.004037089645862579\n",
      "Adjusting learning rate of group 0 to 2.9986e-05.\n",
      "Adjusting learning rate of group 0 to 2.9986e-05.\n",
      "Epoch : 1921,\ttrain_loss : 0.005529812537133694\tvalid_loss :0.00403763260692358\n",
      "Adjusting learning rate of group 0 to 2.9223e-05.\n",
      "Adjusting learning rate of group 0 to 2.9223e-05.\n",
      "Epoch : 1922,\ttrain_loss : 0.005529613699764013\tvalid_loss :0.004037472419440746\n",
      "Adjusting learning rate of group 0 to 2.8470e-05.\n",
      "Adjusting learning rate of group 0 to 2.8470e-05.\n",
      "Epoch : 1923,\ttrain_loss : 0.005531446076929569\tvalid_loss :0.004037665668874979\n",
      "Adjusting learning rate of group 0 to 2.7726e-05.\n",
      "Adjusting learning rate of group 0 to 2.7726e-05.\n",
      "Epoch : 1924,\ttrain_loss : 0.005531312432140112\tvalid_loss :0.004038012120872736\n",
      "Adjusting learning rate of group 0 to 2.6993e-05.\n",
      "Adjusting learning rate of group 0 to 2.6993e-05.\n",
      "Epoch : 1925,\ttrain_loss : 0.005530967842787504\tvalid_loss :0.004038040991872549\n",
      "Adjusting learning rate of group 0 to 2.6269e-05.\n",
      "Adjusting learning rate of group 0 to 2.6269e-05.\n",
      "Epoch : 1926,\ttrain_loss : 0.005529443733394146\tvalid_loss :0.0040381895378232\n",
      "Adjusting learning rate of group 0 to 2.5555e-05.\n",
      "Adjusting learning rate of group 0 to 2.5555e-05.\n",
      "Epoch : 1927,\ttrain_loss : 0.0055290027521550655\tvalid_loss :0.004037788137793541\n",
      "Adjusting learning rate of group 0 to 2.4851e-05.\n",
      "Adjusting learning rate of group 0 to 2.4851e-05.\n",
      "Epoch : 1928,\ttrain_loss : 0.005527976434677839\tvalid_loss :0.004037825390696526\n",
      "Adjusting learning rate of group 0 to 2.4156e-05.\n",
      "Adjusting learning rate of group 0 to 2.4156e-05.\n",
      "Epoch : 1929,\ttrain_loss : 0.005527711007744074\tvalid_loss :0.004037638194859028\n",
      "Adjusting learning rate of group 0 to 2.3472e-05.\n",
      "Adjusting learning rate of group 0 to 2.3472e-05.\n",
      "Epoch : 1930,\ttrain_loss : 0.005527386907488108\tvalid_loss :0.0040375967510044575\n",
      "Adjusting learning rate of group 0 to 2.2797e-05.\n",
      "Adjusting learning rate of group 0 to 2.2797e-05.\n",
      "Epoch : 1931,\ttrain_loss : 0.005526750348508358\tvalid_loss :0.004038073122501373\n",
      "Adjusting learning rate of group 0 to 2.2132e-05.\n",
      "Adjusting learning rate of group 0 to 2.2132e-05.\n",
      "Epoch : 1932,\ttrain_loss : 0.0055262320674955845\tvalid_loss :0.004037861712276936\n",
      "Adjusting learning rate of group 0 to 2.1477e-05.\n",
      "Adjusting learning rate of group 0 to 2.1477e-05.\n",
      "Epoch : 1933,\ttrain_loss : 0.005526217166334391\tvalid_loss :0.004037370905280113\n",
      "Adjusting learning rate of group 0 to 2.0831e-05.\n",
      "Adjusting learning rate of group 0 to 2.0831e-05.\n",
      "Epoch : 1934,\ttrain_loss : 0.005526023916900158\tvalid_loss :0.004037807229906321\n",
      "Adjusting learning rate of group 0 to 2.0196e-05.\n",
      "Adjusting learning rate of group 0 to 2.0196e-05.\n",
      "Epoch : 1935,\ttrain_loss : 0.005525592714548111\tvalid_loss :0.0040380386635661125\n",
      "Adjusting learning rate of group 0 to 1.9570e-05.\n",
      "Adjusting learning rate of group 0 to 1.9570e-05.\n",
      "Epoch : 1936,\ttrain_loss : 0.005525111220777035\tvalid_loss :0.004037922713905573\n",
      "Adjusting learning rate of group 0 to 1.8954e-05.\n",
      "Adjusting learning rate of group 0 to 1.8954e-05.\n",
      "Epoch : 1937,\ttrain_loss : 0.005524946842342615\tvalid_loss :0.0040379916317760944\n",
      "Adjusting learning rate of group 0 to 1.8348e-05.\n",
      "Adjusting learning rate of group 0 to 1.8348e-05.\n",
      "Epoch : 1938,\ttrain_loss : 0.00552436662837863\tvalid_loss :0.0040381839498877525\n",
      "Adjusting learning rate of group 0 to 1.7752e-05.\n",
      "Adjusting learning rate of group 0 to 1.7752e-05.\n",
      "Epoch : 1939,\ttrain_loss : 0.005523855797946453\tvalid_loss :0.004038148559629917\n",
      "Adjusting learning rate of group 0 to 1.7166e-05.\n",
      "Adjusting learning rate of group 0 to 1.7166e-05.\n",
      "Epoch : 1940,\ttrain_loss : 0.005523420870304108\tvalid_loss :0.004037757404148579\n",
      "Adjusting learning rate of group 0 to 1.6589e-05.\n",
      "Adjusting learning rate of group 0 to 1.6589e-05.\n",
      "Epoch : 1941,\ttrain_loss : 0.005523486528545618\tvalid_loss :0.0040376996621489525\n",
      "Adjusting learning rate of group 0 to 1.6022e-05.\n",
      "Adjusting learning rate of group 0 to 1.6022e-05.\n",
      "Epoch : 1942,\ttrain_loss : 0.005523153580725193\tvalid_loss :0.004037569742649794\n",
      "Adjusting learning rate of group 0 to 1.5466e-05.\n",
      "Adjusting learning rate of group 0 to 1.5466e-05.\n",
      "Epoch : 1943,\ttrain_loss : 0.005522862076759338\tvalid_loss :0.0040377117693424225\n",
      "Adjusting learning rate of group 0 to 1.4918e-05.\n",
      "Adjusting learning rate of group 0 to 1.4918e-05.\n",
      "Epoch : 1944,\ttrain_loss : 0.005522354040294886\tvalid_loss :0.0040376572869718075\n",
      "Adjusting learning rate of group 0 to 1.4381e-05.\n",
      "Adjusting learning rate of group 0 to 1.4381e-05.\n",
      "Epoch : 1945,\ttrain_loss : 0.005522576160728931\tvalid_loss :0.004037638660520315\n",
      "Adjusting learning rate of group 0 to 1.3854e-05.\n",
      "Adjusting learning rate of group 0 to 1.3854e-05.\n",
      "Epoch : 1946,\ttrain_loss : 0.005522356368601322\tvalid_loss :0.004037846345454454\n",
      "Adjusting learning rate of group 0 to 1.3336e-05.\n",
      "Adjusting learning rate of group 0 to 1.3336e-05.\n",
      "Epoch : 1947,\ttrain_loss : 0.005521601997315884\tvalid_loss :0.004037333652377129\n",
      "Adjusting learning rate of group 0 to 1.2829e-05.\n",
      "Adjusting learning rate of group 0 to 1.2829e-05.\n",
      "Epoch : 1948,\ttrain_loss : 0.005522141698747873\tvalid_loss :0.004038048908114433\n",
      "Adjusting learning rate of group 0 to 1.2331e-05.\n",
      "Adjusting learning rate of group 0 to 1.2331e-05.\n",
      "Epoch : 1949,\ttrain_loss : 0.005521418992429972\tvalid_loss :0.004037730395793915\n",
      "Adjusting learning rate of group 0 to 1.1843e-05.\n",
      "Adjusting learning rate of group 0 to 1.1843e-05.\n",
      "Epoch : 1950,\ttrain_loss : 0.005521361716091633\tvalid_loss :0.0040378570556640625\n",
      "Adjusting learning rate of group 0 to 1.1364e-05.\n",
      "Adjusting learning rate of group 0 to 1.1364e-05.\n",
      "Epoch : 1951,\ttrain_loss : 0.005521112587302923\tvalid_loss :0.004037883598357439\n",
      "Adjusting learning rate of group 0 to 1.0896e-05.\n",
      "Adjusting learning rate of group 0 to 1.0896e-05.\n",
      "Epoch : 1952,\ttrain_loss : 0.0055211191065609455\tvalid_loss :0.0040379855781793594\n",
      "Adjusting learning rate of group 0 to 1.0437e-05.\n",
      "Adjusting learning rate of group 0 to 1.0437e-05.\n",
      "Epoch : 1953,\ttrain_loss : 0.0055208029225468636\tvalid_loss :0.004037985112518072\n",
      "Adjusting learning rate of group 0 to 9.9888e-06.\n",
      "Adjusting learning rate of group 0 to 9.9888e-06.\n",
      "Epoch : 1954,\ttrain_loss : 0.005520605947822332\tvalid_loss :0.004037921316921711\n",
      "Adjusting learning rate of group 0 to 9.5500e-06.\n",
      "Adjusting learning rate of group 0 to 9.5500e-06.\n",
      "Epoch : 1955,\ttrain_loss : 0.005520480684936047\tvalid_loss :0.004038022365421057\n",
      "Adjusting learning rate of group 0 to 9.1210e-06.\n",
      "Adjusting learning rate of group 0 to 9.1210e-06.\n",
      "Epoch : 1956,\ttrain_loss : 0.0055205002427101135\tvalid_loss :0.004038117825984955\n",
      "Adjusting learning rate of group 0 to 8.7018e-06.\n",
      "Adjusting learning rate of group 0 to 8.7018e-06.\n",
      "Epoch : 1957,\ttrain_loss : 0.005520225502550602\tvalid_loss :0.004038060549646616\n",
      "Adjusting learning rate of group 0 to 8.2925e-06.\n",
      "Adjusting learning rate of group 0 to 8.2925e-06.\n",
      "Epoch : 1958,\ttrain_loss : 0.005520109552890062\tvalid_loss :0.004038188140839338\n",
      "Adjusting learning rate of group 0 to 7.8931e-06.\n",
      "Adjusting learning rate of group 0 to 7.8931e-06.\n",
      "Epoch : 1959,\ttrain_loss : 0.00551982456818223\tvalid_loss :0.004038181621581316\n",
      "Adjusting learning rate of group 0 to 7.5035e-06.\n",
      "Adjusting learning rate of group 0 to 7.5035e-06.\n",
      "Epoch : 1960,\ttrain_loss : 0.005519871134310961\tvalid_loss :0.0040381536819040775\n",
      "Adjusting learning rate of group 0 to 7.1237e-06.\n",
      "Adjusting learning rate of group 0 to 7.1237e-06.\n",
      "Epoch : 1961,\ttrain_loss : 0.005519710946828127\tvalid_loss :0.004038350190967321\n",
      "Adjusting learning rate of group 0 to 6.7538e-06.\n",
      "Adjusting learning rate of group 0 to 6.7538e-06.\n",
      "Epoch : 1962,\ttrain_loss : 0.0055195544846355915\tvalid_loss :0.004038453102111816\n",
      "Adjusting learning rate of group 0 to 6.3938e-06.\n",
      "Adjusting learning rate of group 0 to 6.3938e-06.\n",
      "Epoch : 1963,\ttrain_loss : 0.005519480910152197\tvalid_loss :0.004038204904645681\n",
      "Adjusting learning rate of group 0 to 6.0436e-06.\n",
      "Adjusting learning rate of group 0 to 6.0436e-06.\n",
      "Epoch : 1964,\ttrain_loss : 0.005519398488104343\tvalid_loss :0.004038396291434765\n",
      "Adjusting learning rate of group 0 to 5.7033e-06.\n",
      "Adjusting learning rate of group 0 to 5.7033e-06.\n",
      "Epoch : 1965,\ttrain_loss : 0.0055193048901855946\tvalid_loss :0.004038536921143532\n",
      "Adjusting learning rate of group 0 to 5.3728e-06.\n",
      "Adjusting learning rate of group 0 to 5.3728e-06.\n",
      "Epoch : 1966,\ttrain_loss : 0.005519156809896231\tvalid_loss :0.004038730636239052\n",
      "Adjusting learning rate of group 0 to 5.0522e-06.\n",
      "Adjusting learning rate of group 0 to 5.0522e-06.\n",
      "Epoch : 1967,\ttrain_loss : 0.0055191004648804665\tvalid_loss :0.0040383413434028625\n",
      "Adjusting learning rate of group 0 to 4.7414e-06.\n",
      "Adjusting learning rate of group 0 to 4.7414e-06.\n",
      "Epoch : 1968,\ttrain_loss : 0.005519053898751736\tvalid_loss :0.004038560204207897\n",
      "Adjusting learning rate of group 0 to 4.4405e-06.\n",
      "Adjusting learning rate of group 0 to 4.4405e-06.\n",
      "Epoch : 1969,\ttrain_loss : 0.005518909078091383\tvalid_loss :0.004038758110255003\n",
      "Adjusting learning rate of group 0 to 4.1495e-06.\n",
      "Adjusting learning rate of group 0 to 4.1495e-06.\n",
      "Epoch : 1970,\ttrain_loss : 0.0055188280530273914\tvalid_loss :0.0040383958257734776\n",
      "Adjusting learning rate of group 0 to 3.8683e-06.\n",
      "Adjusting learning rate of group 0 to 3.8683e-06.\n",
      "Epoch : 1971,\ttrain_loss : 0.00551880057901144\tvalid_loss :0.004038396291434765\n",
      "Adjusting learning rate of group 0 to 3.5969e-06.\n",
      "Adjusting learning rate of group 0 to 3.5969e-06.\n",
      "Epoch : 1972,\ttrain_loss : 0.005518727004528046\tvalid_loss :0.00403846800327301\n",
      "Adjusting learning rate of group 0 to 3.3355e-06.\n",
      "Adjusting learning rate of group 0 to 3.3355e-06.\n",
      "Epoch : 1973,\ttrain_loss : 0.005518644116818905\tvalid_loss :0.00403869291767478\n",
      "Adjusting learning rate of group 0 to 3.0839e-06.\n",
      "Adjusting learning rate of group 0 to 3.0839e-06.\n",
      "Epoch : 1974,\ttrain_loss : 0.005518547724932432\tvalid_loss :0.004038374405354261\n",
      "Adjusting learning rate of group 0 to 2.8421e-06.\n",
      "Adjusting learning rate of group 0 to 2.8421e-06.\n",
      "Epoch : 1975,\ttrain_loss : 0.00551851699128747\tvalid_loss :0.004038352519273758\n",
      "Adjusting learning rate of group 0 to 2.6102e-06.\n",
      "Adjusting learning rate of group 0 to 2.6102e-06.\n",
      "Epoch : 1976,\ttrain_loss : 0.0055184620432555676\tvalid_loss :0.004038406535983086\n",
      "Adjusting learning rate of group 0 to 2.3882e-06.\n",
      "Adjusting learning rate of group 0 to 2.3882e-06.\n",
      "Epoch : 1977,\ttrain_loss : 0.005518400575965643\tvalid_loss :0.00403874646872282\n",
      "Adjusting learning rate of group 0 to 2.1761e-06.\n",
      "Adjusting learning rate of group 0 to 2.1761e-06.\n",
      "Epoch : 1978,\ttrain_loss : 0.005518362857401371\tvalid_loss :0.004038661252707243\n",
      "Adjusting learning rate of group 0 to 1.9738e-06.\n",
      "Adjusting learning rate of group 0 to 1.9738e-06.\n",
      "Epoch : 1979,\ttrain_loss : 0.005518236663192511\tvalid_loss :0.004038335755467415\n",
      "Adjusting learning rate of group 0 to 1.7813e-06.\n",
      "Adjusting learning rate of group 0 to 1.7813e-06.\n",
      "Epoch : 1980,\ttrain_loss : 0.005518237594515085\tvalid_loss :0.00403832970187068\n",
      "Adjusting learning rate of group 0 to 1.5988e-06.\n",
      "Adjusting learning rate of group 0 to 1.5988e-06.\n",
      "Epoch : 1981,\ttrain_loss : 0.0055182455107569695\tvalid_loss :0.00403873436152935\n",
      "Adjusting learning rate of group 0 to 1.4261e-06.\n",
      "Adjusting learning rate of group 0 to 1.4261e-06.\n",
      "Epoch : 1982,\ttrain_loss : 0.005518119782209396\tvalid_loss :0.004038463346660137\n",
      "Adjusting learning rate of group 0 to 1.2632e-06.\n",
      "Adjusting learning rate of group 0 to 1.2632e-06.\n",
      "Epoch : 1983,\ttrain_loss : 0.005518087651580572\tvalid_loss :0.004038331564515829\n",
      "Adjusting learning rate of group 0 to 1.1103e-06.\n",
      "Adjusting learning rate of group 0 to 1.1103e-06.\n",
      "Epoch : 1984,\ttrain_loss : 0.005518101621419191\tvalid_loss :0.004038472194224596\n",
      "Adjusting learning rate of group 0 to 9.6718e-07.\n",
      "Adjusting learning rate of group 0 to 9.6718e-07.\n",
      "Epoch : 1985,\ttrain_loss : 0.0055180201306939125\tvalid_loss :0.0040384018793702126\n",
      "Adjusting learning rate of group 0 to 8.3395e-07.\n",
      "Adjusting learning rate of group 0 to 8.3395e-07.\n",
      "Epoch : 1986,\ttrain_loss : 0.005518001038581133\tvalid_loss :0.004038508050143719\n",
      "Adjusting learning rate of group 0 to 7.1059e-07.\n",
      "Adjusting learning rate of group 0 to 7.1059e-07.\n",
      "Epoch : 1987,\ttrain_loss : 0.005517960526049137\tvalid_loss :0.0040384321473538876\n",
      "Adjusting learning rate of group 0 to 5.9710e-07.\n",
      "Adjusting learning rate of group 0 to 5.9710e-07.\n",
      "Epoch : 1988,\ttrain_loss : 0.005517961923032999\tvalid_loss :0.00403849221765995\n",
      "Adjusting learning rate of group 0 to 4.9347e-07.\n",
      "Adjusting learning rate of group 0 to 4.9347e-07.\n",
      "Epoch : 1989,\ttrain_loss : 0.005517903249710798\tvalid_loss :0.004038412123918533\n",
      "Adjusting learning rate of group 0 to 3.9971e-07.\n",
      "Adjusting learning rate of group 0 to 3.9971e-07.\n",
      "Epoch : 1990,\ttrain_loss : 0.005517886020243168\tvalid_loss :0.004038380458950996\n",
      "Adjusting learning rate of group 0 to 3.1582e-07.\n",
      "Adjusting learning rate of group 0 to 3.1582e-07.\n",
      "Epoch : 1991,\ttrain_loss : 0.005517878104001284\tvalid_loss :0.004038428887724876\n",
      "Adjusting learning rate of group 0 to 2.4180e-07.\n",
      "Adjusting learning rate of group 0 to 2.4180e-07.\n",
      "Epoch : 1992,\ttrain_loss : 0.005517848767340183\tvalid_loss :0.00403845589607954\n",
      "Adjusting learning rate of group 0 to 1.7765e-07.\n",
      "Adjusting learning rate of group 0 to 1.7765e-07.\n",
      "Epoch : 1993,\ttrain_loss : 0.00551782688125968\tvalid_loss :0.0040384214371442795\n",
      "Adjusting learning rate of group 0 to 1.2337e-07.\n",
      "Adjusting learning rate of group 0 to 1.2337e-07.\n",
      "Epoch : 1994,\ttrain_loss : 0.005517815239727497\tvalid_loss :0.004038420971482992\n",
      "Adjusting learning rate of group 0 to 7.8957e-08.\n",
      "Adjusting learning rate of group 0 to 7.8957e-08.\n",
      "Epoch : 1995,\ttrain_loss : 0.0055178008042275906\tvalid_loss :0.0040384093299508095\n",
      "Adjusting learning rate of group 0 to 4.4413e-08.\n",
      "Adjusting learning rate of group 0 to 4.4413e-08.\n",
      "Epoch : 1996,\ttrain_loss : 0.0055177947506308556\tvalid_loss :0.004038417711853981\n",
      "Adjusting learning rate of group 0 to 1.9739e-08.\n",
      "Adjusting learning rate of group 0 to 1.9739e-08.\n",
      "Epoch : 1997,\ttrain_loss : 0.005517785903066397\tvalid_loss :0.004038418643176556\n",
      "Adjusting learning rate of group 0 to 4.9348e-09.\n",
      "Adjusting learning rate of group 0 to 4.9348e-09.\n",
      "Epoch : 1998,\ttrain_loss : 0.005517781246453524\tvalid_loss :0.004038420971482992\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Epoch : 1999,\ttrain_loss : 0.005517779849469662\tvalid_loss :0.0040384214371442795\n"
     ]
    }
   ],
   "source": [
    "input_size = 4\n",
    "hidden_size = 4\n",
    "num_layer = 1\n",
    "output_size = 2\n",
    "\n",
    "# encoder = EncoderRNN(input_size, hidden_size, num_layer, batch_size)\n",
    "# decoder = DecoderRNN(output_size, hidden_size, num_layer, batch_size)\n",
    "\n",
    "epoch = 2000\n",
    "learning_rate = 0.008\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size, num_layer, batch_size)\n",
    "decoder = DecoderRNN(output_size, hidden_size, num_layer, batch_size)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "# embeder.to(device)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "# scheduler1 = optim.lr_scheduler.StepLR(encoder_optimizer,step_size=500,gamma=0.6,verbose=True)\n",
    "# scheduler2 = optim.lr_scheduler.StepLR(decoder_optimizer,step_size=500,gamma=0.6,verbose=True)\n",
    "\n",
    "\n",
    "scheduler1 = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer,T_max=epoch,verbose=True)\n",
    "scheduler2 = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer,T_max=epoch,verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# criterion = mse_loss\n",
    "criterion = nn.L1Loss()\n",
    "# Save training loss\n",
    "train_loss = torch.zeros(epoch)\n",
    "valid_loss_min = np.Inf\n",
    "# Save validation loss\n",
    "valid_loss = torch.zeros(epoch)\n",
    "for e in trange(epoch):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        encoder_hidden = encoder.initHidden(num_layer,label.shape[0])\n",
    "        encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs = torch.zeros(in_window, label.shape[0], encoder.hidden_size).to(device)\n",
    "\n",
    "        # Encoder\n",
    "        for ei in range(in_window): #input_window_size 만큼\n",
    "            encoder_output, encoder_hidden = encoder(data[:,ei,:].reshape(label.shape[0],1,-1), encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output.squeeze()\n",
    "\n",
    "        # Decoder_init\n",
    "        d_input = torch.zeros((label.shape[0],1,2)).to(device)\n",
    "        decoder_input = d_input\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Decoder\n",
    "        for di in range(out_window): # input_window_size + output_window_size 만큼\n",
    "            decoder_output, decoder_hidden, attn_weigths = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            loss=criterion(decoder_output, label[:,di,:].reshape(label.shape[0],1,-1))\n",
    "            decoder_input = label[:,di,:].reshape(label.shape[0],1,-1) #if teacher_force else decoder_output\n",
    "            train_loss[e] += loss.item()\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    train_loss[e] /= len(train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        for data, label in valid_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            encoder_hidden = encoder.initHidden(num_layer,label.shape[0])\n",
    "            encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            encoder_outputs = torch.zeros(in_window, label.shape[0], encoder.hidden_size).to(device)\n",
    "\n",
    "            # Encoder\n",
    "            for ei in range(in_window): #input_window_size 만큼\n",
    "                encoder_output, encoder_hidden = encoder(data[:,ei,:].reshape(label.shape[0],1,-1), encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output.squeeze()\n",
    "\n",
    "            # Decoder_init\n",
    "            d_input = torch.zeros((label.shape[0],1,2)).to(device)\n",
    "            decoder_input = d_input\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            # Decoder\n",
    "            for di in range(out_window): # input_window_size + output_window_size 만큼\n",
    "                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "                loss=criterion(decoder_output, label[:,di,:].reshape(label.shape[0],1,-1))\n",
    "                decoder_input = label[:,di,:].reshape(label.shape[0],1,-1) #if teacher_force else decoder_output\n",
    "                valid_loss[e] += loss.item()\n",
    "        valid_loss[e] /= len(valid_loader)\n",
    "        scheduler1.step()\n",
    "        scheduler2.step()\n",
    "    print(f'Epoch : {e},\\ttrain_loss : {train_loss[e]}\\tvalid_loss :{valid_loss[e]}')\n",
    "    if (valid_loss[e] < valid_loss_min):\n",
    "        print(f'**********Valid loss decreased ({valid_loss_min:.6f} ==> {valid_loss[e]:.6f})**********')\n",
    "        valid_loss_min = valid_loss[e]\n",
    "        torch.save(encoder.state_dict(),'Encoder.pt')\n",
    "        torch.save(decoder.state_dict(),'Decoder.pt')\n",
    "        # torch.save(encoder.state_dict(),'Encoder_hype{}.pt'.format(hype))\n",
    "        # torch.save(decoder.state_dict(),'Decoder_hype{}.pt'.format(hype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619d7b4-c26a-4b13-a714-2bde6fc0b137",
   "metadata": {},
   "source": [
    "## Test & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1760,
   "id": "6ad7bbe7-d14f-468d-85f1-c94aed0d903d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2502dcd0e20>]"
      ]
     },
     "execution_count": 1760,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAh9klEQVR4nO3deZhcdZ3v8fe3qrfsIUkDIQkm0QD2VZHYRlBxRSWIxuUZBVTUmbmRZ+QqXhlv1Os8471zvS6IyhWJCHEEFxDEMWogGCRAHCDpkBDShJDO3tm6sy+9VFfV9/5xTjfV1dXdpzu9cerzep5+UnXO75z61unOp371O5u5OyIiEl+J4S5AREQGl4JeRCTmFPQiIjGnoBcRiTkFvYhIzJUMdwGFTJkyxWfOnDncZYiIvGysXbv2oLtXFpo3IoN+5syZ1NTUDHcZIiIvG2a2s7t5GroREYk5Bb2ISMwp6EVEYk5BLyIScwp6EZGYU9CLiMScgl5EJOZiFfS3PLKFx15sHO4yRERGlFgF/W0rt7Jqi4JeRCRXrII+YZDVfVRERDqJV9AnjIySXkSkk1gFfTJh6NaIIiKdxSroE2ZkFPQiIp3EL+izw12FiMjIEqugTybQ0I2ISJ5YBX3Qo1fQi4jkil/Qq0cvItJJpKA3s8vNbLOZ1ZnZogLzLzCzJ82s1cxuLDA/aWbrzOxPA1F0d4KjbgbzFUREXn56DXozSwK3AvOBKuBqM6vKa3YY+AJwUzer+SKw6TTqjCRhaOhGRCRPlB79PKDO3be5ewq4B1iQ28DdG9x9DdCWv7CZTQfeD9wxAPX2KJHQ0I2ISL4oQT8N2J3zvD6cFtUPga8APR74aGYLzazGzGoaG/t3vZqE6YQpEZF8UYLeCkyLlKZmdiXQ4O5re2vr7re7e7W7V1dWVkZZfRdJHXUjItJFlKCvB2bkPJ8O7I24/rcAHzSzHQRDPu8ys1/2qcI+SCRMFzUTEckTJejXAHPMbJaZlQFXAUujrNzdv+ru0919ZrjcX939k/2uthcJg6ySXkSkk5LeGrh72syuB5YDSWCJu9ea2XXh/MVmdjZQA4wHsmZ2A1Dl7scHr/SuktoZKyLSRa9BD+Duy4BledMW5zzeTzCk09M6VgIr+1xhHyRMQzciIvlidmashm5ERPLFKuiTuvGIiEgXsQr6YOhGQS8ikktBLyISc7EKeg3diIh0FaugN0NH3YiI5IlV0CcTGroREckXr6DXGL2ISBexCnrTzcFFRLqIVdAnEzphSkQkX8yCXkM3IiL5YhX0ppuDi4h0EaugL08mSKU1SC8ikitWQV9RlqSlLTPcZYiIjCjxCvqSJC1t6tGLiOSKV9CXJtSjFxHJE7OgT5LOOm06mF5EpEPMgj54O+rVi4i8JFZBP6o0CaBxehGRHJGC3swuN7PNZlZnZosKzL/AzJ40s1YzuzFn+gwze9TMNplZrZl9cSCLz1feEfTq0YuItOv15uBmlgRuBd4D1ANrzGypuz+f0+ww8AXgQ3mLp4Evu/szZjYOWGtmf8lbdsBUhEHfmlbQi4i0i9KjnwfUufs2d08B9wALchu4e4O7rwHa8qbvc/dnwscngE3AtAGpvICKkuDtNKc0dCMi0i5K0E8Dduc8r6cfYW1mM4GLgKe7mb/QzGrMrKaxsbGvqwdgVFk4dKMevYhIhyhBbwWm9emCMmY2FvgdcIO7Hy/Uxt1vd/dqd6+urKzsy+o7VGiMXkSkiyhBXw/MyHk+Hdgb9QXMrJQg5H/l7g/0rby+qSjRUTciIvmiBP0aYI6ZzTKzMuAqYGmUlZuZAXcCm9z95v6XGU37cfTN6tGLiHTo9agbd0+b2fXAciAJLHH3WjO7Lpy/2MzOBmqA8UDWzG4AqoDXAZ8CnjOz9eEqv+buywb8naChGxGRQnoNeoAwmJflTVuc83g/wZBOvlUUHuMfFB2HVyroRUQ6xOrM2JcugaAxehGRdjEL+qBHrzF6EZGXxCroS5MJShKmMXoRkRyxCnoIevUauhEReUkMgz6hM2NFRHLELujLS3TfWBGRXLEL+lG6QbiISCexC/rgvrEaoxcRaRe/oNfQjYhIJ/EL+lIFvYhIrlgGfbOGbkREOsQw6BO61o2ISI4YBr2GbkREcsUw6BO0pDV0IyLSLnZBX5JI0JZR0IuItItd0JcmjXSmT7e0FRGJtdgFfXNbhua2jMbpRURCsQv6Xz61C4D71tYPcyUiIiNDpKA3s8vNbLOZ1ZnZogLzLzCzJ82s1cxu7Muyg8VdwzciIhAh6M0sCdwKzCe44ffVZlaV1+ww8AXgpn4sOyiU8yIigSg9+nlAnbtvc/cUcA+wILeBuze4+xqgra/LiojI4IoS9NOA3TnP68NpUURe1swWmlmNmdU0NjZGXH1X17zpXAAmjy3r9zpEROIkStBbgWlRB0YiL+vut7t7tbtXV1ZWRlx9V9fMC4K+NBm7/cwiIv0SJQ3rgRk5z6cDeyOu/3SW7Zf2gM9kNUgvIgLRgn4NMMfMZplZGXAVsDTi+k9n2X5JJoIvETo7VkQkUNJbA3dPm9n1wHIgCSxx91ozuy6cv9jMzgZqgPFA1sxuAKrc/XihZQfpvQDBmbGgHr2ISLtegx7A3ZcBy/KmLc55vJ9gWCbSsoOpvUevyyCIiARit8eyJBG8pbR69CIiQByDvmPoRmP0IiIQx6Dv2BmrHr2ICMQx6HV4pYhIJ/EL+vYevYZuRESAGAf973SZYhERIIZB33545dbGU8NciYjIyBC7oDcrdHkdEZHiFbugb3fhjInDXYKIyIgQ6czYl5u5505kTHks35qISJ/FskefTJgugSAiEopt0Gd0L0ERESCmQV+SSOiEKRGRUCyDPpEwXdRMRCQUy6AvSRhZBb2ICBDToE8mjNZ0ZrjLEBEZEWIZ9KPLkuw+3DzcZYiIjAixDPpzJ42muS2j4RsRESIGvZldbmabzazOzBYVmG9mdks4f4OZzc2Z9yUzqzWzjWb2GzOrGMg3UEhFaRKAlG4QLiLSe9CbWRK4FZgPVAFXm1lVXrP5wJzwZyFwW7jsNOALQLW7v4bgBuFXDVj13SgvCd5Wm4JeRCRSj34eUOfu29w9BdwDLMhrswC4ywNPARPNbGo4rwQYZWYlwGhg7wDV3q3S8OYjqbSCXkQkStBPA3bnPK8Pp/Xaxt33ADcBu4B9wDF3f7jQi5jZQjOrMbOaxsbGqPUXtGbHYQAeqt1/WusREYmDKEFf6Lq/+Xs5C7YxszMIevuzgHOAMWb2yUIv4u63u3u1u1dXVlZGKKt7Z4wuA+DgidRprUdEJA6iBH09MCPn+XS6Dr901+YyYLu7N7p7G/AA8Ob+lxvNwrfNBuCciYO+31dEZMSLEvRrgDlmNsvMygh2pi7Na7MUuDY8+uZigiGafQRDNheb2WgL7gjybmDTANZf0NjwEsWnWtOD/VIiIiNerxdtd/e0mV0PLCc4amaJu9ea2XXh/MXAMuAKoA5oAj4bznvazO4HngHSwDrg9sF4I7lGlweHV+4/3jrYLyUiMuKZj8DL+VZXV3tNTU2/l3d3Zn11GQA7vv3+gSpLRGTEMrO17l5daF4sz4zVfWNFRF4Sy6DPNRK/sYiIDKXYB32rTpoSkSIX+6DXkTciUuxiG/Q3/d2FADSldF16ESlusQ36MWXBIZanUurRi0hxi23Qj+44aUo9ehEpbrEN+rHhSVMaoxeRYhfboB9dFvTomzR0IyJFLrZBP6ZMQzciIhDjoG+/3o12xopIsYtt0I/VzlgRESDGQV9ekiBhGqMXEYlt0JsZY8pK1KMXkaIX26AHGFNeosMrRaToxTroR5cltTNWRIperIO+vDRJSlevFJEiF++gL0noMsUiUvQiBb2ZXW5mm82szswWFZhvZnZLOH+Dmc3NmTfRzO43sxfMbJOZXTKQb6AnQdBrZ6yIFLdeg97MksCtwHygCrjazKryms0H5oQ/C4Hbcub9CHjI3S8ALgQ2DUDdkZSXJtWjF5GiF6VHPw+oc/dt7p4C7gEW5LVZANzlgaeAiWY21czGA28D7gRw95S7Hx248ntWlkzQpMMrRaTIRQn6acDunOf14bQobWYDjcDPzWydmd1hZmNOo94+WbHpAJsPnKDxROtQvaSIyIgTJeitwLT8O25316YEmAvc5u4XAaeALmP8AGa20MxqzKymsbExQlnR7T3aPKDrExF5OYkS9PXAjJzn04G9EdvUA/Xu/nQ4/X6C4O/C3W9392p3r66srIxSe2TJRKHPIRGR4hAl6NcAc8xslpmVAVcBS/PaLAWuDY++uRg45u773H0/sNvMzg/bvRt4fqCKj8rzv3+IiBSRkt4auHvazK4HlgNJYIm715rZdeH8xcAy4AqgDmgCPpuziv8G/Cr8kNiWN29QXTx7Ek9tO0wqox2yIlK8eg16AHdfRhDmudMW5zx24PPdLLseqO5/if13w2XncdXtT9HapkMsRaR4xfrM2Pax+ayGbkSkiMU66Nv3wWY0SC8iRSzWQW/W3qNX0ItI8Yp10CfDoHcFvYgUsVgHfaK9R699sSJSxGId9GHOa+hGRIparIO+o0evnBeRIhbvoA/fncboRaSYxTvo1aMXEYl70Af/aoxeRIpZrINex9GLiMQ86BMdx9EPcyEiIsMo5kEf/KsevYgUs5gHvXbGiojEOug7TphS0otIEYt10Ce0M1ZEJN5Br+vRi4jEPOh1rRsRkZgHfUKXKRYRiRb0Zna5mW02szozW1RgvpnZLeH8DWY2N29+0szWmdmfBqrwKHTUjYhIhKA3syRwKzAfqAKuNrOqvGbzgTnhz0Lgtrz5XwQ2nXa1faTj6EVEovXo5wF17r7N3VPAPcCCvDYLgLs88BQw0cymApjZdOD9wB0DWHck7ZdA+OYfnx/qlxYRGTGiBP00YHfO8/pwWtQ2PwS+AvR4nyczW2hmNWZW09jYGKGs3rX36EVEilmUoC8Ul/ljIQXbmNmVQIO7r+3tRdz9dnevdvfqysrKCGX1rn2MXkSkmEUJ+npgRs7z6cDeiG3eAnzQzHYQDPm8y8x+2e9q+0hBLyISLejXAHPMbJaZlQFXAUvz2iwFrg2PvrkYOObu+9z9q+4+3d1nhsv91d0/OZBvoCfKeRERKOmtgbunzex6YDmQBJa4e62ZXRfOXwwsA64A6oAm4LODV3J06tGLiEQIegB3X0YQ5rnTFuc8duDzvaxjJbCyzxWeBu2MFREpkjNjRUSKWbyDXl16EZF4B72IiCjoRURiT0EvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxVzRB/+Bz+4a7BBGRYVE0Qf9nBb2IFKmiCfojTanhLkFEZFgUTdD/re7QcJcgIjIsiiboRUSKlYJeRCTmFPQiIjGnoBcRiTkFvYhIzEUKejO73Mw2m1mdmS0qMN/M7JZw/gYzmxtOn2Fmj5rZJjOrNbMvDvQbEBGRnvUa9GaWBG4F5gNVwNVmVpXXbD4wJ/xZCNwWTk8DX3b3VwMXA58vsOyg0t0ERaTYRenRzwPq3H2bu6eAe4AFeW0WAHd54ClgoplNdfd97v4MgLufADYB0waw/l65D+WriYiMPFGCfhqwO+d5PV3Dutc2ZjYTuAh4utCLmNlCM6sxs5rGxsYIZYmISBRRgr7Q4Ed+P7nHNmY2FvgdcIO7Hy/0Iu5+u7tXu3t1ZWVlhLJERCSKKEFfD8zIeT4d2Bu1jZmVEoT8r9z9gf6XKiIi/REl6NcAc8xslpmVAVcBS/PaLAWuDY++uRg45u77zMyAO4FN7n7zgFYuIiKR9Br07p4GrgeWE+xM/a2715rZdWZ2XdhsGbANqAN+BvxTOP0twKeAd5nZ+vDnioF+E6frRyu28I+/qBnuMkREBkVJlEbuvowgzHOnLc557MDnCyy3isLj98Nia+NJXlk5tsv0H6x4cRiqEREZGkV1Zuy7v//YcJcgIjLkiiroRUSKUdEF/fGWNlrTGe5ZvYtsdvDOpjrZmmbjnmODtn4RkahiH/Q/+PiFnZ6v2X6Ynz62jUUPPMfsry3j0Rcaul32WHMbX7p3Pcea23p8DXdn5eYGMjkfHJ+7u4Yr/98qUuns6b0BEZHTFPug//BF0zl30uiO51sbT3YK7m8t29TxeO/RZgAOnmzlJyvrWLJqO79ft4c7n9jW0SadybLlwIlOr/HIpgY+8/M13P74S+1qdhwJ2me7Bv3anYf5/br603xnIiLRxD7oAWZNGdPx+FvLXuAvzx8o2G5DfTDU8s/3Pct3H9rMc+HQyx837AOgpS3D+d94iPf84HG2HzzVsdz+4y0A7Drc1GWdF37zYQCeqz/Gj/+6BYCP3vYkX7r32dN9WwPm7qd2Mv9HTwx3GSIySIoi6CvHlXd6nhvIWxpOdpr3vh88zqObg2vtZMMrom0/eAp355ZHtnQMzxw82drldczg3jW7aDzx0ry2TND+Az9exU0Pv4h75+GdzftP8N2HXqCu4SSt6UyXdW4/eIqZi/5MzY7DfXrPADsOnuLS7/6V/cdaemz3jf/YyKZ9wZUp6hpOsqYfryUiI1ek4+hf7ubNmsT9a3sfKlmyajubc4ZlVm5+6eJqqUy205BPosDZAfVHmvn107uYMnZzt6/xtd8/1/F4ee0BltcG3y5+snIro8uS3HFtNalMls/8fA0AF507EYC7ntzJD1dsYVXdQa5507l868Ov7fX9/Hr1LnYfbuaBdfX80zte1Wv7bNa57ObgENTH/vkdvGLymF6WEJGXg6Lo0f/dG6Yz/zVn99pudQ892bU7j7B0/UuX+Llt5VZufbSO1dsPc/NfghOuUmGP/ODJFK05O2G/89ALHY9/szr3Ip+dNaUyXHPH0x0hD7Bu11EAlj67l1V1BwH49dO7Ci6fSmep3RsMNz1Xf6xjn8GTWw/xwxUvcuRUKu89He60v6Ex51vK27+3ksN57fsim3X+Y90e0pmu+yhOtqY51tTzDm4RGThF0aM3M2775BuYuejP/V7HNT/rfHXlFZsaWLGp8xE73V37/raVW/v9ut1JZ7LsOdrMWeMr+NK967ns1WexvHY/Dz9/gCe+8k4+8ONVHW2f2HKQJ7YcZPX2w9TuPc6x5jZq/udlfPS2JzutM3dnMsCJljaSCeNUa5pzJo7qUsON9z3LrkNNLP7UG5j7v//CzR+7kI/MnQ7AA+v2cON9z9JwooUrX3cO33noBb7z0ddRUZrkzf/3EY63pNnx7fcP+HYRka6Kokff7qNhCA2Wp7cP3dj2N//4PG//3ko+/tMneXDjfr5837M8HO5k3l1gpzDAf2491DH8VP1vK7rMv3PV9k7Psx7sTH7zt//Kw7X7O817/MVG7l9bz+odh3lwY7Cz+pdP7eyYfyj8drBiUwPf/GMtf1i/t+NQ1uMt6aCeuoO05fX4H93c0OMw24HjLcxc9GdWdLND/dsPvsD63Ue7XV6kGBVV0H//Yxey9VtX8JGLpnHh9AnDXc5puTsM1Wfru56Udc0dBe/t0menWtMdjxfevZZVWw6y71gzzakM1y5Z3TGvfXjp7AkVHdPqwp3cq7cf5nhzsB7Lu6/jNXc8zS2PbOl4/p91B/nsz9dw431dj0hyd+54YhtPbTsEwD/eFVyE7icr67j4W49Q13CCtkyWxY9t5UO3/u103rZI7BTF0E2uZMK4+eOvJ5N1vv/wZs6eUMFva3bz46vn8o6bVg53eSPKtpxDSAE+eefTTBpT1mXsvjQZBPiy5/bTcLyFr/xuQ6cd2RvD/QaFdmA/ufUQ8/7PCpZ85o184w8bO6bXNZzknIkVjC4rYXntfj5399ouy+461MR3Hwp2fF928+Od5h043sIzO49wz5rdPPZiI+++4Eze95qzedOsSbz9eyuZfsYofvH386gcV86o0iSlya59npa2DI+/2Mh7/0vh/Tut6QzlJUkg2CcyYVQplePKOdbc1ulD7941u5g4uoz35a0nm3WONKWYPLbzUWEiA818BN5Utbq62mtqhu+ywXUNJ5k2cRQlSePBjfs5eKKV//Wn54etnjj5wrtexS1/resy/crXTeVP4fkKuX77uUv42E+f7DJ9sFwyezJNqTSvOnMcWXd+v24Pt1x9EV/4zToA/v2zb+SSV07mK/dv4A/hzvnr3/kqfvxo5/c0u3IMjSdamXvuGTz2YvCh940rq2jLZPnL8wfY2niSo+EO6fPOGsu0iaN465xKfrN6F9+4sopZk8cwdWJFlw+g5lSGitJEl29HIma21t2rC85T0Ed3sjXNc/XHaG5LU16S5BM5QyRTJ1Rw6yfm8i9/2MjGPV3vlnjmuHIaTnQ99j6qkoSRHsRr88jI9Jk3z2TB68/hVGuGBzfu41fhEVelSes4R2PuuRN50+zJjK8o5UhTijfOnMQFZ49j0pgyWtNZ9h5tZlxFCTsONXHRuRM5eqqNb/6xljfMPIO3vHIKE0aV0tyW4VhzGxv3HONNsyZzpCnF6LIkf6s7xNiKEqZNrOBUa4Z0NssbZ05ia+MpDp1s5TXTJnDGmDJOtaY5e0IFp1rTTB5Tjlnwbc0J9hmdOa6cCaNKGVWWpKI0SVkywYxJo0mls5hBeUnnD6/2XNIHWnQK+iHk7vxpwz7eU3UWTakMLW0Z0hlnVFmSRzYdoHJcOa+dPoGWVJZfr97Fp9/8CtrSzt+2HuSnj23l6++v4vyzxnH/2t1c/pqp/K3uIIdOpbjxvefxhn9bQXNbcAhn+zV0/uuls3jn+WfymZ+vYWxFScewytXzZvDgxv1MmziKo01tvGn2JHYeamLtziMF677g7HG8sP8EV7z2bJY9F+x4/Ye3zuqygzbXlLFlHDwZvN7pfpANlXkzJ/V4GK0Mj4RBSTJBNutk3HEPOjeJhFGeTGAWhH7COoe/ARl3mlIZpowpA4KbVbsHJzxm3TEzMlnHCM6HcYeSpFGWTFCaTJBMGIU+T4yXXqvbjxs7vRtu5H+QTRpdxm+vu6S/61LQS8DDP/y+ymYds+A/UGs6SyIBCTPaMllGlSY71tnSliGVyTK+ohQIDtHc0nCSV0waTXlpMvgPZ3CiJU15SYKdh5qomjqePUeDnbwXTB3Hz57YxntefRYb6o8xZVw5VVPHc7QpxeYDJyhLJrj4lZNpbcvyh/V7eN30iRw43sLY8hJmV47hZGuaWVPGMKo0yfGWNBNGlZJKZ3l+33EunD6h4Hvfc7SZ1nA8vuqcCZSXJKjZeYRDJ1t5+3mV7DzUxKXnTWFseQm/fnoXsyvHMm/WJMZXlFB/pJndR5rY2nCSK147tWO8fUP9UdbtOsr815zNmeMrOrZ7S1smeO+lCTbuOcZ5Z42jrCTB2LISUpksFeE2SqWzPFS7j8c2N/LGWZNoON7K6LIkV807l+0HTzFhVCkb6o9yrLmNVDpLwoxtB0/yx2f38crKMcyuHMs5EyrY2niKfceaqShNcrI1zdvPq+RIU4rdh5sZU55kweuncbIlTWs6S3lJgmd2HaFyXDmTw9Cs2XmEqRNGcemcKSyv3c/RpjbecX4lZpDNwpGmFC1tGZpSGc4YXcaT2w6RMGP6GaOYPKaMebMmsfdYcA2pkkSCEy1pRpUlONWa4eDJVowgvFvTWdqyWZJmJBNGwoxUJkvWnda2bMffbv6X2qw7TvCN4HhzOvwgCOa1rycbfmhk3SkrSWAY6WyWVDpLW8bJFLgeVfuHRfvjQv93PHztfiuw8LiKEr790df1a3UKehGRmOsp6CMdXmlml5vZZjOrM7NFBeabmd0Szt9gZnOjLisiIoOr16A3syRwKzAfqAKuNrOqvGbzgTnhz0Lgtj4sKyIigyhKj34eUOfu29w9BdwDLMhrswC4ywNPARPNbGrEZUVEZBBFCfppQO6VuOrDaVHaRFkWADNbaGY1ZlbT2NhYqImIiPRDlKAvdIhG/h7c7tpEWTaY6H67u1e7e3VlZWWEskREJIool0CoB2bkPJ8O7I3YpizCsiIiMoii9OjXAHPMbJaZlQFXAUvz2iwFrg2PvrkYOObu+yIuKyIig6jXHr27p83semA5kASWuHutmV0Xzl8MLAOuAOqAJuCzPS07KO9EREQKGpEnTJlZI7Cz14aFTQEODmA5A0V19Y3q6hvV1TdxrOsV7l5wB+eIDPrTYWY13Z0dNpxUV9+orr5RXX1TbHUV1Y1HRESKkYJeRCTm4hj0tw93Ad1QXX2juvpGdfVNUdUVuzF6ERHpLI49ehERyaGgFxGJudgE/XBe997MZpjZo2a2ycxqzeyL4fR/NbM9ZrY+/LkiZ5mvhrVuNrP3DWJtO8zsufD1a8Jpk8zsL2a2Jfz3jKGsy8zOz9km683suJndMBzby8yWmFmDmW3Mmdbn7WNmbwi3c114b4bTutlpN3V9z8xeCO/58HszmxhOn2lmzTnbbfFg1dVDbX3+3Q3RNrs3p6YdZrY+nD4k26yHbBjavzF3f9n/EJx1uxWYTXB9nWeBqiF8/anA3PDxOOBFguvv/ytwY4H2VWGN5cCssPbkINW2A5iSN+27wKLw8SLgO0NdV97vbj/wiuHYXsDbgLnAxtPZPsBq4BKCC/k9CMwfhLreC5SEj7+TU9fM3HZ56xnQunqorc+/u6HYZnnzvw/8y1BuM7rPhiH9G4tLj35Yr3vv7vvc/Znw8QlgE91cjjm0ALjH3VvdfTvBpSPmDX6lnV7/F+HjXwAfGsa63g1sdfeezoQetLrc/XEg/27hfdo+Ftx7Yby7P+nB/8i7cpYZsLrc/WF3T4dPnyK4SGC3BqOu7mrrwbBus3Zh7/djwG96WsdA19VDNgzp31hcgj7yde8Hm5nNBC4Cng4nXR9+1V6S8/VsKOt14GEzW2tmC8NpZ3lw0TnCf88chrraXUXn/3zDvb2g79tnWvh4qOoD+HuCXl27WWa2zsweM7NLw2lDXVdffndDXdulwAF335IzbUi3WV42DOnfWFyCPvJ17we1CLOxwO+AG9z9OMEtFV8JvB7YR/DVEYa23re4+1yC2zl+3sze1kPbId2OFlzR9IPAfeGkkbC9enLa910YkCLMvg6kgV+Fk/YB57r7RcB/B35tZuOHuK6+/u6G+nd6NZ07FEO6zQpkQ7dNu3n906orLkEf5Zr5g8rMSgl+kb9y9wcA3P2Au2fcPQv8jJeGG4asXnffG/7bAPw+rOFA+FWw/atqw1DXFZoPPOPuB8Iah317hfq6ferpPIwyaPWZ2aeBK4FPhF/hCb/mHwofryUY1z1vKOvqx+9uKLdZCfAR4N6ceodsmxXKBob4bywuQT+s170Px//uBDa5+80506fmNPsw0H40wFLgKjMrN7NZBDdVXz0IdY0xs3Htjwl25m0MX//TYbNPA38YyrpydOplDff2ytGn7RN+9T5hZheHfwvX5iwzYMzscuB/AB9096ac6ZVmlgwfzw7r2jZUdYWv26ff3VDWBlwGvODuHUMfQ7XNussGhvpvrL97k0faD8H18F8k+GT++hC/9lsJvkZtANaHP1cAdwPPhdOXAlNzlvl6WOtmBuBIiG7qmk2wB/9ZoLZ9uwCTgUeALeG/k4ayrvB1RgOHgAk504Z8exF80OwD2gh6Tf/Qn+0DVBOE21bgx4RnnQ9wXXUE47ftf2OLw7YfDX+/zwLPAB8YrLp6qK3Pv7uh2Gbh9H8HrstrOyTbjO6zYUj/xnQJBBGRmIvL0I2IiHRDQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRibn/D2hTpYzJ4NF0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1761,
   "id": "48ec3f9e-5efe-4b13-826e-a0cbd6f1f434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2502f06f1f0>]"
      ]
     },
     "execution_count": 1761,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "id": "9eaabf8e-82ea-41d2-954f-a27ebbba56ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2502f0d1bb0>]"
      ]
     },
     "execution_count": 1762,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_loss)\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1763,
   "id": "ddcce7e9-94fb-4329-a1df-ee416f4d24e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (emb): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (attn): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (attn_combine): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (gru): GRU(4, 4, batch_first=True)\n",
       "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (ln): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder = EncoderRNN(input_size, 11, num_layer, batch_size)\n",
    "# decoder = DecoderRNN(output_size, 11, num_layer, batch_size)\n",
    "\n",
    "encoder.load_state_dict(torch.load('./Encoder.pt'))\n",
    "decoder.load_state_dict(torch.load('./Decoder.pt'))\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "id": "15b1805a-7e67-4ff9-9fc4-628a5f777871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_1 = next(iter(train_loader))[0][1,:,:]\n",
    "# test_label_1 = next(iter(train_loader))[1][1,:,:]\n",
    "# test_data_1 = test_data_1.reshape(1,4,-1)\n",
    "# test_label_1 = test_label_1.reshape(1,6,-1)\n",
    "# test_data_2 = next(iter(train_loader))[0][2,:,:]\n",
    "# test_label_2 = next(iter(train_loader))[1][2,:,:]\n",
    "# test_data_2 = test_data_2.reshape(1,4,-1)\n",
    "# test_label_2 = test_label_2.reshape(1,6,-1)\n",
    "# test_data_3 = next(iter(train_loader))[0][3,:,:]\n",
    "# test_label_3 = next(iter(train_loader))[1][3,:,:]\n",
    "# test_data_3 = test_data_3.reshape(1,4,-1)\n",
    "# test_label_3 = test_label_3.reshape(1,6,-1)\n",
    "\n",
    "# test_data_1 = test_data_1.to(device)\n",
    "# hid = encoder.initHidden(2,1)\n",
    "# hid = hid.to(device)\n",
    "# pred_path = []\n",
    "# for i in range(in_window):\n",
    "#     out, hidden = encoder(test_data_1[:,i,:].reshape(1,1,-1), hid)\n",
    "# d_input = torch.zeros((1,1,2)).to(device)\n",
    "# # d_input = test_label_1[:,0,:].reshape(1,1,-1).to(device)\n",
    "# decoder_input = d_input\n",
    "# decoder_hidden = hidden\n",
    "\n",
    "# for i in range(in_window):\n",
    "#     decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "#     decoder_input = test_label_1[:,i,:].reshape(1,1,-1).to(device)\n",
    "#     pred_path.append(decoder_input)\n",
    "    \n",
    "# for i in range(out_window):\n",
    "#     decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "#     decoder_input = decoder_output\n",
    "#     pred_path.append(decoder_output)\n",
    "\n",
    "\n",
    "# pred_path, test_label_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ab54-cbe4-45eb-9578-54ed4751642c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "id": "7b034c49-e06f-48fd-a6c0-fa1bf27624a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hin_X = Hin.copy()\n",
    "Hin_Y = pd.concat([Hin['위도(N)'],Hin['경도(E)']],axis=1)\n",
    "\n",
    "Megi_X = Megi.copy()\n",
    "Megi_Y = pd.concat([Megi['위도(N)'],Megi['경도(E)']],axis=1)\n",
    "\n",
    "Hin_real = Hin_Y.copy()\n",
    "Megi_real = Megi_Y.copy()\n",
    "\n",
    "for j in ['위도(N)','경도(E)','중심기압','초속(m/s)','시속(km/h)','강풍반경(km)[예외반경]','이동속도(km/h)']:\n",
    "        Hin_X[j] = (Hin_X[j] - minmax[j][0])/(minmax[j][1] - minmax[j][0])\n",
    "        Megi_X[j] = (Megi_X[j] - minmax[j][0])/(minmax[j][1] - minmax[j][0])\n",
    "        \n",
    "        \n",
    "for j in ['위도(N)','경도(E)']:\n",
    "    Hin_Y[j] = (Hin_Y[j] - minmax[j][0])/(minmax[j][1] - minmax[j][0])\n",
    "    Megi_Y[j] = (Megi_Y[j] - minmax[j][0])/(minmax[j][1] - minmax[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "id": "a0f10ac2-13d5-401d-b0d9-89f51f8d83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hin_X = Hin_X.drop(['중심기압','초속(m/s)','시속(km/h)','강도','크기','강풍반경(km)[예외반경]'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "id": "f5de0de8-77f9-4c44-98bf-2044c835bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hin_X=Hin_X.values.reshape(-1,44,4).astype(np.float32)\n",
    "# Megi_X=Megi_X.values.reshape(-1,9,10).astype(np.float32)\n",
    "Hin_Y=Hin_Y.values.reshape(-1,44,2).astype(np.float32)\n",
    "# Megi_Y=Megi_Y.values.reshape(-1,9,2).astype(np.float32)\n",
    "\n",
    "Hin_X = torch.tensor(Hin_X, dtype=torch.float32)\n",
    "# Megi_X = torch.tensor(Megi_X, dtype=torch.float32)\n",
    "Hin_Y = torch.tensor(Hin_Y, dtype=torch.float32)\n",
    "# Megi_Y = torch.tensor(Megi_Y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "id": "6c570748-7465-4faa-9805-51318555067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hid = encoder.initHidden(num_layer,1)\n",
    "# hid = hid.to(device)\n",
    "# pred_path = []\n",
    "# encoder_outputs = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "# for i,j in zip(range(e,e+in_window),range(in_window)):\n",
    "#     out, hidden = encoder(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid)\n",
    "#     encoder_outputs[j] = out.squeeze()\n",
    "# d_input = torch.zeros((1,1,2)).to(device)\n",
    "# decoder_input = d_input\n",
    "# decoder_hidden = hidden\n",
    "\n",
    "\n",
    "# for i in range(e,out_window+e):\n",
    "#     decoder_output, decoder_hidden, attn = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "#     decoder_input = decoder_output\n",
    "#     pred_path.append(decoder_output.squeeze().to('cpu').detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "id": "21e84ddc-cdc2-40a6-b60f-40b17190ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "hid = encoder.initHidden(num_layer,1)\n",
    "hid = hid.to(device)\n",
    "\n",
    "hin_loss=np.zeros((44,1))\n",
    "pred_path = np.zeros((44,2))\n",
    "\n",
    "encoder_outputs = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "\n",
    "for e in range(40):\n",
    "    \n",
    "    for i,j in zip(range(e,e+in_window),range(in_window)):\n",
    "        cnt=0\n",
    "        out, hidden = encoder(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid)\n",
    "        encoder_outputs[j] = out.squeeze()\n",
    "    d_input = torch.zeros((1,1,2)).to(device)\n",
    "    decoder_input = d_input\n",
    "    decoder_hidden = hidden\n",
    "    \n",
    "    for i in range(e,out_window+e):\n",
    "        decoder_output, decoder_hidden, attn = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_input = decoder_output\n",
    "        loss = criterion(decoder_output, Hin_Y[:,i+in_window,:].reshape(1,1,-1).to(device))\n",
    "        hin_loss[i] += loss.item()\n",
    "        pred_path[i]+=decoder_output.squeeze().to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "id": "343e6603-582d-42e4-bbbd-e5b6ff997052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.4416841 , 0.62791306],\n",
       "        [0.42518187, 0.61000049],\n",
       "        [0.40319863, 0.59246898],\n",
       "        [0.38135979, 0.57804978],\n",
       "        [0.35563329, 0.56332183],\n",
       "        [0.33365387, 0.54573077],\n",
       "        [0.30788809, 0.53260124],\n",
       "        [0.28490809, 0.51982522],\n",
       "        [0.26003096, 0.51058173],\n",
       "        [0.23799464, 0.50549924],\n",
       "        [0.21462816, 0.50358236],\n",
       "        [0.19095625, 0.50753474],\n",
       "        [0.17155252, 0.51661891],\n",
       "        [0.16643548, 0.52725339],\n",
       "        [0.16301253, 0.5342201 ],\n",
       "        [0.17109218, 0.54411381],\n",
       "        [0.16817045, 0.53716242],\n",
       "        [0.16573712, 0.52832532],\n",
       "        [0.16188797, 0.5202024 ],\n",
       "        [0.16249207, 0.51583266],\n",
       "        [0.16097613, 0.50393951],\n",
       "        [0.16480812, 0.49639609],\n",
       "        [0.17055763, 0.48654166],\n",
       "        [0.16863643, 0.46892184],\n",
       "        [0.17295796, 0.45632941],\n",
       "        [0.18210115, 0.44128868],\n",
       "        [0.18015628, 0.42650983],\n",
       "        [0.18861251, 0.41497782],\n",
       "        [0.19609243, 0.40955642],\n",
       "        [0.20260885, 0.40197784],\n",
       "        [0.20615254, 0.39423257],\n",
       "        [0.22408095, 0.38964015],\n",
       "        [0.23887265, 0.3838256 ],\n",
       "        [0.25460011, 0.37852845],\n",
       "        [0.27817744, 0.3740007 ],\n",
       "        [0.30061936, 0.3676073 ],\n",
       "        [0.33822551, 0.36800835],\n",
       "        [0.37971944, 0.36319008],\n",
       "        [0.41440624, 0.3534748 ],\n",
       "        [0.4671481 , 0.35050729],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]]),\n",
       " tensor([[[0.4483, 0.6500],\n",
       "          [0.4533, 0.6333],\n",
       "          [0.4550, 0.6133],\n",
       "          [0.4567, 0.5922],\n",
       "          [0.4550, 0.5689],\n",
       "          [0.4517, 0.5478],\n",
       "          [0.4467, 0.5256],\n",
       "          [0.4467, 0.5044],\n",
       "          [0.4417, 0.4844],\n",
       "          [0.4383, 0.4656],\n",
       "          [0.4317, 0.4478],\n",
       "          [0.4233, 0.4333],\n",
       "          [0.4117, 0.4189],\n",
       "          [0.3950, 0.4044],\n",
       "          [0.3750, 0.3967],\n",
       "          [0.3633, 0.3944],\n",
       "          [0.3550, 0.3944],\n",
       "          [0.3550, 0.3944],\n",
       "          [0.3583, 0.3933],\n",
       "          [0.3650, 0.3900],\n",
       "          [0.3700, 0.3867],\n",
       "          [0.3750, 0.3856],\n",
       "          [0.3833, 0.3844],\n",
       "          [0.3933, 0.3844],\n",
       "          [0.4050, 0.3867],\n",
       "          [0.4183, 0.3844],\n",
       "          [0.4333, 0.3833],\n",
       "          [0.4500, 0.3867],\n",
       "          [0.4617, 0.3844],\n",
       "          [0.4767, 0.3856],\n",
       "          [0.4867, 0.3867],\n",
       "          [0.4967, 0.3878],\n",
       "          [0.5033, 0.3900],\n",
       "          [0.5167, 0.3956],\n",
       "          [0.5283, 0.4011],\n",
       "          [0.5400, 0.4067],\n",
       "          [0.5550, 0.4144],\n",
       "          [0.5700, 0.4222],\n",
       "          [0.5867, 0.4356],\n",
       "          [0.6083, 0.4500],\n",
       "          [0.6300, 0.4622],\n",
       "          [0.6550, 0.4778],\n",
       "          [0.7000, 0.5078],\n",
       "          [0.7400, 0.5189]]]))"
      ]
     },
     "execution_count": 1765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_path,Hin_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "id": "f82fe922-d97c-49b2-91c2-d015bca268fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = pred_path[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "id": "41502b8a-edaf-4d4a-bbed-7280c059abd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (emb): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (attn): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (attn_combine): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (gru): GRU(4, 4, batch_first=True)\n",
       "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (ln): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(input_size, 5, num_layer, batch_size)\n",
    "decoder1 = DecoderRNN(output_size, 5, num_layer, batch_size)\n",
    "\n",
    "encoder2 = EncoderRNN(input_size, 6, num_layer, batch_size)\n",
    "decoder2 = DecoderRNN(output_size, 6, num_layer, batch_size)\n",
    "\n",
    "encoder3 = EncoderRNN(input_size, 6, num_layer, batch_size)\n",
    "decoder3 = DecoderRNN(output_size, 6, num_layer, batch_size)\n",
    "\n",
    "encoder4 = EncoderRNN(input_size, 5, num_layer, batch_size)\n",
    "decoder4 = DecoderRNN(output_size, 5, num_layer, batch_size)\n",
    "\n",
    "encoder5 = EncoderRNN(input_size, 4, num_layer, batch_size)\n",
    "decoder5 = DecoderRNN(output_size, 4, num_layer, batch_size)\n",
    "\n",
    "\n",
    "encoder1.load_state_dict(torch.load('./Best_En.pt'))\n",
    "decoder1.load_state_dict(torch.load('./Best_De.pt'))\n",
    "encoder1.eval()\n",
    "decoder1.eval()\n",
    "\n",
    "encoder2.load_state_dict(torch.load('./Best_En_2.pt'))\n",
    "decoder2.load_state_dict(torch.load('./Best_De_2.pt'))\n",
    "encoder2.eval()\n",
    "decoder2.eval()\n",
    "\n",
    "encoder3.load_state_dict(torch.load('./Best_En_3.pt'))\n",
    "decoder3.load_state_dict(torch.load('./Best_De_3.pt'))\n",
    "encoder3.eval()\n",
    "decoder3.eval()\n",
    "\n",
    "encoder4.load_state_dict(torch.load('./Best_En_4.pt'))\n",
    "decoder4.load_state_dict(torch.load('./Best_De_4.pt'))\n",
    "encoder4.eval()\n",
    "decoder4.eval()\n",
    "\n",
    "encoder5.load_state_dict(torch.load('./Best_En_5.pt'))\n",
    "decoder5.load_state_dict(torch.load('./Best_De_5.pt'))\n",
    "encoder5.eval()\n",
    "decoder5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "id": "59dd489f-5243-4ad7-84d6-84fad7f76889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder1.to(device)\n",
    "decoder1.to(device)\n",
    "encoder2.to(device)\n",
    "decoder2.to(device)\n",
    "encoder3.to(device)\n",
    "decoder3.to(device)\n",
    "encoder4.to(device)\n",
    "decoder4.to(device)\n",
    "encoder5.to(device)\n",
    "decoder5.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "hid1 = encoder1.initHidden(num_layer,1)\n",
    "hid1 = hid1.to(device)\n",
    "\n",
    "hid2 = encoder2.initHidden(num_layer,1)\n",
    "hid2 = hid2.to(device)\n",
    "\n",
    "hid3 = encoder3.initHidden(num_layer,1)\n",
    "hid3 = hid3.to(device)\n",
    "\n",
    "hid4 = encoder4.initHidden(num_layer,1)\n",
    "hid4 = hid4.to(device)\n",
    "\n",
    "hid5 = encoder5.initHidden(num_layer,1)\n",
    "hid5 = hid5.to(device)\n",
    "\n",
    "hin_loss=np.zeros((44,1))\n",
    "pred_path1 = np.zeros((44,2))\n",
    "pred_path2 = np.zeros((44,2))\n",
    "pred_path3 = np.zeros((44,2))\n",
    "pred_path4 = np.zeros((44,2))\n",
    "pred_path5 = np.zeros((44,2))\n",
    "\n",
    "encoder_outputs1 = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "encoder_outputs2 = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "encoder_outputs3 = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "encoder_outputs4 = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "encoder_outputs5 = torch.zeros((in_window,1,encoder.hidden_size)).to(device)\n",
    "\n",
    "for e in range(40):\n",
    "    \n",
    "    for i,j in zip(range(e,e+in_window),range(in_window)):\n",
    "        cnt=0\n",
    "        out1, hidden1 = encoder1(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid1)\n",
    "        encoder_outputs1[j] = out.squeeze()\n",
    "        out2, hidden2 = encoder2(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid2)\n",
    "        encoder_outputs2[j] = out.squeeze()\n",
    "        out3, hidden3 = encoder3(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid3)\n",
    "        encoder_outputs3[j] = out.squeeze()\n",
    "        out4, hidden4 = encoder4(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid4)\n",
    "        encoder_outputs4[j] = out.squeeze()        \n",
    "        out5, hidden5 = encoder5(Hin_X[:,i,:].reshape(1,1,-1).to(device), hid5)\n",
    "        encoder_outputs5[j] = out.squeeze()        \n",
    "        \n",
    "    d_input = torch.zeros((1,1,2)).to(device)\n",
    "    decoder_input1 = d_input\n",
    "    decoder_hidden1 = hidden1\n",
    "    decoder_input2 = d_input\n",
    "    decoder_hidden2 = hidden2\n",
    "    decoder_input3 = d_input\n",
    "    decoder_hidden3 = hidden3\n",
    "    decoder_input4 = d_input\n",
    "    decoder_hidden4 = hidden4\n",
    "    decoder_input5 = d_input\n",
    "    decoder_hidden5 = hidden5\n",
    "    \n",
    "    for i in range(e,out_window+e):\n",
    "        decoder_output1, decoder_hidden1, attn1 = decoder1(decoder_input1, decoder_hidden1, encoder_outputs1)\n",
    "        decoder_input1 = decoder_output1\n",
    "        decoder_output2, decoder_hidden2, attn2 = decoder2(decoder_input2, decoder_hidden2, encoder_outputs2)\n",
    "        decoder_input2 = decoder_output2\n",
    "        decoder_output3, decoder_hidden3, attn3 = decoder3(decoder_input3, decoder_hidden3, encoder_outputs3)\n",
    "        decoder_input3 = decoder_output3\n",
    "        decoder_output4, decoder_hidden4, attn4 = decoder4(decoder_input4, decoder_hidden4, encoder_outputs4)\n",
    "        decoder_input4 = decoder_output4\n",
    "        decoder_output5, decoder_hidden5, attn5 = decoder5(decoder_input5, decoder_hidden5, encoder_outputs5)\n",
    "        decoder_input5 = decoder_output5\n",
    "        \n",
    "        # loss = criterion(decoder_output, Hin_Y[:,i+in_window,:].reshape(1,1,-1).to(device))\n",
    "        # hin_loss[i] += loss.item()\n",
    "        pred_path1[i]+=decoder_output1.squeeze().to('cpu').detach().numpy()\n",
    "        pred_path2[i]+=decoder_output2.squeeze().to('cpu').detach().numpy()\n",
    "        pred_path3[i]+=decoder_output3.squeeze().to('cpu').detach().numpy()\n",
    "        pred_path4[i]+=decoder_output4.squeeze().to('cpu').detach().numpy()\n",
    "        pred_path5[i]+=decoder_output5.squeeze().to('cpu').detach().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "id": "c2af8995-84da-420a-84d7-a9f86b135e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path =0.4*(0.6*pred_path1 + 0.3*pred_path2 + 0.1*pred_path3)+0.5*(pred_path4)+0.1*(pred_path5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1783,
   "id": "c064821f-9791-4475-9dee-f2b00e2dd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(pred_path)):\n",
    "#     if i==0:\n",
    "#         continue\n",
    "#     elif i==1 or i==40:\n",
    "#         pred_path[i] = pred_path[i]/2\n",
    "#     elif i==2 or i==39:\n",
    "#         pred_path[i] = pred_path[i]/3\n",
    "#     else:\n",
    "#         pred_path[i] = pred_path[i]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1784,
   "id": "a79f9f3c-9496-4722-bd4f-f46c7db46bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4686, 0.2725, 0.2589]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 1784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "id": "c761ef50-3a21-4f91-98d7-486664d742da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.48145987, 0.62406541],\n",
       "        [0.48069408, 0.60405569],\n",
       "        [0.47547733, 0.58145647],\n",
       "        [0.46838403, 0.5600357 ],\n",
       "        [0.45863902, 0.53817378],\n",
       "        [0.45422902, 0.51526764],\n",
       "        [0.44389398, 0.49380124],\n",
       "        [0.43585496, 0.47262516],\n",
       "        [0.42436485, 0.45236959],\n",
       "        [0.41311065, 0.43750921],\n",
       "        [0.39846129, 0.42134923],\n",
       "        [0.38000629, 0.40469228],\n",
       "        [0.3557458 , 0.39622248],\n",
       "        [0.34367885, 0.39272028],\n",
       "        [0.33452506, 0.39262685],\n",
       "        [0.33985696, 0.38981515],\n",
       "        [0.34142307, 0.38989455],\n",
       "        [0.34745447, 0.38603195],\n",
       "        [0.35132573, 0.38267129],\n",
       "        [0.35683879, 0.38107771],\n",
       "        [0.36353472, 0.38143341],\n",
       "        [0.37446381, 0.380959  ],\n",
       "        [0.3853215 , 0.38380607],\n",
       "        [0.3957914 , 0.38282687],\n",
       "        [0.41054046, 0.37994891],\n",
       "        [0.42720588, 0.38504327],\n",
       "        [0.43894175, 0.38569878],\n",
       "        [0.4576926 , 0.38815727],\n",
       "        [0.47137794, 0.38937362],\n",
       "        [0.48403771, 0.39149138],\n",
       "        [0.49058386, 0.39667106],\n",
       "        [0.50993022, 0.40237283],\n",
       "        [0.52513648, 0.40946101],\n",
       "        [0.54067824, 0.41603357],\n",
       "        [0.5618993 , 0.423476  ],\n",
       "        [0.58177001, 0.43209782],\n",
       "        [0.60596702, 0.44273935],\n",
       "        [0.63604024, 0.4548206 ],\n",
       "        [0.6657908 , 0.46689423],\n",
       "        [0.70210767, 0.47510661],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]]),\n",
       " tensor([[[0.4483, 0.6500],\n",
       "          [0.4533, 0.6333],\n",
       "          [0.4550, 0.6133],\n",
       "          [0.4567, 0.5922],\n",
       "          [0.4550, 0.5689],\n",
       "          [0.4517, 0.5478],\n",
       "          [0.4467, 0.5256],\n",
       "          [0.4467, 0.5044],\n",
       "          [0.4417, 0.4844],\n",
       "          [0.4383, 0.4656],\n",
       "          [0.4317, 0.4478],\n",
       "          [0.4233, 0.4333],\n",
       "          [0.4117, 0.4189],\n",
       "          [0.3950, 0.4044],\n",
       "          [0.3750, 0.3967],\n",
       "          [0.3633, 0.3944],\n",
       "          [0.3550, 0.3944],\n",
       "          [0.3550, 0.3944],\n",
       "          [0.3583, 0.3933],\n",
       "          [0.3650, 0.3900],\n",
       "          [0.3700, 0.3867],\n",
       "          [0.3750, 0.3856],\n",
       "          [0.3833, 0.3844],\n",
       "          [0.3933, 0.3844],\n",
       "          [0.4050, 0.3867],\n",
       "          [0.4183, 0.3844],\n",
       "          [0.4333, 0.3833],\n",
       "          [0.4500, 0.3867],\n",
       "          [0.4617, 0.3844],\n",
       "          [0.4767, 0.3856],\n",
       "          [0.4867, 0.3867],\n",
       "          [0.4967, 0.3878],\n",
       "          [0.5033, 0.3900],\n",
       "          [0.5167, 0.3956],\n",
       "          [0.5283, 0.4011],\n",
       "          [0.5400, 0.4067],\n",
       "          [0.5550, 0.4144],\n",
       "          [0.5700, 0.4222],\n",
       "          [0.5867, 0.4356],\n",
       "          [0.6083, 0.4500],\n",
       "          [0.6300, 0.4622],\n",
       "          [0.6550, 0.4778],\n",
       "          [0.7000, 0.5078],\n",
       "          [0.7400, 0.5189]]]))"
      ]
     },
     "execution_count": 1785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_path,Hin_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "id": "6f7087af-6e7e-4421-b17f-2111fa24a5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13250668])"
      ]
     },
     "execution_count": 1786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin_loss.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1787,
   "id": "f58ad3aa-dc29-41a0-aca8-4f6ad347e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = pred_path[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1788,
   "id": "1e6b002a-f313-45e8-9772-de0cfe689bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4483, 0.6500],\n",
       "         [0.4533, 0.6333],\n",
       "         [0.4550, 0.6133],\n",
       "         [0.4567, 0.5922],\n",
       "         [0.4550, 0.5689],\n",
       "         [0.4517, 0.5478],\n",
       "         [0.4467, 0.5256],\n",
       "         [0.4467, 0.5044],\n",
       "         [0.4417, 0.4844],\n",
       "         [0.4383, 0.4656],\n",
       "         [0.4317, 0.4478],\n",
       "         [0.4233, 0.4333],\n",
       "         [0.4117, 0.4189],\n",
       "         [0.3950, 0.4044],\n",
       "         [0.3750, 0.3967],\n",
       "         [0.3633, 0.3944],\n",
       "         [0.3550, 0.3944],\n",
       "         [0.3550, 0.3944],\n",
       "         [0.3583, 0.3933],\n",
       "         [0.3650, 0.3900],\n",
       "         [0.3700, 0.3867],\n",
       "         [0.3750, 0.3856],\n",
       "         [0.3833, 0.3844],\n",
       "         [0.3933, 0.3844],\n",
       "         [0.4050, 0.3867],\n",
       "         [0.4183, 0.3844],\n",
       "         [0.4333, 0.3833],\n",
       "         [0.4500, 0.3867],\n",
       "         [0.4617, 0.3844],\n",
       "         [0.4767, 0.3856],\n",
       "         [0.4867, 0.3867],\n",
       "         [0.4967, 0.3878],\n",
       "         [0.5033, 0.3900],\n",
       "         [0.5167, 0.3956],\n",
       "         [0.5283, 0.4011],\n",
       "         [0.5400, 0.4067],\n",
       "         [0.5550, 0.4144],\n",
       "         [0.5700, 0.4222],\n",
       "         [0.5867, 0.4356],\n",
       "         [0.6083, 0.4500],\n",
       "         [0.6300, 0.4622],\n",
       "         [0.6550, 0.4778],\n",
       "         [0.7000, 0.5078],\n",
       "         [0.7400, 0.5189]]])"
      ]
     },
     "execution_count": 1788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hin_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1789,
   "id": "b1c3d72b-4077-4c61-bda7-ac36afc1d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_x=[]\n",
    "path_y=[]\n",
    "for i in range(len(pred_path)):\n",
    "    path_x.append(pred_path[i][0] * (minmax['위도(N)'][1] - minmax['위도(N)'][0]) + minmax['위도(N)'][0])\n",
    "    path_y.append(pred_path[i][1] * (minmax['경도(E)'][1] - minmax['경도(E)'][0]) + minmax['경도(E)'][0])\n",
    "    \n",
    "    \n",
    "path_x = pd.DataFrame(path_x)\n",
    "path_y = pd.DataFrame(path_y)\n",
    "\n",
    "pred1_path = pd.concat([path_x, path_y],axis=1)\n",
    "\n",
    "pred1_path.columns=['위도','경도']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1790,
   "id": "51be32e4-313b-4e8d-8453-a747a8c2bec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>위도</th>\n",
       "      <th>경도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.887592</td>\n",
       "      <td>146.165887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.841645</td>\n",
       "      <td>144.365012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.528640</td>\n",
       "      <td>142.331082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.103042</td>\n",
       "      <td>140.403213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.518341</td>\n",
       "      <td>138.435640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.253741</td>\n",
       "      <td>136.374088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.633639</td>\n",
       "      <td>134.442112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.151298</td>\n",
       "      <td>132.536264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.461891</td>\n",
       "      <td>130.713263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24.786639</td>\n",
       "      <td>129.375829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.907678</td>\n",
       "      <td>127.921431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.800377</td>\n",
       "      <td>126.422305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21.344748</td>\n",
       "      <td>125.660023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.620731</td>\n",
       "      <td>125.344826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.071504</td>\n",
       "      <td>125.336416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.391418</td>\n",
       "      <td>125.083364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20.485384</td>\n",
       "      <td>125.090510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.847268</td>\n",
       "      <td>124.742875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21.079544</td>\n",
       "      <td>124.440416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21.410327</td>\n",
       "      <td>124.296994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.812083</td>\n",
       "      <td>124.329007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.467828</td>\n",
       "      <td>124.286310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.119290</td>\n",
       "      <td>124.542546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.747484</td>\n",
       "      <td>124.454419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.632428</td>\n",
       "      <td>124.195402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.632353</td>\n",
       "      <td>124.653895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.336505</td>\n",
       "      <td>124.712890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.461556</td>\n",
       "      <td>124.934154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.282676</td>\n",
       "      <td>125.043626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.042263</td>\n",
       "      <td>125.234225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29.435031</td>\n",
       "      <td>125.700395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30.595813</td>\n",
       "      <td>126.213555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31.508189</td>\n",
       "      <td>126.851491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32.440695</td>\n",
       "      <td>127.443021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33.713958</td>\n",
       "      <td>128.112840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34.906200</td>\n",
       "      <td>128.888804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36.358021</td>\n",
       "      <td>129.846541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38.162415</td>\n",
       "      <td>130.933854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39.947448</td>\n",
       "      <td>132.020481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>42.126460</td>\n",
       "      <td>132.759595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           위도          경도\n",
       "0   28.887592  146.165887\n",
       "1   28.841645  144.365012\n",
       "2   28.528640  142.331082\n",
       "3   28.103042  140.403213\n",
       "4   27.518341  138.435640\n",
       "5   27.253741  136.374088\n",
       "6   26.633639  134.442112\n",
       "7   26.151298  132.536264\n",
       "8   25.461891  130.713263\n",
       "9   24.786639  129.375829\n",
       "10  23.907678  127.921431\n",
       "11  22.800377  126.422305\n",
       "12  21.344748  125.660023\n",
       "13  20.620731  125.344826\n",
       "14  20.071504  125.336416\n",
       "15  20.391418  125.083364\n",
       "16  20.485384  125.090510\n",
       "17  20.847268  124.742875\n",
       "18  21.079544  124.440416\n",
       "19  21.410327  124.296994\n",
       "20  21.812083  124.329007\n",
       "21  22.467828  124.286310\n",
       "22  23.119290  124.542546\n",
       "23  23.747484  124.454419\n",
       "24  24.632428  124.195402\n",
       "25  25.632353  124.653895\n",
       "26  26.336505  124.712890\n",
       "27  27.461556  124.934154\n",
       "28  28.282676  125.043626\n",
       "29  29.042263  125.234225\n",
       "30  29.435031  125.700395\n",
       "31  30.595813  126.213555\n",
       "32  31.508189  126.851491\n",
       "33  32.440695  127.443021\n",
       "34  33.713958  128.112840\n",
       "35  34.906200  128.888804\n",
       "36  36.358021  129.846541\n",
       "37  38.162415  130.933854\n",
       "38  39.947448  132.020481\n",
       "39  42.126460  132.759595"
      ]
     },
     "execution_count": 1790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "id": "5e1cfee7-08c7-4a1f-b3bb-9926312c37d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>위도(N)</th>\n",
       "      <th>경도(E)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.9</td>\n",
       "      <td>148.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.2</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.3</td>\n",
       "      <td>145.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.4</td>\n",
       "      <td>143.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.3</td>\n",
       "      <td>141.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.1</td>\n",
       "      <td>139.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.8</td>\n",
       "      <td>137.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.8</td>\n",
       "      <td>135.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26.5</td>\n",
       "      <td>133.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26.3</td>\n",
       "      <td>131.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.9</td>\n",
       "      <td>130.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25.4</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24.7</td>\n",
       "      <td>127.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23.7</td>\n",
       "      <td>126.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22.5</td>\n",
       "      <td>125.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21.8</td>\n",
       "      <td>125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21.3</td>\n",
       "      <td>125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.3</td>\n",
       "      <td>125.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21.5</td>\n",
       "      <td>125.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21.9</td>\n",
       "      <td>125.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22.2</td>\n",
       "      <td>124.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.5</td>\n",
       "      <td>124.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.0</td>\n",
       "      <td>124.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.6</td>\n",
       "      <td>124.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.3</td>\n",
       "      <td>124.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.1</td>\n",
       "      <td>124.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>124.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>124.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27.7</td>\n",
       "      <td>124.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28.6</td>\n",
       "      <td>124.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29.2</td>\n",
       "      <td>124.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>29.8</td>\n",
       "      <td>124.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>30.2</td>\n",
       "      <td>125.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>31.0</td>\n",
       "      <td>125.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>31.7</td>\n",
       "      <td>126.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32.4</td>\n",
       "      <td>126.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>33.3</td>\n",
       "      <td>127.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>34.2</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>35.2</td>\n",
       "      <td>129.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>36.5</td>\n",
       "      <td>130.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>37.8</td>\n",
       "      <td>131.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>39.3</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42.0</td>\n",
       "      <td>135.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44.4</td>\n",
       "      <td>136.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    위도(N)  경도(E)\n",
       "0    26.9  148.5\n",
       "1    27.2  147.0\n",
       "2    27.3  145.2\n",
       "3    27.4  143.3\n",
       "4    27.3  141.2\n",
       "5    27.1  139.3\n",
       "6    26.8  137.3\n",
       "7    26.8  135.4\n",
       "8    26.5  133.6\n",
       "9    26.3  131.9\n",
       "10   25.9  130.3\n",
       "11   25.4  129.0\n",
       "12   24.7  127.7\n",
       "13   23.7  126.4\n",
       "14   22.5  125.7\n",
       "15   21.8  125.5\n",
       "16   21.3  125.5\n",
       "17   21.3  125.5\n",
       "18   21.5  125.4\n",
       "19   21.9  125.1\n",
       "20   22.2  124.8\n",
       "21   22.5  124.7\n",
       "22   23.0  124.6\n",
       "23   23.6  124.6\n",
       "24   24.3  124.8\n",
       "25   25.1  124.6\n",
       "26   26.0  124.5\n",
       "27   27.0  124.8\n",
       "28   27.7  124.6\n",
       "29   28.6  124.7\n",
       "30   29.2  124.8\n",
       "31   29.8  124.9\n",
       "32   30.2  125.1\n",
       "33   31.0  125.6\n",
       "34   31.7  126.1\n",
       "35   32.4  126.6\n",
       "36   33.3  127.3\n",
       "37   34.2  128.0\n",
       "38   35.2  129.2\n",
       "39   36.5  130.5\n",
       "40   37.8  131.6\n",
       "41   39.3  133.0\n",
       "42   42.0  135.7\n",
       "43   44.4  136.7"
      ]
     },
     "execution_count": 1811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hin_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "id": "a609b434-8caa-4e27-b83f-0b0ada78af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeonjw\\anaconda3\\envs\\jeon\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:118: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "C:\\Users\\jeonjw\\anaconda3\\envs\\jeon\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:118: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 1812,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf_path = gpd.GeoDataFrame(pred1_path, geometry=gpd.points_from_xy(pred1_path[:]['경도'],pred1_path[:]['위도']), crs='epsg:4326')\n",
    "line_geometry = LineString(gdf_path[\"geometry\"])\n",
    "df_path_line = pd.DataFrame({'geometry' : [line_geometry]})\n",
    "gdf_path_line = gpd.GeoDataFrame(df_path_line, geometry=df_path_line.geometry)\n",
    "\n",
    "gdf_path_1 = gpd.GeoDataFrame(Hin_real, geometry=gpd.points_from_xy(Hin_real[:]['경도(E)'],Hin_real[:]['위도(N)']), crs='epsg:4326')\n",
    "line_geometry_1 = LineString(gdf_path_1[\"geometry\"])\n",
    "df_path_line_1 = pd.DataFrame({'geometry' : [line_geometry_1]})\n",
    "gdf_path_line_1 = gpd.GeoDataFrame(df_path_line_1, geometry=df_path_line_1.geometry)\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "basemap = world.loc[world['name'].isin(['South Korea', 'North Korea','Japan', 'China', 'Philippines','Taiwan'])]\n",
    "ax = basemap.plot(figsize=(20,20), color='whitesmoke', edgecolor='black', linewidth=1)\n",
    "ax.axis('off')\n",
    "gdf_path_line.plot(ax=ax, linewidth=1., color='red', zorder=1)\n",
    "gdf_path_line_1.plot(ax=ax, linewidth=1. , color='blue', zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dbab0-d016-4dbc-8a20-839ac16a0823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
